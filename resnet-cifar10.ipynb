{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.datasets as datasets\n",
    "from tensorflow.keras.layers import Input, Conv2D,BatchNormalization, AveragePooling2D, Flatten, Dense,Activation, Dropout, MaxPooling2D\n",
    "import tensorflow.keras.layers.experimental.preprocessing\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import numpy\n",
    "import random\n",
    "from skimage import transform\n",
    "from scipy import ndimage\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cifar10 = datasets.cifar10\n",
    "(xtrain,ytrain),(xtest,ytest) = cifar10.load_data()\n",
    "xtrain = tf.cast(xtrain,tf.float32)/255\n",
    "xtest = tf.cast(xtest,tf.float32)/255\n",
    "\n",
    "\n",
    "print(xtrain.shape)\n",
    "plt.imshow(xtrain[0])\n",
    "plt.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHT_DECAY       = 5e-4\n",
    "\n",
    "def identity_block(filters,input_tensor):\n",
    "    \n",
    "    X = Conv2D(filters,(3,3),padding='same',kernel_regularizer=l2(WEIGHT_DECAY))(input_tensor)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = tf.nn.relu(X)\n",
    "    \n",
    "    \n",
    "    X = Conv2D(filters,(3,3),padding='same',kernel_regularizer=l2(WEIGHT_DECAY))(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    \n",
    "    X += input_tensor\n",
    "    \n",
    "    return tf.nn.relu(X)\n",
    "    \n",
    "def conv_block(filters,input_tensor):\n",
    "    X = Conv2D(filters,(3,3),padding='same',kernel_regularizer=l2(WEIGHT_DECAY))(input_tensor)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = tf.nn.relu(X)\n",
    "    \n",
    "    X = Conv2D(filters,(3,3),padding='same',kernel_regularizer=l2(WEIGHT_DECAY))(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    \n",
    "    \n",
    "    X += BatchNormalization()(Conv2D(filters,(3,3),padding='same',kernel_regularizer=l2(WEIGHT_DECAY))(input_tensor))\n",
    "    \n",
    "    return tf.nn.relu(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def Resnet34(input_tensor):\n",
    "   \n",
    "    X = Conv2D(64,(3,3))(input_tensor)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = MaxPooling2D((2,2))(X)\n",
    "    \n",
    "    X = identity_block(64,X)\n",
    "    X = identity_block(64,X)\n",
    "    X = identity_block(64,X)\n",
    "    \n",
    "    X = conv_block(128,X)\n",
    "    X = identity_block(128,X)\n",
    "    X = identity_block(128,X)\n",
    "    X = identity_block(128,X)\n",
    "    \n",
    "    \n",
    "    X = conv_block(256,X)\n",
    "    X = identity_block(256,X)\n",
    "    X = identity_block(256,X)\n",
    "    X = identity_block(256,X)\n",
    "    X = identity_block(256,X)\n",
    "    X = identity_block(256,X)\n",
    "    \n",
    "    X = conv_block(512,X)\n",
    "    X = identity_block(512,X)\n",
    "    X = identity_block(512,X)\n",
    "    \n",
    "    X = AveragePooling2D((2,2))(X)\n",
    "    \n",
    "    X = Flatten()(X)\n",
    "    X = Dense(1000, activation='relu')(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dense(1000, activation='relu')(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dense(10, activation = 'softmax')(X)\n",
    "    \n",
    "    \n",
    "    return X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 30, 30, 64)   1792        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_114 (BatchN (None, 30, 30, 64)   256         conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 15, 15, 64)   0           batch_normalization_114[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, 15, 15, 64)   36928       max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_115 (BatchN (None, 15, 15, 64)   256         conv2d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_96 (TensorFlow [(None, 15, 15, 64)] 0           batch_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_110 (Conv2D)             (None, 15, 15, 64)   36928       tf_op_layer_Relu_96[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_116 (BatchN (None, 15, 15, 64)   256         conv2d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_48 (TensorFlowO [(None, 15, 15, 64)] 0           batch_normalization_116[0][0]    \n",
      "                                                                 max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_97 (TensorFlow [(None, 15, 15, 64)] 0           tf_op_layer_add_48[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_111 (Conv2D)             (None, 15, 15, 64)   36928       tf_op_layer_Relu_97[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_117 (BatchN (None, 15, 15, 64)   256         conv2d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_98 (TensorFlow [(None, 15, 15, 64)] 0           batch_normalization_117[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_112 (Conv2D)             (None, 15, 15, 64)   36928       tf_op_layer_Relu_98[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_118 (BatchN (None, 15, 15, 64)   256         conv2d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_49 (TensorFlowO [(None, 15, 15, 64)] 0           batch_normalization_118[0][0]    \n",
      "                                                                 tf_op_layer_Relu_97[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_99 (TensorFlow [(None, 15, 15, 64)] 0           tf_op_layer_add_49[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_113 (Conv2D)             (None, 15, 15, 64)   36928       tf_op_layer_Relu_99[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_119 (BatchN (None, 15, 15, 64)   256         conv2d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_100 (TensorFlo [(None, 15, 15, 64)] 0           batch_normalization_119[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_114 (Conv2D)             (None, 15, 15, 64)   36928       tf_op_layer_Relu_100[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_120 (BatchN (None, 15, 15, 64)   256         conv2d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_50 (TensorFlowO [(None, 15, 15, 64)] 0           batch_normalization_120[0][0]    \n",
      "                                                                 tf_op_layer_Relu_99[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_101 (TensorFlo [(None, 15, 15, 64)] 0           tf_op_layer_add_50[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_115 (Conv2D)             (None, 15, 15, 128)  73856       tf_op_layer_Relu_101[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_121 (BatchN (None, 15, 15, 128)  512         conv2d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_102 (TensorFlo [(None, 15, 15, 128) 0           batch_normalization_121[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_116 (Conv2D)             (None, 15, 15, 128)  147584      tf_op_layer_Relu_102[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_117 (Conv2D)             (None, 15, 15, 128)  73856       tf_op_layer_Relu_101[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_122 (BatchN (None, 15, 15, 128)  512         conv2d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_123 (BatchN (None, 15, 15, 128)  512         conv2d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_51 (TensorFlowO [(None, 15, 15, 128) 0           batch_normalization_122[0][0]    \n",
      "                                                                 batch_normalization_123[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_103 (TensorFlo [(None, 15, 15, 128) 0           tf_op_layer_add_51[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_118 (Conv2D)             (None, 15, 15, 128)  147584      tf_op_layer_Relu_103[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_124 (BatchN (None, 15, 15, 128)  512         conv2d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_104 (TensorFlo [(None, 15, 15, 128) 0           batch_normalization_124[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_119 (Conv2D)             (None, 15, 15, 128)  147584      tf_op_layer_Relu_104[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_125 (BatchN (None, 15, 15, 128)  512         conv2d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_52 (TensorFlowO [(None, 15, 15, 128) 0           batch_normalization_125[0][0]    \n",
      "                                                                 tf_op_layer_Relu_103[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_105 (TensorFlo [(None, 15, 15, 128) 0           tf_op_layer_add_52[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_120 (Conv2D)             (None, 15, 15, 128)  147584      tf_op_layer_Relu_105[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_126 (BatchN (None, 15, 15, 128)  512         conv2d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_106 (TensorFlo [(None, 15, 15, 128) 0           batch_normalization_126[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_121 (Conv2D)             (None, 15, 15, 128)  147584      tf_op_layer_Relu_106[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_127 (BatchN (None, 15, 15, 128)  512         conv2d_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_53 (TensorFlowO [(None, 15, 15, 128) 0           batch_normalization_127[0][0]    \n",
      "                                                                 tf_op_layer_Relu_105[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_107 (TensorFlo [(None, 15, 15, 128) 0           tf_op_layer_add_53[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_122 (Conv2D)             (None, 15, 15, 128)  147584      tf_op_layer_Relu_107[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_128 (BatchN (None, 15, 15, 128)  512         conv2d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_108 (TensorFlo [(None, 15, 15, 128) 0           batch_normalization_128[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_123 (Conv2D)             (None, 15, 15, 128)  147584      tf_op_layer_Relu_108[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_129 (BatchN (None, 15, 15, 128)  512         conv2d_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_54 (TensorFlowO [(None, 15, 15, 128) 0           batch_normalization_129[0][0]    \n",
      "                                                                 tf_op_layer_Relu_107[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_109 (TensorFlo [(None, 15, 15, 128) 0           tf_op_layer_add_54[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_124 (Conv2D)             (None, 15, 15, 256)  295168      tf_op_layer_Relu_109[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_130 (BatchN (None, 15, 15, 256)  1024        conv2d_124[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_110 (TensorFlo [(None, 15, 15, 256) 0           batch_normalization_130[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_125 (Conv2D)             (None, 15, 15, 256)  590080      tf_op_layer_Relu_110[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_126 (Conv2D)             (None, 15, 15, 256)  295168      tf_op_layer_Relu_109[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_131 (BatchN (None, 15, 15, 256)  1024        conv2d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_132 (BatchN (None, 15, 15, 256)  1024        conv2d_126[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_55 (TensorFlowO [(None, 15, 15, 256) 0           batch_normalization_131[0][0]    \n",
      "                                                                 batch_normalization_132[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_111 (TensorFlo [(None, 15, 15, 256) 0           tf_op_layer_add_55[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_127 (Conv2D)             (None, 15, 15, 256)  590080      tf_op_layer_Relu_111[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_133 (BatchN (None, 15, 15, 256)  1024        conv2d_127[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_112 (TensorFlo [(None, 15, 15, 256) 0           batch_normalization_133[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_128 (Conv2D)             (None, 15, 15, 256)  590080      tf_op_layer_Relu_112[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_134 (BatchN (None, 15, 15, 256)  1024        conv2d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_56 (TensorFlowO [(None, 15, 15, 256) 0           batch_normalization_134[0][0]    \n",
      "                                                                 tf_op_layer_Relu_111[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_113 (TensorFlo [(None, 15, 15, 256) 0           tf_op_layer_add_56[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_129 (Conv2D)             (None, 15, 15, 256)  590080      tf_op_layer_Relu_113[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_135 (BatchN (None, 15, 15, 256)  1024        conv2d_129[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_114 (TensorFlo [(None, 15, 15, 256) 0           batch_normalization_135[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_130 (Conv2D)             (None, 15, 15, 256)  590080      tf_op_layer_Relu_114[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_136 (BatchN (None, 15, 15, 256)  1024        conv2d_130[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_57 (TensorFlowO [(None, 15, 15, 256) 0           batch_normalization_136[0][0]    \n",
      "                                                                 tf_op_layer_Relu_113[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_115 (TensorFlo [(None, 15, 15, 256) 0           tf_op_layer_add_57[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_131 (Conv2D)             (None, 15, 15, 256)  590080      tf_op_layer_Relu_115[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_137 (BatchN (None, 15, 15, 256)  1024        conv2d_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_116 (TensorFlo [(None, 15, 15, 256) 0           batch_normalization_137[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_132 (Conv2D)             (None, 15, 15, 256)  590080      tf_op_layer_Relu_116[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_138 (BatchN (None, 15, 15, 256)  1024        conv2d_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_58 (TensorFlowO [(None, 15, 15, 256) 0           batch_normalization_138[0][0]    \n",
      "                                                                 tf_op_layer_Relu_115[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_117 (TensorFlo [(None, 15, 15, 256) 0           tf_op_layer_add_58[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_133 (Conv2D)             (None, 15, 15, 256)  590080      tf_op_layer_Relu_117[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_139 (BatchN (None, 15, 15, 256)  1024        conv2d_133[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_118 (TensorFlo [(None, 15, 15, 256) 0           batch_normalization_139[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_134 (Conv2D)             (None, 15, 15, 256)  590080      tf_op_layer_Relu_118[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_140 (BatchN (None, 15, 15, 256)  1024        conv2d_134[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_59 (TensorFlowO [(None, 15, 15, 256) 0           batch_normalization_140[0][0]    \n",
      "                                                                 tf_op_layer_Relu_117[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_119 (TensorFlo [(None, 15, 15, 256) 0           tf_op_layer_add_59[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_135 (Conv2D)             (None, 15, 15, 256)  590080      tf_op_layer_Relu_119[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_141 (BatchN (None, 15, 15, 256)  1024        conv2d_135[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_120 (TensorFlo [(None, 15, 15, 256) 0           batch_normalization_141[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_136 (Conv2D)             (None, 15, 15, 256)  590080      tf_op_layer_Relu_120[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_142 (BatchN (None, 15, 15, 256)  1024        conv2d_136[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_60 (TensorFlowO [(None, 15, 15, 256) 0           batch_normalization_142[0][0]    \n",
      "                                                                 tf_op_layer_Relu_119[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_121 (TensorFlo [(None, 15, 15, 256) 0           tf_op_layer_add_60[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_137 (Conv2D)             (None, 15, 15, 512)  1180160     tf_op_layer_Relu_121[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_143 (BatchN (None, 15, 15, 512)  2048        conv2d_137[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_122 (TensorFlo [(None, 15, 15, 512) 0           batch_normalization_143[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_138 (Conv2D)             (None, 15, 15, 512)  2359808     tf_op_layer_Relu_122[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_139 (Conv2D)             (None, 15, 15, 512)  1180160     tf_op_layer_Relu_121[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_144 (BatchN (None, 15, 15, 512)  2048        conv2d_138[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_145 (BatchN (None, 15, 15, 512)  2048        conv2d_139[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_61 (TensorFlowO [(None, 15, 15, 512) 0           batch_normalization_144[0][0]    \n",
      "                                                                 batch_normalization_145[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_123 (TensorFlo [(None, 15, 15, 512) 0           tf_op_layer_add_61[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_140 (Conv2D)             (None, 15, 15, 512)  2359808     tf_op_layer_Relu_123[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_146 (BatchN (None, 15, 15, 512)  2048        conv2d_140[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_124 (TensorFlo [(None, 15, 15, 512) 0           batch_normalization_146[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_141 (Conv2D)             (None, 15, 15, 512)  2359808     tf_op_layer_Relu_124[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_147 (BatchN (None, 15, 15, 512)  2048        conv2d_141[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_62 (TensorFlowO [(None, 15, 15, 512) 0           batch_normalization_147[0][0]    \n",
      "                                                                 tf_op_layer_Relu_123[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_125 (TensorFlo [(None, 15, 15, 512) 0           tf_op_layer_add_62[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_142 (Conv2D)             (None, 15, 15, 512)  2359808     tf_op_layer_Relu_125[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_148 (BatchN (None, 15, 15, 512)  2048        conv2d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_126 (TensorFlo [(None, 15, 15, 512) 0           batch_normalization_148[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_143 (Conv2D)             (None, 15, 15, 512)  2359808     tf_op_layer_Relu_126[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_149 (BatchN (None, 15, 15, 512)  2048        conv2d_143[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_63 (TensorFlowO [(None, 15, 15, 512) 0           batch_normalization_149[0][0]    \n",
      "                                                                 tf_op_layer_Relu_125[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_127 (TensorFlo [(None, 15, 15, 512) 0           tf_op_layer_add_63[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 7, 7, 512)    0           tf_op_layer_Relu_127[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 25088)        0           average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1000)         25089000    flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 1000)         0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_150 (BatchN (None, 1000)         4000        dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 1000)         1001000     batch_normalization_150[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 1000)         0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_151 (BatchN (None, 1000)         4000        dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 10)           10010       batch_normalization_151[0][0]    \n",
      "==================================================================================================\n",
      "Total params: 48,786,794\n",
      "Trainable params: 48,765,770\n",
      "Non-trainable params: 21,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_input = Input(shape=(32,32,3))\n",
    "output = Resnet34(img_input)\n",
    "model = Model(img_input,output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        \n",
    "        rotation_range=30,\n",
    "        width_shift_range=0.125,\n",
    "        height_shift_range=0.125,\n",
    "        shear_range=0.125,\n",
    "        rescale=0/1.,\n",
    "        zoom_range=0.125,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "\n",
    "\n",
    "datagen.fit(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = []\n",
    "\n",
    "class TestCallback(Callback):\n",
    "    def __init__(self, test_data):\n",
    "        train_acc = []\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        loss, acc = self.model.evaluate(x, y, verbose=1)\n",
    "        train_acc.append(acc)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 782 steps\n",
      "Epoch 1/100\n",
      "10000/10000 [==============================] - 9s 937us/sample - loss: 2.4711 - accuracy: 0.1873\n",
      "782/782 [==============================] - 120s 154ms/step - loss: 2.1189 - accuracy: 0.2524\n",
      "Epoch 2/100\n",
      "10000/10000 [==============================] - 9s 859us/sample - loss: 1.9069 - accuracy: 0.2865\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 1.8749 - accuracy: 0.3205\n",
      "Epoch 3/100\n",
      "10000/10000 [==============================] - 9s 868us/sample - loss: 2.3510 - accuracy: 0.2640\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 1.7554 - accuracy: 0.3607\n",
      "Epoch 4/100\n",
      "10000/10000 [==============================] - 9s 854us/sample - loss: 1.6278 - accuracy: 0.4006\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 1.6737 - accuracy: 0.3907\n",
      "Epoch 5/100\n",
      "10000/10000 [==============================] - 9s 855us/sample - loss: 7.3457 - accuracy: 0.3954\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 1.5742 - accuracy: 0.4309\n",
      "Epoch 6/100\n",
      "10000/10000 [==============================] - 9s 856us/sample - loss: 1.7367 - accuracy: 0.4429\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 1.5064 - accuracy: 0.4568\n",
      "Epoch 7/100\n",
      "10000/10000 [==============================] - 9s 852us/sample - loss: 6.9412 - accuracy: 0.3824\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 1.4134 - accuracy: 0.4934\n",
      "Epoch 8/100\n",
      "10000/10000 [==============================] - 9s 851us/sample - loss: 1.4525 - accuracy: 0.4986\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 1.3906 - accuracy: 0.5016\n",
      "Epoch 9/100\n",
      "10000/10000 [==============================] - 9s 854us/sample - loss: 1.2917 - accuracy: 0.5470\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 1.3153 - accuracy: 0.5287\n",
      "Epoch 10/100\n",
      "10000/10000 [==============================] - 9s 854us/sample - loss: 1.6491 - accuracy: 0.4982\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 1.2481 - accuracy: 0.5565\n",
      "Epoch 11/100\n",
      "10000/10000 [==============================] - 9s 851us/sample - loss: 1.3879 - accuracy: 0.5490\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 1.1933 - accuracy: 0.5712\n",
      "Epoch 12/100\n",
      "10000/10000 [==============================] - 9s 852us/sample - loss: 1.1258 - accuracy: 0.6148\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 1.1824 - accuracy: 0.5765\n",
      "Epoch 13/100\n",
      "10000/10000 [==============================] - 8s 849us/sample - loss: 1.3138 - accuracy: 0.5682\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 1.1303 - accuracy: 0.5976\n",
      "Epoch 14/100\n",
      "10000/10000 [==============================] - 9s 858us/sample - loss: 1.2951 - accuracy: 0.5727\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 1.0605 - accuracy: 0.6232\n",
      "Epoch 15/100\n",
      "10000/10000 [==============================] - 9s 851us/sample - loss: 1.0068 - accuracy: 0.6474\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 1.0502 - accuracy: 0.6289\n",
      "Epoch 16/100\n",
      "10000/10000 [==============================] - 8s 849us/sample - loss: 0.9879 - accuracy: 0.6609\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.9883 - accuracy: 0.6503\n",
      "Epoch 17/100\n",
      "10000/10000 [==============================] - 9s 850us/sample - loss: 1.0180 - accuracy: 0.6538\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.9698 - accuracy: 0.6561\n",
      "Epoch 18/100\n",
      "10000/10000 [==============================] - 9s 853us/sample - loss: 0.9766 - accuracy: 0.6625\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.9311 - accuracy: 0.6721\n",
      "Epoch 19/100\n",
      "10000/10000 [==============================] - 9s 853us/sample - loss: 1.0052 - accuracy: 0.6705\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.9157 - accuracy: 0.6788\n",
      "Epoch 20/100\n",
      "10000/10000 [==============================] - 9s 853us/sample - loss: 0.8303 - accuracy: 0.7063\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.8730 - accuracy: 0.6969\n",
      "Epoch 21/100\n",
      "10000/10000 [==============================] - 9s 863us/sample - loss: 1.2674 - accuracy: 0.6297\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.8457 - accuracy: 0.7038\n",
      "Epoch 22/100\n",
      "10000/10000 [==============================] - 9s 856us/sample - loss: 0.9919 - accuracy: 0.6791\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.8241 - accuracy: 0.7131\n",
      "Epoch 23/100\n",
      "10000/10000 [==============================] - 8s 850us/sample - loss: 0.9939 - accuracy: 0.6709\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.7953 - accuracy: 0.7223\n",
      "Epoch 24/100\n",
      "10000/10000 [==============================] - 9s 852us/sample - loss: 0.8228 - accuracy: 0.7205\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.7689 - accuracy: 0.7322\n",
      "Epoch 25/100\n",
      "10000/10000 [==============================] - 9s 852us/sample - loss: 0.8859 - accuracy: 0.7066\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.7523 - accuracy: 0.7383\n",
      "Epoch 26/100\n",
      "10000/10000 [==============================] - 9s 852us/sample - loss: 0.9053 - accuracy: 0.6986\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.7347 - accuracy: 0.7428\n",
      "Epoch 27/100\n",
      "10000/10000 [==============================] - 9s 851us/sample - loss: 0.7993 - accuracy: 0.7209\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.7304 - accuracy: 0.7448\n",
      "Epoch 28/100\n",
      "10000/10000 [==============================] - 9s 852us/sample - loss: 0.6908 - accuracy: 0.7600\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.7083 - accuracy: 0.7556\n",
      "Epoch 29/100\n",
      "10000/10000 [==============================] - 9s 854us/sample - loss: 1.0961 - accuracy: 0.6550\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.6858 - accuracy: 0.7629\n",
      "Epoch 30/100\n",
      "10000/10000 [==============================] - 9s 852us/sample - loss: 0.8547 - accuracy: 0.7234\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.6729 - accuracy: 0.7665\n",
      "Epoch 31/100\n",
      "10000/10000 [==============================] - 8s 850us/sample - loss: 0.6947 - accuracy: 0.7703\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.6616 - accuracy: 0.7699\n",
      "Epoch 32/100\n",
      "10000/10000 [==============================] - 9s 859us/sample - loss: 0.5986 - accuracy: 0.7963\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.6420 - accuracy: 0.7784\n",
      "Epoch 33/100\n",
      "10000/10000 [==============================] - 9s 852us/sample - loss: 0.7340 - accuracy: 0.7556\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.6271 - accuracy: 0.7810\n",
      "Epoch 34/100\n",
      "10000/10000 [==============================] - 9s 851us/sample - loss: 0.7245 - accuracy: 0.7557\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.6162 - accuracy: 0.7870\n",
      "Epoch 35/100\n",
      "10000/10000 [==============================] - 9s 851us/sample - loss: 0.7014 - accuracy: 0.7647\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.6099 - accuracy: 0.7882\n",
      "Epoch 36/100\n",
      "10000/10000 [==============================] - 9s 852us/sample - loss: 0.6298 - accuracy: 0.7878\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.5978 - accuracy: 0.7933\n",
      "Epoch 37/100\n",
      "10000/10000 [==============================] - 9s 854us/sample - loss: 1.5263 - accuracy: 0.5454\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.5890 - accuracy: 0.7951\n",
      "Epoch 38/100\n",
      "10000/10000 [==============================] - 9s 852us/sample - loss: 0.6315 - accuracy: 0.7881\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.5798 - accuracy: 0.7976\n",
      "Epoch 39/100\n",
      "10000/10000 [==============================] - 9s 855us/sample - loss: 0.8677 - accuracy: 0.7325\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.5709 - accuracy: 0.7999\n",
      "Epoch 40/100\n",
      "10000/10000 [==============================] - 9s 857us/sample - loss: 0.7048 - accuracy: 0.7756\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.5518 - accuracy: 0.8095\n",
      "Epoch 41/100\n",
      "10000/10000 [==============================] - 9s 852us/sample - loss: 0.5878 - accuracy: 0.8017\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.5470 - accuracy: 0.8107\n",
      "Epoch 42/100\n",
      "10000/10000 [==============================] - 8s 850us/sample - loss: 0.6158 - accuracy: 0.7942\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.5383 - accuracy: 0.8150\n",
      "Epoch 43/100\n",
      "10000/10000 [==============================] - 8s 847us/sample - loss: 0.5531 - accuracy: 0.8080\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.5275 - accuracy: 0.8158\n",
      "Epoch 44/100\n",
      "10000/10000 [==============================] - 9s 853us/sample - loss: 0.6034 - accuracy: 0.7974\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.5160 - accuracy: 0.8203\n",
      "Epoch 45/100\n",
      "10000/10000 [==============================] - 9s 851us/sample - loss: 0.5798 - accuracy: 0.8036\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.5051 - accuracy: 0.8250\n",
      "Epoch 46/100\n",
      "10000/10000 [==============================] - 9s 850us/sample - loss: 0.7966 - accuracy: 0.7504\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.5019 - accuracy: 0.8253\n",
      "Epoch 47/100\n",
      "10000/10000 [==============================] - 8s 848us/sample - loss: 0.4937 - accuracy: 0.8342\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.4900 - accuracy: 0.8299\n",
      "Epoch 48/100\n",
      "10000/10000 [==============================] - 9s 855us/sample - loss: 0.4882 - accuracy: 0.8373\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.4852 - accuracy: 0.8321\n",
      "Epoch 49/100\n",
      "10000/10000 [==============================] - 8s 849us/sample - loss: 0.6639 - accuracy: 0.7908\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.4767 - accuracy: 0.8351\n",
      "Epoch 50/100\n",
      "10000/10000 [==============================] - 9s 855us/sample - loss: 0.5035 - accuracy: 0.8308\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.4675 - accuracy: 0.8356\n",
      "Epoch 51/100\n",
      "10000/10000 [==============================] - 9s 867us/sample - loss: 0.6180 - accuracy: 0.7962\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.4669 - accuracy: 0.8378\n",
      "Epoch 52/100\n",
      "10000/10000 [==============================] - 9s 851us/sample - loss: 0.6000 - accuracy: 0.8087\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.4595 - accuracy: 0.8416\n",
      "Epoch 53/100\n",
      "10000/10000 [==============================] - 8s 849us/sample - loss: 0.5046 - accuracy: 0.8283\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.4544 - accuracy: 0.8427\n",
      "Epoch 54/100\n",
      "10000/10000 [==============================] - 8s 850us/sample - loss: 0.6193 - accuracy: 0.7927\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.4390 - accuracy: 0.8483\n",
      "Epoch 55/100\n",
      "10000/10000 [==============================] - 9s 856us/sample - loss: 0.5304 - accuracy: 0.8298\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.4393 - accuracy: 0.8459\n",
      "Epoch 56/100\n",
      "10000/10000 [==============================] - 9s 858us/sample - loss: 0.5974 - accuracy: 0.8076\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.4264 - accuracy: 0.8531\n",
      "Epoch 57/100\n",
      "10000/10000 [==============================] - 9s 852us/sample - loss: 0.4540 - accuracy: 0.8513\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.4254 - accuracy: 0.8532\n",
      "Epoch 58/100\n",
      "10000/10000 [==============================] - 9s 852us/sample - loss: 0.4814 - accuracy: 0.8442\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.4265 - accuracy: 0.8504\n",
      "Epoch 59/100\n",
      "10000/10000 [==============================] - 9s 851us/sample - loss: 0.6743 - accuracy: 0.7901\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.4165 - accuracy: 0.8547\n",
      "Epoch 60/100\n",
      "10000/10000 [==============================] - 9s 851us/sample - loss: 0.4883 - accuracy: 0.8398\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.4052 - accuracy: 0.8596\n",
      "Epoch 61/100\n",
      "10000/10000 [==============================] - 9s 851us/sample - loss: 0.8902 - accuracy: 0.7449\n",
      "782/782 [==============================] - 114s 145ms/step - loss: 0.3987 - accuracy: 0.8613\n",
      "Epoch 62/100\n",
      "10000/10000 [==============================] - 9s 854us/sample - loss: 0.4809 - accuracy: 0.8416\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.4020 - accuracy: 0.8596\n",
      "Epoch 63/100\n",
      "10000/10000 [==============================] - 8s 849us/sample - loss: 0.5456 - accuracy: 0.8249\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.3942 - accuracy: 0.8624\n",
      "Epoch 64/100\n",
      "10000/10000 [==============================] - 9s 857us/sample - loss: 0.5242 - accuracy: 0.8290\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.3843 - accuracy: 0.8647\n",
      "Epoch 65/100\n",
      "10000/10000 [==============================] - 9s 854us/sample - loss: 0.4718 - accuracy: 0.8452\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.3804 - accuracy: 0.8676\n",
      "Epoch 66/100\n",
      "10000/10000 [==============================] - 9s 851us/sample - loss: 0.7055 - accuracy: 0.7879\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.3770 - accuracy: 0.8685\n",
      "Epoch 67/100\n",
      "10000/10000 [==============================] - 9s 855us/sample - loss: 0.4426 - accuracy: 0.8568\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.3691 - accuracy: 0.8725\n",
      "Epoch 68/100\n",
      "10000/10000 [==============================] - 9s 852us/sample - loss: 0.5784 - accuracy: 0.8143\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.3664 - accuracy: 0.8725\n",
      "Epoch 69/100\n",
      "10000/10000 [==============================] - 9s 852us/sample - loss: 0.5039 - accuracy: 0.8380\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.3610 - accuracy: 0.8752\n",
      "Epoch 70/100\n",
      "10000/10000 [==============================] - 9s 852us/sample - loss: 0.4184 - accuracy: 0.8639\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.3574 - accuracy: 0.8765\n",
      "Epoch 71/100\n",
      "10000/10000 [==============================] - 9s 854us/sample - loss: 0.4658 - accuracy: 0.8516\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.3534 - accuracy: 0.8766\n",
      "Epoch 72/100\n",
      "10000/10000 [==============================] - 9s 860us/sample - loss: 0.6375 - accuracy: 0.8174\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.3493 - accuracy: 0.8781\n",
      "Epoch 73/100\n",
      "10000/10000 [==============================] - 9s 857us/sample - loss: 0.4421 - accuracy: 0.8550\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.3439 - accuracy: 0.8798\n",
      "Epoch 74/100\n",
      "10000/10000 [==============================] - 9s 853us/sample - loss: 0.4818 - accuracy: 0.8461\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.3384 - accuracy: 0.8808\n",
      "Epoch 75/100\n",
      "10000/10000 [==============================] - 9s 851us/sample - loss: 0.4547 - accuracy: 0.8498\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.3348 - accuracy: 0.8834\n",
      "Epoch 76/100\n",
      "10000/10000 [==============================] - 9s 855us/sample - loss: 0.4917 - accuracy: 0.8463\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.3358 - accuracy: 0.8832\n",
      "Epoch 77/100\n",
      "10000/10000 [==============================] - 9s 854us/sample - loss: 0.4529 - accuracy: 0.8510\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.3289 - accuracy: 0.8854\n",
      "Epoch 78/100\n",
      "10000/10000 [==============================] - 9s 851us/sample - loss: 0.5014 - accuracy: 0.8421\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.3254 - accuracy: 0.8866\n",
      "Epoch 79/100\n",
      "10000/10000 [==============================] - 9s 850us/sample - loss: 0.4472 - accuracy: 0.8552\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.3199 - accuracy: 0.8880\n",
      "Epoch 80/100\n",
      "10000/10000 [==============================] - 9s 860us/sample - loss: 0.4572 - accuracy: 0.8544\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.3126 - accuracy: 0.8912\n",
      "Epoch 81/100\n",
      "10000/10000 [==============================] - 9s 879us/sample - loss: 0.4493 - accuracy: 0.8583\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.3118 - accuracy: 0.8904\n",
      "Epoch 82/100\n",
      "10000/10000 [==============================] - 9s 851us/sample - loss: 1.0109 - accuracy: 0.7316\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.3075 - accuracy: 0.8926\n",
      "Epoch 83/100\n",
      "10000/10000 [==============================] - 9s 853us/sample - loss: 0.4438 - accuracy: 0.8546\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.3043 - accuracy: 0.8925\n",
      "Epoch 84/100\n",
      "10000/10000 [==============================] - 9s 855us/sample - loss: 0.5138 - accuracy: 0.8411\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.3044 - accuracy: 0.8942\n",
      "Epoch 85/100\n",
      "10000/10000 [==============================] - 9s 854us/sample - loss: 0.5409 - accuracy: 0.8433\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.2926 - accuracy: 0.8984\n",
      "Epoch 86/100\n",
      "10000/10000 [==============================] - 9s 852us/sample - loss: 0.4271 - accuracy: 0.8657\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.2913 - accuracy: 0.8975\n",
      "Epoch 87/100\n",
      "10000/10000 [==============================] - 9s 858us/sample - loss: 0.4279 - accuracy: 0.8633\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.2912 - accuracy: 0.8990\n",
      "Epoch 88/100\n",
      "10000/10000 [==============================] - 9s 857us/sample - loss: 0.4137 - accuracy: 0.8655\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.2862 - accuracy: 0.8998\n",
      "Epoch 89/100\n",
      "10000/10000 [==============================] - 9s 858us/sample - loss: 0.4834 - accuracy: 0.8533\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.2854 - accuracy: 0.8991\n",
      "Epoch 90/100\n",
      "10000/10000 [==============================] - 9s 854us/sample - loss: 0.4031 - accuracy: 0.8719\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.2751 - accuracy: 0.9025\n",
      "Epoch 91/100\n",
      "10000/10000 [==============================] - 9s 857us/sample - loss: 0.4860 - accuracy: 0.8504\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.2761 - accuracy: 0.9029\n",
      "Epoch 92/100\n",
      "10000/10000 [==============================] - 9s 856us/sample - loss: 0.3898 - accuracy: 0.8763\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.2689 - accuracy: 0.9054\n",
      "Epoch 93/100\n",
      "10000/10000 [==============================] - 9s 852us/sample - loss: 0.5158 - accuracy: 0.8512\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.2715 - accuracy: 0.9043\n",
      "Epoch 94/100\n",
      "10000/10000 [==============================] - 9s 851us/sample - loss: 0.4609 - accuracy: 0.8575\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.2713 - accuracy: 0.9053\n",
      "Epoch 95/100\n",
      "10000/10000 [==============================] - 9s 855us/sample - loss: 0.4803 - accuracy: 0.8599\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.2644 - accuracy: 0.9084\n",
      "Epoch 96/100\n",
      "10000/10000 [==============================] - 9s 859us/sample - loss: 0.4028 - accuracy: 0.8726\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.2638 - accuracy: 0.9065\n",
      "Epoch 97/100\n",
      "10000/10000 [==============================] - 9s 855us/sample - loss: 0.4629 - accuracy: 0.8585\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.2594 - accuracy: 0.9091\n",
      "Epoch 98/100\n",
      "10000/10000 [==============================] - 9s 866us/sample - loss: 0.4667 - accuracy: 0.8608\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.2562 - accuracy: 0.9095\n",
      "Epoch 99/100\n",
      "10000/10000 [==============================] - 9s 855us/sample - loss: 0.6345 - accuracy: 0.8192\n",
      "782/782 [==============================] - 115s 146ms/step - loss: 0.2536 - accuracy: 0.9106\n",
      "Epoch 100/100\n",
      "10000/10000 [==============================] - 9s 855us/sample - loss: 0.3856 - accuracy: 0.8766\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.2468 - accuracy: 0.9138\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(datagen.flow(xtrain, ytrain, batch_size=64),epochs=100,callbacks=[TestCallback((xtest, ytest))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 700us/sample - loss: 0.3856 - accuracy: 0.8766\n"
     ]
    }
   ],
   "source": [
    "eval_hist = model.evaluate(xtest,ytest,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "#  \"Accuracy\"\n",
    "plt.plot(history.history['accuracy'])\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Resnet50withfractionpooling(input_tensor):\n",
    "   \n",
    "    X = Conv2D(64,(3,3))(input_tensor)\n",
    "    X = BatchNormalization()(X)\n",
    "    \n",
    "    X = MaxPooling2D((2,2))(X)\n",
    "    \n",
    "    X = identity_block(64,X)\n",
    "    X = identity_block(64,X)\n",
    "    X = identity_block(64,X)\n",
    "    \n",
    "    X = tf.nn.fractional_max_pool(X, [1.0, 1.41, 1.41, 1.0], pseudo_random=True, overlapping=True)[0]\n",
    "    \n",
    "    X = conv_block(128,X)\n",
    "    X = identity_block(128,X)\n",
    "    X = identity_block(128,X)\n",
    "    X = identity_block(128,X)\n",
    "    \n",
    "    \n",
    "    X = tf.nn.fractional_max_pool(X, [1.0, 1.41, 1.41, 1.0], pseudo_random=True, overlapping=True)[0]\n",
    "    \n",
    "    \n",
    "    X = conv_block(256,X)\n",
    "    X = identity_block(256,X)\n",
    "    X = identity_block(256,X)\n",
    "    X = identity_block(256,X)\n",
    "    X = identity_block(256,X)\n",
    "    X = identity_block(256,X)\n",
    "    \n",
    "    \n",
    "    X = tf.nn.fractional_max_pool(X, [1.0, 1.2, 1.2, 1.0], pseudo_random=True, overlapping=True)[0]\n",
    "    \n",
    "    X = conv_block(512,X)\n",
    "    X = identity_block(512,X)\n",
    "    X = identity_block(512,X)\n",
    "    \n",
    "    X = tf.nn.fractional_max_pool(X, [1.0, 1.41, 1.41, 1.0], pseudo_random=True, overlapping=True)[0]\n",
    "    \n",
    "    X = Flatten()(X)\n",
    "    X = BatchNormalization()(X)\n",
    "\n",
    "    X = Dense(1024, activation='relu')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    \n",
    "    X = Dense(512, activation='relu')(X)\n",
    "    \n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dense(10, activation = 'softmax')(X)\n",
    "    \n",
    "    \n",
    "    return X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_21\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_23 (InputLayer)           [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_758 (Conv2D)             (None, 30, 30, 64)   1792        input_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_799 (BatchN (None, 30, 30, 64)   256         conv2d_758[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling2D) (None, 15, 15, 64)   0           batch_normalization_799[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_759 (Conv2D)             (None, 15, 15, 64)   36928       max_pooling2d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_800 (BatchN (None, 15, 15, 64)   256         conv2d_759[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_672 (TensorFlo [(None, 15, 15, 64)] 0           batch_normalization_800[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_760 (Conv2D)             (None, 15, 15, 64)   36928       tf_op_layer_Relu_672[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_801 (BatchN (None, 15, 15, 64)   256         conv2d_760[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_336 (TensorFlow [(None, 15, 15, 64)] 0           batch_normalization_801[0][0]    \n",
      "                                                                 max_pooling2d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_673 (TensorFlo [(None, 15, 15, 64)] 0           tf_op_layer_add_336[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_761 (Conv2D)             (None, 15, 15, 64)   36928       tf_op_layer_Relu_673[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_802 (BatchN (None, 15, 15, 64)   256         conv2d_761[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_674 (TensorFlo [(None, 15, 15, 64)] 0           batch_normalization_802[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_762 (Conv2D)             (None, 15, 15, 64)   36928       tf_op_layer_Relu_674[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_803 (BatchN (None, 15, 15, 64)   256         conv2d_762[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_337 (TensorFlow [(None, 15, 15, 64)] 0           batch_normalization_803[0][0]    \n",
      "                                                                 tf_op_layer_Relu_673[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_675 (TensorFlo [(None, 15, 15, 64)] 0           tf_op_layer_add_337[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_763 (Conv2D)             (None, 15, 15, 64)   36928       tf_op_layer_Relu_675[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_804 (BatchN (None, 15, 15, 64)   256         conv2d_763[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_676 (TensorFlo [(None, 15, 15, 64)] 0           batch_normalization_804[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_764 (Conv2D)             (None, 15, 15, 64)   36928       tf_op_layer_Relu_676[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_805 (BatchN (None, 15, 15, 64)   256         conv2d_764[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_338 (TensorFlow [(None, 15, 15, 64)] 0           batch_normalization_805[0][0]    \n",
      "                                                                 tf_op_layer_Relu_675[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_677 (TensorFlo [(None, 15, 15, 64)] 0           tf_op_layer_add_338[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_FractionalMaxPool_4 [(None, 10, 10, 64), 0           tf_op_layer_Relu_677[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_765 (Conv2D)             (None, 10, 10, 128)  73856       tf_op_layer_FractionalMaxPool_45[\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_806 (BatchN (None, 10, 10, 128)  512         conv2d_765[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_678 (TensorFlo [(None, 10, 10, 128) 0           batch_normalization_806[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_766 (Conv2D)             (None, 10, 10, 128)  147584      tf_op_layer_Relu_678[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_767 (Conv2D)             (None, 10, 10, 128)  73856       tf_op_layer_FractionalMaxPool_45[\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_807 (BatchN (None, 10, 10, 128)  512         conv2d_766[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_808 (BatchN (None, 10, 10, 128)  512         conv2d_767[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_339 (TensorFlow [(None, 10, 10, 128) 0           batch_normalization_807[0][0]    \n",
      "                                                                 batch_normalization_808[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_679 (TensorFlo [(None, 10, 10, 128) 0           tf_op_layer_add_339[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_768 (Conv2D)             (None, 10, 10, 128)  147584      tf_op_layer_Relu_679[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_809 (BatchN (None, 10, 10, 128)  512         conv2d_768[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_680 (TensorFlo [(None, 10, 10, 128) 0           batch_normalization_809[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_769 (Conv2D)             (None, 10, 10, 128)  147584      tf_op_layer_Relu_680[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_810 (BatchN (None, 10, 10, 128)  512         conv2d_769[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_340 (TensorFlow [(None, 10, 10, 128) 0           batch_normalization_810[0][0]    \n",
      "                                                                 tf_op_layer_Relu_679[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_681 (TensorFlo [(None, 10, 10, 128) 0           tf_op_layer_add_340[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_770 (Conv2D)             (None, 10, 10, 128)  147584      tf_op_layer_Relu_681[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_811 (BatchN (None, 10, 10, 128)  512         conv2d_770[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_682 (TensorFlo [(None, 10, 10, 128) 0           batch_normalization_811[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_771 (Conv2D)             (None, 10, 10, 128)  147584      tf_op_layer_Relu_682[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_812 (BatchN (None, 10, 10, 128)  512         conv2d_771[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_341 (TensorFlow [(None, 10, 10, 128) 0           batch_normalization_812[0][0]    \n",
      "                                                                 tf_op_layer_Relu_681[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_683 (TensorFlo [(None, 10, 10, 128) 0           tf_op_layer_add_341[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_772 (Conv2D)             (None, 10, 10, 128)  147584      tf_op_layer_Relu_683[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_813 (BatchN (None, 10, 10, 128)  512         conv2d_772[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_684 (TensorFlo [(None, 10, 10, 128) 0           batch_normalization_813[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_773 (Conv2D)             (None, 10, 10, 128)  147584      tf_op_layer_Relu_684[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_814 (BatchN (None, 10, 10, 128)  512         conv2d_773[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_342 (TensorFlow [(None, 10, 10, 128) 0           batch_normalization_814[0][0]    \n",
      "                                                                 tf_op_layer_Relu_683[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_685 (TensorFlo [(None, 10, 10, 128) 0           tf_op_layer_add_342[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_FractionalMaxPool_4 [(None, 7, 7, 128),  0           tf_op_layer_Relu_685[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_774 (Conv2D)             (None, 7, 7, 256)    295168      tf_op_layer_FractionalMaxPool_46[\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_815 (BatchN (None, 7, 7, 256)    1024        conv2d_774[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_686 (TensorFlo [(None, 7, 7, 256)]  0           batch_normalization_815[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_775 (Conv2D)             (None, 7, 7, 256)    590080      tf_op_layer_Relu_686[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_776 (Conv2D)             (None, 7, 7, 256)    295168      tf_op_layer_FractionalMaxPool_46[\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_816 (BatchN (None, 7, 7, 256)    1024        conv2d_775[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_817 (BatchN (None, 7, 7, 256)    1024        conv2d_776[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_343 (TensorFlow [(None, 7, 7, 256)]  0           batch_normalization_816[0][0]    \n",
      "                                                                 batch_normalization_817[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_687 (TensorFlo [(None, 7, 7, 256)]  0           tf_op_layer_add_343[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_777 (Conv2D)             (None, 7, 7, 256)    590080      tf_op_layer_Relu_687[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_818 (BatchN (None, 7, 7, 256)    1024        conv2d_777[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_688 (TensorFlo [(None, 7, 7, 256)]  0           batch_normalization_818[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_778 (Conv2D)             (None, 7, 7, 256)    590080      tf_op_layer_Relu_688[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_819 (BatchN (None, 7, 7, 256)    1024        conv2d_778[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_344 (TensorFlow [(None, 7, 7, 256)]  0           batch_normalization_819[0][0]    \n",
      "                                                                 tf_op_layer_Relu_687[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_689 (TensorFlo [(None, 7, 7, 256)]  0           tf_op_layer_add_344[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_779 (Conv2D)             (None, 7, 7, 256)    590080      tf_op_layer_Relu_689[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_820 (BatchN (None, 7, 7, 256)    1024        conv2d_779[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_690 (TensorFlo [(None, 7, 7, 256)]  0           batch_normalization_820[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_780 (Conv2D)             (None, 7, 7, 256)    590080      tf_op_layer_Relu_690[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_821 (BatchN (None, 7, 7, 256)    1024        conv2d_780[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_345 (TensorFlow [(None, 7, 7, 256)]  0           batch_normalization_821[0][0]    \n",
      "                                                                 tf_op_layer_Relu_689[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_691 (TensorFlo [(None, 7, 7, 256)]  0           tf_op_layer_add_345[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_781 (Conv2D)             (None, 7, 7, 256)    590080      tf_op_layer_Relu_691[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_822 (BatchN (None, 7, 7, 256)    1024        conv2d_781[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_692 (TensorFlo [(None, 7, 7, 256)]  0           batch_normalization_822[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_782 (Conv2D)             (None, 7, 7, 256)    590080      tf_op_layer_Relu_692[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_823 (BatchN (None, 7, 7, 256)    1024        conv2d_782[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_346 (TensorFlow [(None, 7, 7, 256)]  0           batch_normalization_823[0][0]    \n",
      "                                                                 tf_op_layer_Relu_691[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_693 (TensorFlo [(None, 7, 7, 256)]  0           tf_op_layer_add_346[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_783 (Conv2D)             (None, 7, 7, 256)    590080      tf_op_layer_Relu_693[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_824 (BatchN (None, 7, 7, 256)    1024        conv2d_783[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_694 (TensorFlo [(None, 7, 7, 256)]  0           batch_normalization_824[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_784 (Conv2D)             (None, 7, 7, 256)    590080      tf_op_layer_Relu_694[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_825 (BatchN (None, 7, 7, 256)    1024        conv2d_784[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_347 (TensorFlow [(None, 7, 7, 256)]  0           batch_normalization_825[0][0]    \n",
      "                                                                 tf_op_layer_Relu_693[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_695 (TensorFlo [(None, 7, 7, 256)]  0           tf_op_layer_add_347[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_785 (Conv2D)             (None, 7, 7, 256)    590080      tf_op_layer_Relu_695[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_826 (BatchN (None, 7, 7, 256)    1024        conv2d_785[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_696 (TensorFlo [(None, 7, 7, 256)]  0           batch_normalization_826[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_786 (Conv2D)             (None, 7, 7, 256)    590080      tf_op_layer_Relu_696[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_827 (BatchN (None, 7, 7, 256)    1024        conv2d_786[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_348 (TensorFlow [(None, 7, 7, 256)]  0           batch_normalization_827[0][0]    \n",
      "                                                                 tf_op_layer_Relu_695[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_697 (TensorFlo [(None, 7, 7, 256)]  0           tf_op_layer_add_348[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_FractionalMaxPool_4 [(None, 5, 5, 256),  0           tf_op_layer_Relu_697[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_787 (Conv2D)             (None, 5, 5, 512)    1180160     tf_op_layer_FractionalMaxPool_47[\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_828 (BatchN (None, 5, 5, 512)    2048        conv2d_787[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_698 (TensorFlo [(None, 5, 5, 512)]  0           batch_normalization_828[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_788 (Conv2D)             (None, 5, 5, 512)    2359808     tf_op_layer_Relu_698[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_789 (Conv2D)             (None, 5, 5, 512)    1180160     tf_op_layer_FractionalMaxPool_47[\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_829 (BatchN (None, 5, 5, 512)    2048        conv2d_788[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_830 (BatchN (None, 5, 5, 512)    2048        conv2d_789[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_349 (TensorFlow [(None, 5, 5, 512)]  0           batch_normalization_829[0][0]    \n",
      "                                                                 batch_normalization_830[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_699 (TensorFlo [(None, 5, 5, 512)]  0           tf_op_layer_add_349[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_790 (Conv2D)             (None, 5, 5, 512)    2359808     tf_op_layer_Relu_699[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_831 (BatchN (None, 5, 5, 512)    2048        conv2d_790[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_700 (TensorFlo [(None, 5, 5, 512)]  0           batch_normalization_831[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_791 (Conv2D)             (None, 5, 5, 512)    2359808     tf_op_layer_Relu_700[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_832 (BatchN (None, 5, 5, 512)    2048        conv2d_791[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_350 (TensorFlow [(None, 5, 5, 512)]  0           batch_normalization_832[0][0]    \n",
      "                                                                 tf_op_layer_Relu_699[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_701 (TensorFlo [(None, 5, 5, 512)]  0           tf_op_layer_add_350[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_792 (Conv2D)             (None, 5, 5, 512)    2359808     tf_op_layer_Relu_701[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_833 (BatchN (None, 5, 5, 512)    2048        conv2d_792[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_702 (TensorFlo [(None, 5, 5, 512)]  0           batch_normalization_833[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_793 (Conv2D)             (None, 5, 5, 512)    2359808     tf_op_layer_Relu_702[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_834 (BatchN (None, 5, 5, 512)    2048        conv2d_793[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_351 (TensorFlow [(None, 5, 5, 512)]  0           batch_normalization_834[0][0]    \n",
      "                                                                 tf_op_layer_Relu_701[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_703 (TensorFlo [(None, 5, 5, 512)]  0           tf_op_layer_add_351[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_FractionalMaxPool_4 [(None, 3, 3, 512),  0           tf_op_layer_Relu_703[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 4608)         0           tf_op_layer_FractionalMaxPool_48[\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_835 (BatchN (None, 4608)         18432       flatten_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_53 (Dense)                (None, 1000)         4609000     batch_normalization_835[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_43 (Dropout)            (None, 1000)         0           dense_53[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_836 (BatchN (None, 1000)         4000        dropout_43[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_54 (Dense)                (None, 10)           10010       batch_normalization_836[0][0]    \n",
      "==================================================================================================\n",
      "Total params: 27,320,226\n",
      "Trainable params: 27,291,986\n",
      "Non-trainable params: 28,240\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_input = Input(shape=(32,32,3))\n",
    "output = Resnet34withfractionpooling(img_input)\n",
    "model2 = Model(img_input,output)\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "first100trainacc = train_acc \n",
    "print(history.history.keys())\n",
    "#  \"Accuracy\"\n",
    "\n",
    "plt.plot(history2.history['accuracy'])\n",
    "plt.plot(first100trainacc)\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 782 steps\n",
      "Epoch 1/100\n",
      "10000/10000 [==============================] - 6s 556us/sample - loss: 0.3903 - accuracy: 0.8915\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.2036 - accuracy: 0.9300\n",
      "Epoch 2/100\n",
      "10000/10000 [==============================] - 6s 556us/sample - loss: 0.4690 - accuracy: 0.8597\n",
      "782/782 [==============================] - 79s 101ms/step - loss: 0.1930 - accuracy: 0.9320\n",
      "Epoch 3/100\n",
      "10000/10000 [==============================] - 6s 556us/sample - loss: 0.3993 - accuracy: 0.8877\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1930 - accuracy: 0.9327\n",
      "Epoch 4/100\n",
      "10000/10000 [==============================] - 6s 556us/sample - loss: 0.3575 - accuracy: 0.8947\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1913 - accuracy: 0.9331\n",
      "Epoch 5/100\n",
      "10000/10000 [==============================] - 6s 555us/sample - loss: 0.4943 - accuracy: 0.8645\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1856 - accuracy: 0.9369\n",
      "Epoch 6/100\n",
      "10000/10000 [==============================] - 6s 556us/sample - loss: 0.4284 - accuracy: 0.8773\n",
      "782/782 [==============================] - 79s 100ms/step - loss: 0.1897 - accuracy: 0.9335\n",
      "Epoch 7/100\n",
      "10000/10000 [==============================] - 6s 556us/sample - loss: 0.4653 - accuracy: 0.8722\n",
      "782/782 [==============================] - 79s 101ms/step - loss: 0.1845 - accuracy: 0.9354\n",
      "Epoch 8/100\n",
      "10000/10000 [==============================] - 6s 554us/sample - loss: 0.3630 - accuracy: 0.8923\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1726 - accuracy: 0.9402\n",
      "Epoch 9/100\n",
      "10000/10000 [==============================] - 6s 553us/sample - loss: 0.4749 - accuracy: 0.8702\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1770 - accuracy: 0.9384\n",
      "Epoch 10/100\n",
      "10000/10000 [==============================] - 6s 556us/sample - loss: 0.4410 - accuracy: 0.8765\n",
      "782/782 [==============================] - 79s 100ms/step - loss: 0.1729 - accuracy: 0.9403\n",
      "Epoch 11/100\n",
      "10000/10000 [==============================] - 6s 555us/sample - loss: 0.4489 - accuracy: 0.8750\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1758 - accuracy: 0.9399\n",
      "Epoch 12/100\n",
      "10000/10000 [==============================] - 6s 551us/sample - loss: 0.4843 - accuracy: 0.8761\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1726 - accuracy: 0.9410\n",
      "Epoch 13/100\n",
      "10000/10000 [==============================] - 6s 554us/sample - loss: 0.3748 - accuracy: 0.8916\n",
      "782/782 [==============================] - 78s 99ms/step - loss: 0.1733 - accuracy: 0.9400\n",
      "Epoch 14/100\n",
      "10000/10000 [==============================] - 6s 553us/sample - loss: 0.4394 - accuracy: 0.8785\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1688 - accuracy: 0.9419\n",
      "Epoch 15/100\n",
      "10000/10000 [==============================] - 6s 553us/sample - loss: 0.4139 - accuracy: 0.8860\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1636 - accuracy: 0.9436\n",
      "Epoch 16/100\n",
      "10000/10000 [==============================] - 6s 555us/sample - loss: 0.3637 - accuracy: 0.8963\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1666 - accuracy: 0.9417\n",
      "Epoch 17/100\n",
      "10000/10000 [==============================] - 6s 556us/sample - loss: 0.4233 - accuracy: 0.8854\n",
      "782/782 [==============================] - 78s 99ms/step - loss: 0.1609 - accuracy: 0.9438\n",
      "Epoch 18/100\n",
      "10000/10000 [==============================] - 6s 558us/sample - loss: 0.4221 - accuracy: 0.8765\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1569 - accuracy: 0.9451\n",
      "Epoch 19/100\n",
      "10000/10000 [==============================] - 6s 556us/sample - loss: 0.6110 - accuracy: 0.8472\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1586 - accuracy: 0.9443\n",
      "Epoch 20/100\n",
      "10000/10000 [==============================] - 6s 553us/sample - loss: 0.3769 - accuracy: 0.8936\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1570 - accuracy: 0.9458\n",
      "Epoch 21/100\n",
      "10000/10000 [==============================] - 6s 560us/sample - loss: 0.5449 - accuracy: 0.8641\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1584 - accuracy: 0.9447\n",
      "Epoch 22/100\n",
      "10000/10000 [==============================] - 6s 557us/sample - loss: 0.4180 - accuracy: 0.8898\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1542 - accuracy: 0.9468\n",
      "Epoch 23/100\n",
      "10000/10000 [==============================] - 6s 556us/sample - loss: 0.4108 - accuracy: 0.8961\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1551 - accuracy: 0.9471\n",
      "Epoch 24/100\n",
      "10000/10000 [==============================] - 6s 553us/sample - loss: 0.3653 - accuracy: 0.8990\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1534 - accuracy: 0.9466\n",
      "Epoch 25/100\n",
      "10000/10000 [==============================] - 6s 556us/sample - loss: 0.4156 - accuracy: 0.8838\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1551 - accuracy: 0.9455\n",
      "Epoch 26/100\n",
      "10000/10000 [==============================] - 6s 554us/sample - loss: 0.4173 - accuracy: 0.8898\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1509 - accuracy: 0.9471\n",
      "Epoch 27/100\n",
      "10000/10000 [==============================] - 6s 556us/sample - loss: 0.3905 - accuracy: 0.8930\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1471 - accuracy: 0.9486\n",
      "Epoch 28/100\n",
      "10000/10000 [==============================] - 6s 557us/sample - loss: 0.4859 - accuracy: 0.8785\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1473 - accuracy: 0.9499\n",
      "Epoch 29/100\n",
      "10000/10000 [==============================] - 6s 578us/sample - loss: 0.3992 - accuracy: 0.8944\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1418 - accuracy: 0.9500\n",
      "Epoch 30/100\n",
      "10000/10000 [==============================] - 6s 558us/sample - loss: 0.4319 - accuracy: 0.8897\n",
      "782/782 [==============================] - 79s 101ms/step - loss: 0.1475 - accuracy: 0.9496\n",
      "Epoch 31/100\n",
      "10000/10000 [==============================] - 6s 556us/sample - loss: 0.4379 - accuracy: 0.8854\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1438 - accuracy: 0.9500\n",
      "Epoch 32/100\n",
      "10000/10000 [==============================] - 6s 555us/sample - loss: 0.4962 - accuracy: 0.8760\n",
      "782/782 [==============================] - 78s 99ms/step - loss: 0.1397 - accuracy: 0.9508\n",
      "Epoch 33/100\n",
      "10000/10000 [==============================] - 6s 582us/sample - loss: 0.4062 - accuracy: 0.8923\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1395 - accuracy: 0.9519\n",
      "Epoch 34/100\n",
      "10000/10000 [==============================] - 6s 558us/sample - loss: 0.3885 - accuracy: 0.8954\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1402 - accuracy: 0.9509\n",
      "Epoch 35/100\n",
      "10000/10000 [==============================] - 6s 557us/sample - loss: 0.3902 - accuracy: 0.8980\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1361 - accuracy: 0.9518\n",
      "Epoch 36/100\n",
      "10000/10000 [==============================] - 6s 557us/sample - loss: 0.4919 - accuracy: 0.8757\n",
      "782/782 [==============================] - 78s 99ms/step - loss: 0.1359 - accuracy: 0.9534\n",
      "Epoch 37/100\n",
      "10000/10000 [==============================] - 6s 584us/sample - loss: 0.3855 - accuracy: 0.9005\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1352 - accuracy: 0.9535\n",
      "Epoch 38/100\n",
      "10000/10000 [==============================] - 6s 558us/sample - loss: 0.3977 - accuracy: 0.8924\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1346 - accuracy: 0.9534\n",
      "Epoch 39/100\n",
      "10000/10000 [==============================] - 6s 553us/sample - loss: 0.3908 - accuracy: 0.8956\n",
      "782/782 [==============================] - 80s 102ms/step - loss: 0.1278 - accuracy: 0.9555\n",
      "Epoch 40/100\n",
      "10000/10000 [==============================] - 6s 554us/sample - loss: 0.4044 - accuracy: 0.8920\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1353 - accuracy: 0.9522\n",
      "Epoch 41/100\n",
      "10000/10000 [==============================] - 6s 596us/sample - loss: 0.4625 - accuracy: 0.8827\n",
      "782/782 [==============================] - 79s 101ms/step - loss: 0.1297 - accuracy: 0.9543\n",
      "Epoch 42/100\n",
      "10000/10000 [==============================] - 6s 554us/sample - loss: 0.4781 - accuracy: 0.8851\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1312 - accuracy: 0.9540\n",
      "Epoch 43/100\n",
      "10000/10000 [==============================] - 6s 554us/sample - loss: 0.4418 - accuracy: 0.8875\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1294 - accuracy: 0.9552\n",
      "Epoch 44/100\n",
      "10000/10000 [==============================] - 6s 552us/sample - loss: 0.4733 - accuracy: 0.8856\n",
      "782/782 [==============================] - 78s 99ms/step - loss: 0.1261 - accuracy: 0.9564\n",
      "Epoch 45/100\n",
      "10000/10000 [==============================] - 6s 593us/sample - loss: 0.4693 - accuracy: 0.8736\n",
      "782/782 [==============================] - 79s 101ms/step - loss: 0.1240 - accuracy: 0.9568\n",
      "Epoch 46/100\n",
      "10000/10000 [==============================] - 6s 555us/sample - loss: 0.4394 - accuracy: 0.8884\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1283 - accuracy: 0.9549\n",
      "Epoch 47/100\n",
      "10000/10000 [==============================] - 6s 555us/sample - loss: 0.3897 - accuracy: 0.8955\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1253 - accuracy: 0.9562\n",
      "Epoch 48/100\n",
      "10000/10000 [==============================] - 6s 553us/sample - loss: 0.3743 - accuracy: 0.9015\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1317 - accuracy: 0.9546\n",
      "Epoch 49/100\n",
      "10000/10000 [==============================] - 6s 577us/sample - loss: 0.4689 - accuracy: 0.8872\n",
      "782/782 [==============================] - 79s 101ms/step - loss: 0.1184 - accuracy: 0.9588\n",
      "Epoch 50/100\n",
      "10000/10000 [==============================] - 6s 556us/sample - loss: 0.3930 - accuracy: 0.8944\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1221 - accuracy: 0.9577\n",
      "Epoch 51/100\n",
      "10000/10000 [==============================] - 6s 554us/sample - loss: 0.4294 - accuracy: 0.8914\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1180 - accuracy: 0.9577\n",
      "Epoch 52/100\n",
      "10000/10000 [==============================] - 6s 553us/sample - loss: 0.4363 - accuracy: 0.8842\n",
      "782/782 [==============================] - 78s 99ms/step - loss: 0.1216 - accuracy: 0.9582\n",
      "Epoch 53/100\n",
      "10000/10000 [==============================] - 6s 566us/sample - loss: 0.3881 - accuracy: 0.8961\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1181 - accuracy: 0.9585\n",
      "Epoch 54/100\n",
      "10000/10000 [==============================] - 6s 553us/sample - loss: 0.4496 - accuracy: 0.8884\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1194 - accuracy: 0.9602\n",
      "Epoch 55/100\n",
      "10000/10000 [==============================] - 6s 554us/sample - loss: 0.4000 - accuracy: 0.9000\n",
      "782/782 [==============================] - 78s 99ms/step - loss: 0.1169 - accuracy: 0.9596\n",
      "Epoch 56/100\n",
      "10000/10000 [==============================] - 6s 556us/sample - loss: 0.4118 - accuracy: 0.8942\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1149 - accuracy: 0.9597\n",
      "Epoch 57/100\n",
      "10000/10000 [==============================] - 6s 567us/sample - loss: 0.4617 - accuracy: 0.8899\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1188 - accuracy: 0.9591\n",
      "Epoch 58/100\n",
      "10000/10000 [==============================] - 6s 556us/sample - loss: 0.4886 - accuracy: 0.8857\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1113 - accuracy: 0.9612\n",
      "Epoch 59/100\n",
      "10000/10000 [==============================] - 6s 557us/sample - loss: 0.4267 - accuracy: 0.8936\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1152 - accuracy: 0.9590\n",
      "Epoch 60/100\n",
      "10000/10000 [==============================] - 6s 556us/sample - loss: 0.4104 - accuracy: 0.8959\n",
      "782/782 [==============================] - 78s 99ms/step - loss: 0.1143 - accuracy: 0.9608\n",
      "Epoch 61/100\n",
      "10000/10000 [==============================] - 6s 552us/sample - loss: 0.3969 - accuracy: 0.8993\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1122 - accuracy: 0.9612\n",
      "Epoch 62/100\n",
      "10000/10000 [==============================] - 6s 556us/sample - loss: 0.4389 - accuracy: 0.8870\n",
      "782/782 [==============================] - 78s 99ms/step - loss: 0.1148 - accuracy: 0.9597\n",
      "Epoch 63/100\n",
      "10000/10000 [==============================] - 6s 553us/sample - loss: 0.4252 - accuracy: 0.8912\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1116 - accuracy: 0.9620\n",
      "Epoch 64/100\n",
      "10000/10000 [==============================] - 6s 556us/sample - loss: 0.3725 - accuracy: 0.9050\n",
      "782/782 [==============================] - 79s 101ms/step - loss: 0.1126 - accuracy: 0.9607\n",
      "Epoch 65/100\n",
      "10000/10000 [==============================] - 6s 559us/sample - loss: 0.4876 - accuracy: 0.8844\n",
      "782/782 [==============================] - 80s 102ms/step - loss: 0.1094 - accuracy: 0.9632\n",
      "Epoch 66/100\n",
      "10000/10000 [==============================] - 6s 561us/sample - loss: 0.3956 - accuracy: 0.9007\n",
      "782/782 [==============================] - 79s 101ms/step - loss: 0.1116 - accuracy: 0.9614\n",
      "Epoch 67/100\n",
      "10000/10000 [==============================] - 6s 555us/sample - loss: 0.4959 - accuracy: 0.8847\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1081 - accuracy: 0.9634\n",
      "Epoch 68/100\n",
      "10000/10000 [==============================] - 6s 556us/sample - loss: 0.4897 - accuracy: 0.8764\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1141 - accuracy: 0.9601\n",
      "Epoch 69/100\n",
      "10000/10000 [==============================] - 6s 555us/sample - loss: 0.4778 - accuracy: 0.8847\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1055 - accuracy: 0.9641\n",
      "Epoch 70/100\n",
      "10000/10000 [==============================] - 6s 554us/sample - loss: 0.3786 - accuracy: 0.8999\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1080 - accuracy: 0.9627\n",
      "Epoch 71/100\n",
      "10000/10000 [==============================] - 6s 553us/sample - loss: 0.4227 - accuracy: 0.8945\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1093 - accuracy: 0.9627\n",
      "Epoch 72/100\n",
      "10000/10000 [==============================] - 6s 580us/sample - loss: 0.4476 - accuracy: 0.8893\n",
      "782/782 [==============================] - 79s 101ms/step - loss: 0.1024 - accuracy: 0.9650\n",
      "Epoch 73/100\n",
      "10000/10000 [==============================] - 6s 558us/sample - loss: 0.3885 - accuracy: 0.8980\n",
      "782/782 [==============================] - 79s 101ms/step - loss: 0.1066 - accuracy: 0.9629\n",
      "Epoch 74/100\n",
      "10000/10000 [==============================] - 6s 558us/sample - loss: 0.4109 - accuracy: 0.8989\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1077 - accuracy: 0.9629\n",
      "Epoch 75/100\n",
      "10000/10000 [==============================] - 6s 556us/sample - loss: 0.4894 - accuracy: 0.8853\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1010 - accuracy: 0.9644\n",
      "Epoch 76/100\n",
      "10000/10000 [==============================] - 6s 557us/sample - loss: 0.4057 - accuracy: 0.8949\n",
      "782/782 [==============================] - 79s 100ms/step - loss: 0.1031 - accuracy: 0.9648\n",
      "Epoch 77/100\n",
      "10000/10000 [==============================] - 6s 556us/sample - loss: 0.4903 - accuracy: 0.8865\n",
      "782/782 [==============================] - 79s 101ms/step - loss: 0.1005 - accuracy: 0.9648\n",
      "Epoch 78/100\n",
      "10000/10000 [==============================] - 6s 557us/sample - loss: 0.4174 - accuracy: 0.8942\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.0965 - accuracy: 0.9672\n",
      "Epoch 79/100\n",
      "10000/10000 [==============================] - 6s 557us/sample - loss: 0.4936 - accuracy: 0.8782\n",
      "782/782 [==============================] - 79s 100ms/step - loss: 0.0989 - accuracy: 0.9662\n",
      "Epoch 80/100\n",
      "10000/10000 [==============================] - 6s 557us/sample - loss: 0.4358 - accuracy: 0.8950\n",
      "782/782 [==============================] - 79s 101ms/step - loss: 0.1003 - accuracy: 0.9650\n",
      "Epoch 81/100\n",
      "10000/10000 [==============================] - 6s 562us/sample - loss: 0.4173 - accuracy: 0.8950\n",
      "782/782 [==============================] - 79s 101ms/step - loss: 0.1005 - accuracy: 0.9646\n",
      "Epoch 82/100\n",
      "10000/10000 [==============================] - 6s 558us/sample - loss: 0.4874 - accuracy: 0.8862\n",
      "782/782 [==============================] - 79s 101ms/step - loss: 0.0954 - accuracy: 0.9675\n",
      "Epoch 83/100\n",
      "10000/10000 [==============================] - 6s 558us/sample - loss: 0.4139 - accuracy: 0.8951\n",
      "782/782 [==============================] - 79s 100ms/step - loss: 0.0990 - accuracy: 0.9650\n",
      "Epoch 84/100\n",
      "10000/10000 [==============================] - 6s 557us/sample - loss: 0.4080 - accuracy: 0.8954\n",
      "782/782 [==============================] - 79s 101ms/step - loss: 0.0963 - accuracy: 0.9667\n",
      "Epoch 85/100\n",
      "10000/10000 [==============================] - 6s 557us/sample - loss: 0.4534 - accuracy: 0.8917\n",
      "782/782 [==============================] - 79s 101ms/step - loss: 0.0990 - accuracy: 0.9664\n",
      "Epoch 86/100\n",
      "10000/10000 [==============================] - 6s 556us/sample - loss: 0.3961 - accuracy: 0.8981\n",
      "782/782 [==============================] - 79s 100ms/step - loss: 0.0947 - accuracy: 0.9674\n",
      "Epoch 87/100\n",
      "10000/10000 [==============================] - 6s 556us/sample - loss: 0.3873 - accuracy: 0.9055\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.0953 - accuracy: 0.9665\n",
      "Epoch 88/100\n",
      "10000/10000 [==============================] - 6s 554us/sample - loss: 0.4447 - accuracy: 0.8909\n",
      "782/782 [==============================] - 79s 100ms/step - loss: 0.0947 - accuracy: 0.9673\n",
      "Epoch 89/100\n",
      "10000/10000 [==============================] - 6s 561us/sample - loss: 0.7198 - accuracy: 0.8465\n",
      "782/782 [==============================] - 79s 101ms/step - loss: 0.0935 - accuracy: 0.9670\n",
      "Epoch 90/100\n",
      "10000/10000 [==============================] - 6s 558us/sample - loss: 0.3716 - accuracy: 0.9085\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.1001 - accuracy: 0.9652\n",
      "Epoch 91/100\n",
      "10000/10000 [==============================] - 6s 555us/sample - loss: 0.4465 - accuracy: 0.8915\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.0941 - accuracy: 0.9672\n",
      "Epoch 92/100\n",
      "10000/10000 [==============================] - 6s 555us/sample - loss: 0.4712 - accuracy: 0.8932\n",
      "782/782 [==============================] - 79s 101ms/step - loss: 0.0956 - accuracy: 0.9661\n",
      "Epoch 93/100\n",
      "10000/10000 [==============================] - 6s 558us/sample - loss: 0.5011 - accuracy: 0.8837\n",
      "782/782 [==============================] - 79s 101ms/step - loss: 0.0954 - accuracy: 0.9663\n",
      "Epoch 94/100\n",
      "10000/10000 [==============================] - 6s 557us/sample - loss: 0.3978 - accuracy: 0.8999\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.0902 - accuracy: 0.9689\n",
      "Epoch 95/100\n",
      "10000/10000 [==============================] - 6s 557us/sample - loss: 0.4661 - accuracy: 0.8855\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.0891 - accuracy: 0.9693\n",
      "Epoch 96/100\n",
      "10000/10000 [==============================] - 6s 556us/sample - loss: 0.3974 - accuracy: 0.8991\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.0935 - accuracy: 0.9680\n",
      "Epoch 97/100\n",
      "10000/10000 [==============================] - 6s 561us/sample - loss: 0.4662 - accuracy: 0.8924\n",
      "782/782 [==============================] - 79s 101ms/step - loss: 0.0937 - accuracy: 0.9677\n",
      "Epoch 98/100\n",
      "10000/10000 [==============================] - 6s 557us/sample - loss: 0.3881 - accuracy: 0.9027\n",
      "782/782 [==============================] - 79s 101ms/step - loss: 0.0911 - accuracy: 0.9693\n",
      "Epoch 99/100\n",
      "10000/10000 [==============================] - 6s 557us/sample - loss: 0.6429 - accuracy: 0.8596\n",
      "782/782 [==============================] - 78s 100ms/step - loss: 0.0904 - accuracy: 0.9686\n",
      "Epoch 100/100\n",
      "10000/10000 [==============================] - 6s 557us/sample - loss: 0.7533 - accuracy: 0.8373\n",
      "782/782 [==============================] - 79s 101ms/step - loss: 0.0885 - accuracy: 0.9691\n"
     ]
    }
   ],
   "source": [
    "train_acc = []\n",
    "history3 = model2.fit(datagen.flow(xtrain, ytrain, batch_size=64),epochs=100,callbacks=[TestCallback((xtest, ytest))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABL3ElEQVR4nO2dd3hcxdX/P2fVe5cty7033DC2AQMGQ8CA6QFDSIAEHFoI5E0CSd4kpP2SvAFCDQ4QAwkthB4wzWBswBTb4N6LbMtFxeplpS3z+2PuWitZkte2VpK95/M8enbv3LLnXu3Od+bMmTNijEFRFEWJXFxdbYCiKIrStagQKIqiRDgqBIqiKBGOCoGiKEqEo0KgKIoS4agQKIqiRDgqBEpEISJPicjvQzy2QETODLdNitLVqBAoiqJEOCoEinIUIiLRXW2DcuygQqB0OxyXzE9EZKWI1IrIP0Skh4i8LSLVIjJfRDKCjr9ARNaISIWIfCQiI4L2jReRr5zz/g3Et/is80VkuXPuYhEZE6KN54nI1yJSJSI7ReTuFvunOtercPZf65QniMi9IrJdRCpF5BOnbJqIFLbyHM503t8tIi+JyDMiUgVcKyKTROQz5zP2iMjDIhIbdP4oEXlfRMpEpEhEfi4iPUWkTkSygo47XkRKRCQmlHtXjj1UCJTuyqXAWcBQYCbwNvBzIBv7vb0NQESGAs8DtwM5wDzgvyIS61SKrwH/AjKB/zjXxTl3AjAX+D6QBfwdeENE4kKwrxb4DpAOnAfcJCIXOdft69j7kGPTOGC5c949wPHASY5NPwX8IT6TC4GXnM98FvABd2CfyYnAdOBmx4YUYD7wDtALGAx8YIzZC3wEXB503auBF4wxnhDtUI4xVAiU7spDxpgiY8wu4GPgC2PM18aYBuBVYLxz3BXAW8aY952K7B4gAVvRTgFigPuNMR5jzEvAkqDPuAH4uzHmC2OMzxjzNNDgnNcuxpiPjDGrjDF+Y8xKrBid5uz+FjDfGPO887n7jDHLRcQFfBf4oTFml/OZi517CoXPjDGvOZ9Zb4xZZoz53BjjNcYUYIUsYMP5wF5jzL3GGLcxptoY84Wz72ls5Y+IRAFXYsVSiVBUCJTuSlHQ+/pWtpOd972A7YEdxhg/sBPId/btMs0zK24Pet8P+B/HtVIhIhVAH+e8dhGRySKywHGpVAI3YlvmONfY0spp2VjXVGv7QmFnCxuGisibIrLXcRf9vxBsAHgdGCkiA7G9rkpjzJeHaZNyDKBCoBzt7MZW6ACIiGArwV3AHiDfKQvQN+j9TuAPxpj0oL9EY8zzIXzuc8AbQB9jTBowBwh8zk5gUCvnlALuNvbVAolB9xGFdSsF0zJV8KPAemCIMSYV6zo7mA0YY9zAi9iey7fR3kDEo0KgHO28CJwnItOdwc7/wbp3FgOfAV7gNhGJFpFLgElB5z4O3Oi07kVEkpxB4JQQPjcFKDPGuEVkEnBV0L5ngTNF5HLnc7NEZJzTW5kL3CcivUQkSkROdMYkNgLxzufHAP8LHGysIgWoAmpEZDhwU9C+N4GeInK7iMSJSIqITA7a/0/gWuAC4JkQ7lc5hlEhUI5qjDEbsP7uh7At7pnATGNMozGmEbgEW+GVY8cTXgk6dyl2nOBhZ/9m59hQuBn4rYhUA7/CClLgujuAc7GiVIYdKB7r7P4xsAo7VlEG/BlwGWMqnWs+ge3N1ALNooha4cdYAarGitq/g2yoxrp9ZgJ7gU3A6UH7P8UOUn/ljC8oEYzowjSKEpmIyIfAc8aYJ7raFqVrUSFQlAhERE4A3seOcVR3tT1K16KuIUWJMETkaewcg9tVBBQIY49AROZiY5mLjTGjW9kvwANYX2odcK0x5quwGKMoiqK0STh7BE8B57SzfwYwxPmbjQ2FUxRFUTqZsCWuMsYsEpH+7RxyIfBPZ7LP5yKSLiJ5xpg97V03Ozvb9O/f3mUVRVGUlixbtqzUGNNybgoQRiEIgXyaz5QsdMoOEAIRmY3tNdC3b1+WLl3aKQYqiqIcK4jI9rb2deVgsbRS1uqAhTHmMWPMRGPMxJycVgVNURRFOUy6UggKsakAAvTGpgtQFEVROpGuFII3gO84U/unYBNftTs+oCiKonQ8YRsjEJHngWlAtrPgxq+xKYExxszB5o0/Fzutvw647nA/y+PxUFhYiNvtPlKzFYf4+Hh69+5NTIyuVaIoxzrhjBq68iD7DXBLR3xWYWEhKSkp9O/fn+aJJpXDwRjDvn37KCwsZMCAAV1tjqIoYeaYmFnsdrvJyspSEeggRISsrCztYSlKhHBMCAGgItDB6PNUlMihK+cRKIqiRDyNXj/z1xVRWF5HTJSLyQOyyE9PoKSmgUUbS4iPiSI9MYaiKjcj81KZPDCrw21QIegAKioqeO6557j55psP6bxzzz2X5557jvT09PAYpihKu1TUNSIipCU0BUV4fX7qPT7qPT72Vrp5anEBW0tqGdM7jfF90xmRl0q0y8WjH21hze5KLp/Yh3qPjxU7K4iNdlFYXs/uinoG5iTh8Rl2ldeTFBfFiLxUJg/IZP66YqrcHsbkpyEivL+2iF0V9SHZe/3UAWERgqMuDfXEiRNNy5nF69atY8SIEV1kERQUFHD++eezevXqZuU+n4+oqKgusurI6ernqijt4fX5AYiOclFa08DOsjpG5KXy9uo9rN5VxbRhOazdXUXBvlq+f+og3l9bxAtLdjC2dzp9MhPZUVbHmyt34xJhxuiepCfGsn5vFUsLyvH6m+rFxNgoRvVKZc3uKuoaffvLY6NcDM5NZu2eKgAG5SThN5CbEkd+RgLbSmuJiXLRJyOR2gYvXxaUUVbbSK+0ePIzEli7uwqXCEN6JPODM4YwsX8G1W4vSwrKKK1pJD7GxWlD7QTaynoPPVPjyUyKPWy3rYgsM8ZMbG2f9gg6gLvuuostW7Ywbtw4YmJiSE5OJi8vj+XLl7N27Vouuugidu7cidvt5oc//CGzZ88GoH///ixdupSamhpmzJjB1KlTWbx4Mfn5+bz++uskJCR08Z0pSsezbk8VfmMY1iOF6CgXbo+PfbWNlFQ38OnmUpZtLyc7OZbE2Gj8xpCTHMfq3ZUsWF/C1CHZ9EqP5/21RRRXN5AYE8X4vhl8WVBGo9ePS8BvIMol/OOTbQDERrv495Kd+A2M6Z3Ggg3FlNd5SIqN4spJffH6DW+v2oPPb+iVnsB3pw4gNyWOuJgokuOimDY0l4ykWHx+w6biajYX11BZ7+GUwTn0yUxg7Z4qMhJj6ZXe/u/V4/OzrbSWQTnJRLlar8xT4mO4cFz+AeW9M478ubfHMdcj+M1/17B2d1WHfubIXqn8euaoNvcH9wg++ugjzjvvPFavXr0/9LKsrIzMzEzq6+s54YQTWLhwIVlZWc2EYPDgwSxdupRx48Zx+eWXc8EFF3D11Vd36H0cKtojUFqjrtHLuj3VjO2dRnRUU7yJ2+NjZWElSwrK2FJSQ22DlxP6ZzKyVyrLd1YwMDuJzcU13PPeRgBEIEqkWesbYEhuMpX1Htwe2/qucnvJSIzhjOE9+GhDMTUNXs4c0YNBucmUVDfwxbZ9TB6QyUmDslm1q5IJfTM4ZUg2i7fso19WIinx0fzlnQ2M7JXK96Y2hUNHWkCE9gg6mUmTJjWLv3/wwQd59dVXAdi5cyebNm0iK6u5n2/AgAGMGzcOgOOPP56CgoLOMldR9lNS3UBJdQMDc5Kob/Tx9c5yPlhXTFFVA2Dw+Q1LC8qpbvAytncao/PTmLdqDy4Rqt1eGh13Ta+0eGKjXby7puiAz7hoXC9OH57L5uIafH5Dcnw0WUmxZCbFMapX6gEt6/pGH9FRQkyUC4/Pj89viI9p3eU6c2yv/e/PGtlj//v7rhh35A/nGOaYE4L2Wu6dRVJS0v73H330EfPnz+ezzz4jMTGRadOmtRqfHxcXt/99VFQU9fWhDR4pSqgEXC9LCsqoafASJUJqQgyNPj97KupZs7uK4uoGgP0uFoCk2Cj6ZCbiEkEEzhzZg7G903jww82s21vNOaN6khIfTXJcNBP7Z3J8vwwyk2IB2FRUzY6yOsb1SWf93mpqGrx8Y2SPQ2qNJ8Q2VfoxUS7a0ADlCDjmhKArSElJobq69RX/KisrycjIIDExkfXr1/P55593snVKuDHGdKqbocHrY3eFm/z0BPZU1rN6VxWDc5PZVFzNO6v3UlrTQLTLRW5KHCN7peLxGd5fu5evd1ZgDKQ4LXCPz1BZ7yEu2kVOShxTB2czslcqPVLj2VRcQ3JcFCPz0jhhQAZx0QfWvrMm9cXj85MS33YakiE9UhjSIwWAkwfHtXmc0rWoEHQAWVlZnHzyyYwePZqEhAR69Gjqkp5zzjnMmTOHMWPGMGzYMKZMmdKFliqHQ32jj9ho1/7Bwr6Zibg9fh76cBMfri9mX00jF43P544zh5CbGt9hn9vo9bNoYwnvrd1L74xEBucm8/KyQj7dUorb42/Wag/QIzWOvpmJNHq9LN5Swytf7wJgdH4qt08fyhnDcxnZK7XNwcpDIT4mqk0XjXJ0ccwNFisdRyQ910avn482FO9vJW/fV4vPb9hYXMMnm0pwieByCY1eP7FRLmKjXTR4fZwxPJekuGjeXLGHU4dm88Q1J4T0ec9/uYP1e6qYNjyXL7eVsaW4huS4aLaU1FBe52FAdhIrCiuoqPOQEhdNdYMXsKGJ5x6Xx4i8FArL68lKimVc3wy2ltSQnRzHyYOzm1XyxVVuDNCjAwVKOTrRwWJFCaLR62dvpRsRGyO+YEMJD3+4iYJ9dfuPiY12EeMSspLjuOHUgfujW0bmpbJqVyVltY3ccvpgBucmA9AzNZ45C7ewu6L+gMHO9XurmPvJNq45qT/De6by0IebuH/+JlwCT3+2nSiXMDA7idoGL32zEsnPSGBLcS2nDsnhovG9OGVIDsXVDRSU1jJpQCYxUQdmhhnXJ73Ve+3IHopy7KJCoByT1DV6mbdqL09+uo0eqfFMH5HL/LVFrN1jB0RbdoQH5ybz2LePp19WEomxUeSnJ+Bqw31y0fgD47yvnNSXRxdu4YUlO/nW5L5sLKpmb6Wb7fvqeOKTrbg9fl75aheZSbEUVzdw6YTe/Or8kSzbUcZx+enkpLTvP89PTyD/IHHqinK4qBAoRzX1jT4+2lDM++uKWLu7itKaBhJjo9lVUY/PbxjeM4UVOyv4cH0xvdLiOWVIjq1UMxIwxlBe52HygEzG9k5vs+IPhT6ZiZw6JIe/L9zCQx9uaiY0pwzJ5tczR/KPTwrYV9PAJRPy+cbInrhcwhnDe7R9UUXpJFQIlG6P1+fnqcUFFOyrZdKALJYVlLG1tBaApQXl1Ht8ZCTGML5vBuP7plPb4GPm2DxOGpTNSYOyaPD62VJSw/CeHTNI2hY3TxvE3ko33xjVg6mDs+mZFk+P1Pj9A6p/vOS4sH22ohwJKgRKt6TK7eGtlXtYs7uSZdsrWLenirhoF898voO4aBfD81Lx+vxcenw+547OY9KAzGazXIOJj4liVK+0sNs8eWAW795xatg/R1E6GhUCpUsxxvDltjI+3FDMqsJKGrx+ymsbKSyvp9HnJz0xhry0BB65agJnjsxlze4qhuQmtxu7rijKoaFC0AUkJydTU1PD7t27ue2223jppZcOOGbatGncc889TJzYarQXAPfffz+zZ88mMTER6P5prT0+Py8tK+Sr7eXMOK4nheX1/Ouz7WwqriEmShiZl0pyfDTD81L4xqiezBjdkzG905pN1prQN8zZtxQlAlEh6EJ69erVqgiEyv3338/VV1+9XwjmzZvXUaZ1GB6fnz+8tY7/LLXZH+s9PuJjXPxnWSEAx+Wn8X+XjeG84/JIitOvo6J0BfrL6wDuvPNO+vXrt39hmrvvvhsRYdGiRZSXl+PxePj973/PhRde2Oy84Kyl9fX1XHfddaxdu5YRI0Y0yzV00003sWTJEurr67nsssv4zW9+w4MPPsju3bs5/fTTyc7OZsGCBfuzmWZnZ3Pfffcxd+5cAK6//npuv/12CgoKwpbuendFPXur3BSU1rKysJIGr48Gr5+1u6tYv7eaC8f1IjMplqmDs5k6JJuFG0rITY1vM/5dUY5J/H5wdb8VgsMqBCJyDvAAEAU8YYz5U4v9GcBcYBDgBr5rjFl9wIUOhbfvgr2rjugSB9DzOJjxpzZ3z5o1i9tvv32/ELz44ou888473HHHHaSmplJaWsqUKVO44IIL2sxJ8+ijj5KYmMjKlStZuXIlEyZM2L/vD3/4A5mZmfh8PqZPn87KlSu57bbbuO+++1iwYAHZ2dnNrrVs2TKefPJJvvjiC4wxTJ48mdNOO42MjAw2bdrE888/z+OPP87ll1/Oyy+/fETprjcWVfN/72xg/rqmLJOJsVEkxUUTG+UiOzmWB2aNOyDH+jdG9Tzsz1SUowJ3FUTH2T+A4nXw2Olw/XzoObprbWtB2IRARKKAR4CzgEJgiYi8YYxZG3TYz4HlxpiLRWS4c/z0cNkULsaPH09xcTG7d++mpKSEjIwM8vLyuOOOO1i0aBEul4tdu3ZRVFREz56tV4CLFi3itttuA2DMmDGMGTNm/74XX3yRxx57DK/Xy549e1i7dm2z/S355JNPuPjii/dnQb3kkkv4+OOPueCCCzos3bXH5+fZz7fz/95eT3y0i9vPHMLYPun0SktgcG7bC28oSthoqIE5U+H0X8CYb3atLfXl8OhU6H8yXPKYLSteB956WPNq6EJQtAYy+kNs0kEPPRLC2SOYBGw2xmwFEJEXgAuBYCEYCfwRwBizXkT6i0gPY8yBScxDpZ2Wezi57LLLeOmll9i7dy+zZs3i2WefpaSkhGXLlhETE0P//v1bTT8dTGu9hW3btnHPPfewZMkSMjIyuPbaaw96nfbyRx1quus1uyt5c+UeTh2SgzGG+euK2VtVzxdby9hX28i0YTnc882xZCdrZsluhTFQvRdS8zrmejXF8OkDMO0uiEs5vGssfhjWvwXfeR2iY9s/dvtn9pj849s+pq4MGmsgva/dXv8mlG+Dta9ZIZj/GxhyFvQ7qfl5O76A+FTIDcqjZYy9x7jkjql05/0Uqgph7etw3r32mdXts/s2vgvTf9n8s1f9B4adaz+/ugiSc6F0oxW26b+GqbcfuU3tEE5nVT6wM2i70CkLZgVwCYCITAL6Ab1bXkhEZovIUhFZWlJSEiZzj4xZs2bxwgsv8NJLL3HZZZdRWVlJbm4uMTExLFiwgO3bt7d7/qmnnsqzzz4LwOrVq1m5ciUAVVVVJCUlkZaWRlFREW+//fb+c9pKf33qqafy2muvUVdXR21tLa+++iqnnHLKId2P3xiq6j1c+PCnPPrRFq58/HOueuILnvtyO+v3VjNlUBZPfGcic6854dgQgYYa2LYIfN6utqR1GqqtfaFQuhmeuQTuGw4b3zu8z6sthXd/AXPPsZ+95lX47GFY8P8ckTnEttrqV+C9X8COxbB1QevHVOwAdyU01sLzV8CT50HhsgOP8/th5Yvw4Hh4bBp4nMbMiuft6/bFULIRPrkPPr63+bneBnj2MnjiLNj1lS0rWgN/HQ33DoVnLj20+wrYU19unxPA6pdh1Ysw5GzwumGD85utLXU+bxVUFjadX/AJvHIDfPEolG2D+0fDW/8Di+4B44eyrYdu0yESzh5Ba76Blk3VPwEPiMhyYBXwNXDAL9EY8xjwGNjsox1rZscwatQoqquryc/PJy8vj29961vMnDmTiRMnMm7cOIYPH97u+TfddBPXXXcdY8aMYdy4cUyaNAmAsWPHMn78eEaNGsXAgQM5+eST958ze/ZsZsyYQV5eHgsWNP24JkyYwLXXXrv/Gtdffz3jx48P2Q1U4/awu9JNldvLBWN78ZNzhvHltjKiXMKZI3oce6mHFz8MC/4AnjoYMwsuejS0Ab3aUvA1QlIuRLXyU/L74fWbIWswnPrj5vs8bjC+0Fuf7/0Slj0Jsz+CXuObyhuq4at/wuQbwRUFlbvgyRnga4DYFFj5bxj6DXts2TZbyfYcDRvesZX7jD9BQlBIrt8HS/4BH/4OGpwlX3d8Abu/tu+/mAPFa2HrRzDuanv+wXoIxevhtZuhz2Qo3WRbv1Gx8Pnf4JQfQ9/J9nk8foZ9lhO+bQUhMdsKwndehx7OglNbP4K374SS9ZA1BPZtsvcxcBpsXQiZA23FGRCArQutrz4+1W5v+dDeV2yKFctr3oR5P7Yum5EX2d5EyUYrKmVb4LKnYM0rtnV++s/h80etQLqi4fy/wsgLrN2lGyEqDk6+DT77m73XK56BB8dZYRhzOdSVgrhs5b7sKWvrqEtsLwlg1Uvg89jv1NJ/sL8KDRaNMBG2NNQiciJwtzHmbGf7ZwDGmD+2cbwA24Axxpg2Fx3WNNQdj8/vp7bRR12Dl5oGL3WNPmKiXLhLdjB+TPca1AoLfzsJMDDgNNsqO/mHcNZvDzxu7yp480dw4s3Q90R4+ARbqSTlwm1f2/3v3AXXvmW7+J89Au/+HHJHwc2Lm1/r5RugvACuf//Az/G44R9nwSk/glEX24rggXHg99iK45tPNh274gV49ftw3dvWjfLkDCjZANd/YCva1S/DjzdZWxb9H0QnwI83wtPnQ+ESyBkOV78MaU5H/Iu/w9s/hYGnw/RfwRPT4dSfWBdHQqatZN2VMGyGrTT7TIHr5tkFiFvD2wCPT4fqPXDTYlj4J2tzTKKtGMG6PpJ7WNEEkCjIG2t960/PtGJ32VzIGwePTILETDjtLhh9KTx6ohWi/OPhy8fg6ldsBQ8QlwYNlXDpP+C4y2zZK9+Hje/YAdunZ9p78dTB+ffD8PPg3uH23jbMsxX2qT+1PSFPvX2O/7rI3lNcsn0Woy6xAn36L2Dnl7D5fXsvsxdat9x7/wufz4GfbII377DfEb/X/u/B3sfy56C22PYe4tIgb4wVg72roOcY29u49cu2v78h0l4a6nC6hpYAQ0RkgIjEArOAN1oYlu7sA7geWNSeCCgdh98YjOP+Wb+3moLSWkqqGwGhV3oCw3qktN7yD8Ul4HHDOz+DV2YfuaHlBbbb3tGRYAGMsX7lgafDOX+EsVfZFl1NCxfk5g+sO6HwS/jv7bYV6amHk35gf8RbF9iW+Z7lULTatnzn321bvvs2NXc5+f2w6T1bEbsrD7Rpy4ewd6X1yQN8cr99Pe6btvINdhVU2YVnKF4Lm96HXctg5gOQOxxGXWR96M9cCgt+byvShkr4+l/2s4efD1W77X0VrbHXWf0K9DgOvv0q5E+AHqOtPSUbYOBpcMMHcNtXVozOvce6eta/aZ/j5vm2ov38Ueu/B1j8oHWFXPgIpPSw9+Cps5X7DR/CyAtt7+OjP0LOCLvf+GDKTZA9xB6TNQienwX/vNCee+W/YewVthc28bv2nr98DI6/DgadAamOB3rKTZCUY+0DW4FvmGfvO3sIfOcNK0h542DCd6xffvCZ9viYJFu+6P9sLwljP6NotT32ojnWnbjsSRhzBZz2U7jqRbjkcfj2a01jMwOnWQEvXmd7kInZVnTO/qN1HX18L1TugNPutALYUGmv/+3X4ObP7P+gspAD0uV2MGETAmOMF7gVeBdYB7xojFkjIjeKyI3OYSOANSKyHpgB/DBc9kQyfmNwe3w0eH34jaGirpF1u6tYs7uKgn21xEa5GJwujE7Yx+CcJLKT41rPxLnzS7h3WNuV8vLnbev6oeNta3Tlv+2P72A0VIO3sfV96+dB5U7bIm12To0dkCvddPDrg618P/oTrPuv3a4ptpVVTZGtXDIH2Fbt1DvsD/frfzU//4PfQmov60porLXXmfx925qNS7Pd+43v2GNLNthK0ddof+C+xuaVd9FqcFcAxlbILVnntJd2f23dBV89DeOuhLN+Z10SH9/XdGzVbvtavN6KkETZli1A/1Os22fHYphwje01JOXA+7+2+8/4pS3DwFPnWbt3fmHPD7Tw+0x2bDTWJZXW2z4HsNfMHmpdJY+dZgVn/Vu2V/TEdCt+y5+zPa1h5zjXmwJDz4Hz77Ot+Asettes3AmTboDz7rOV6SinVZ/ay9o4/DwoWWd7JzlDm+5/7JX2Omf91rpqRKCf4z4dcb4dgN30vhXcje/YHtyoi+z+nKHwg6VwzX+tWw3scwY46Va44CGIT4eZ90N6P/j0frtv2AwrtFNusv/76c7zdLmsC6jHyCb70vrY18pddrA4KRsGnW57lWf/wfY6EBj/bRg83bqshp8PsYk2WiitN3hqba8gjIR1ZoMxZp4xZqgxZpAx5g9O2RxjzBzn/WfGmCHGmOHGmEuMMYd9t0fbSmudRb3Hx6aiGjYWVbNhbzVrdlWxo6yOuJgospJiyU2JZ1BOMomNZUhDpa20aON57voKMLBnResftmGeHfDreZz9gUJTi3XzfCsQS+c2P8fvtz7Wd3/W+jW3fOi8Bg0w+jzw4nfgy7/D8mcPPGfRPVa0AhgD8/7HtjpfuwX2bYG/n2r91oEKOnOAfc0ZaivQZU86LUFshbxnuf3hDzgFzvy1rQBP/QlExcCQM+3gZb3TCi7daCv7pBz74wZbiQUo+Nh5I83tBCuI6+fZyjIqFl6+HuJSbWWTmgcnXG/vOSDG+4VgHexebl09Mc4EwagYmPoj22I9717bgh51sa1YsgZDzjA7XnD1y7aifH4WYJqEBKwQBMgb19zWqGhbAVdst8J8wUPw0622Ii/bap932VbrwgngcsFV/4bxztyV+FT45lO2JzZ2lt0ec3nzMZfYJPjmP+F779sxhWAS0m2v4eQfNonX5O/Dibfa3szE66xwv/9r+5c1xLbS95+f0TR+ADDiAutKOvl266L5yRYYdxWMmGl/G5mDbG8C4Bu/hztWQ9qB61PsJ9A7qSp0egRZTfuyh1i7x86C5Bzbk7v2TSsCAQIuuzCPE3S/KW6HQXx8PPv27VMxCMIYQ1ltI1uKa0jzVzAsdh+9MxLJSYmjZ1o8A3OSyEtPoGdaPC4xTQODPg/GGPbt20d8fIvVrfY5re99m1v/0PpyO6h31QtNQlBZaN0Nz1xqz/vkflv5B9jxma04WxMXbwNs/9QOwu3+CuorbPnCP8OWDyA+DXZ83vycyl3W1fDCt5rcO8ufswI05gporLbCU73Htn4DQpAxoOkaJ3zPCtqCP9hW7bKnrG/9OCc2/aQfwC1f2koIbKVtfOCKsa240o2wd7V9FtnDALEt9g3v2BZ+wSd2oLDnaGt/xQ4b3dNQAxvesu6B469zKmRjW89JzqTB035q7/vdn1uB2+8aWmOfYd7Y5s/j5Nusrz3KSdI32vGVDz+/qeLsMcr+v8q2QlpfK+QB+tiAA1LzrWunJcNmwA++gluXWJdGdKyt+NP6wMf32B7MiJkHnhdM/vFw8aPtD5y7XNaWUAbxe0+0rW0R24uZ+F0r7OXb4Lx7mp5Fq58TZccTYpzvfkCQAuI4bEbTsSLNRaQ14pLt/6tip+0RBAsB2EbFxXPs+9Re0Gtc8/2dJATHRIqJ3r17U1hYSHcNLe1M/MZQ7fbS4PXT6PUTH+3CJZWUe92QUrv/R7Av+CSP2/q5AUr9EJNIfHw8vXu3iOQtDUEIAjHd6U6XuGKnrcyTcuDMu+H1W2DbR9aXC7DyBfvaWojczi+t2+akH8Dih2wFOuJ8KyyDpts48C8fs/YHfriB1nZdqR18vOpF23PIHQUX/x2i462rJXOQjQrZutC6UwJ2AwyfaUXj43vhq3/Z+xpzRVPFD80HRwefaaNB+k+1LczCJVBbYlvvsYn22ntX2oib+jJbOY67ygrc8uesW6ZiBzZKxFg/8sBp9v6GzrB+9AAJGbaV//4v7TOr2m2vF3AdtBSClvSZBBf+zYpXMNN+ZgeWR8xsfm/pfa0I9G47+SFZg5pvu6Js5fvBb+z/OTGzfZvCzfRfWrfQgFOb9wYOhT5T7Pc30Bg4FFJ72zEc42sS9FDZ71pSITgoMTExDBgw4OAHHuNUuT1cO/dLVhRWMr5POmeP6sl3pwwg6sExtqJpKxrmzTtshef3wDl/si6QALu+gjd+YAev9gvBltYNqC9vqogCXeLKQttCzhluW6Pv/S8se9pWEB43rHndtqTr9tkWf3Blu3WBraSn/giWzLXbPUbZCnzy9+1nfPawddv0nWLP2bbIVpan3Wl91a/falvK591rK7izfmtbibkjYO7Z1p2V3rd5KzEq2raih55tW+quaBvB0xaJmdYtkjvSThZa84ot7+FEXOWOaIpC6Xmcdev0P9Xas+RxqPZYX3nFdtsqHHyWFbaMfvavJf2n2tfdX1vB6X9KkwAeTAhEYPy3DixP7wO3fGEjoFoe/+3XDt7ybcmEa2xPauJ3D+28cJCQAbcubXKZHQ4ulx0/OhzS8u3cBrAifygkZtsGQ+XOgx97BBwTQhDpLN5Syv3zN7Fml83n/8hVEzhntJPKwtvQ1JpY+R+Yfnfz7nVjrR3gG3q2HVQL+JwDBCIlVr8M1budKJgt9riXvmsrsOzB9ti6sqaY9Og4SO5pIyJKN8HoS2zlNvZK+PJxe37Bp9YNMvF7Nm66fBus/NIO0I29wu7PP95WtANOsQO06U7FOPhM2+UG617pO8W6SrYtshXj5BttpM/yZyA22bbowQrNxOvsc3HF2KiagPujJaMvbe7fbo+Az7t8W1NZIPY9Z7htkab1tWGd2xbZHk1tiXUVnPW71ivntshx3E2B8ZNBZzhCIM3dOodKRv82Pm9o6+XtkZQFt688fFs6mmC/e2eTmm+/Z2Cfy6Hgclkh0TECpS3qGr38+vXVXPX4F+yprOebE/vw3A1TmkQAbE/A+G2oWvXuoIFKh7d/aiNoptwEKT2t7zxAQw2sdSJYljxhX/ufYicrffqg9e8Hyj31dlJO8OSktN6we4WNkMl2KpNJs609n/zVhublDIfjr7H79m2xfvnPHrLjCEVrmlq4U++wET4f/t5WWJkDbTc7a0jTOEH5NttyGuC0ti/6m+1an/C9Ayc9Rcc1VdSZAw/10bdN9jD7KlFOhY29R7ATpaLjbNoDl8v63H+y5dBEAKwvPaO/HYAH+4wSs+3gY1xyh9yG0oEEDyYfao8AnKgqFQKlFVYVVnLO/R/z1ucruXlyFu/dfhp3XzCKSQNa+GMDvvcTb7Ex04HwSbBT379+Bk75H+tuSO0FVUFCsO6/NsIke2jTQHFgsOyrf9rX1c5syICPOlgI0vvYGHKwFTbY6Jyxs2xPIzBbM8vpUWyYZ6NXitfZcYjG6qbkXH2nOJEbDbY1HfBj951sY/uhKQVDwA+cnGsHMs/8TesPMd/J8JrRgW7FrEGA2GcWyDo55Cw47vLW3SRtTcQ6GD1GWWEE2+I8/ho7WKt0P1KDxtoOdYwAbGNGhUAJxu83/GfpTi6bsxif37Cw54P8tPFhEmLbSPsQEIIeo6wLYcPbTZNTCj6x0TDTnNDNlDzbawiw4nnb8gz4RsVlKzWwAtFzjHVvbPmwSQiCBwbTgn4AgZA7sMIjLnv+8Jm2hZvcE9Y5E3/8XpuGAJr87GAr9NTeTbNEwba26/ZZt9TeVdZdFBAWsFEsbVW2gVQNHdkjiEmwNge7m5Ky4dLHD68SaIvghGmpvews4JN+0HHXVzqO4N9By6ihUEjva3vqjbUdZ1MLVAiOEvx+w1sr9zDjgY/5yUsrGdM7jf9e3Zukcid+vC3Kttk49MQs25qvKmyKQa+vsBV3IEQu0CMwxuZnKfjExp0PPN3uT+9n/2KcML9z/2Kvu+L5ppmkzVxDTsRDdHzTe7Ct5sv/aePNA+MVmQNtaz/aGdBb8TwgzSu8rEHwozXNs0kGKvHybda1lDko9Fb2sHPt2EHL7JRHyrVvwow/d+w1WxJ4LrEphz6Qq3QuAddQTNLhDVj3mgCYpiR5YUCF4Chgb6Wbi/72Kbc89xVev58HZo3jhdknkrl7oT2gcmdT5kOwse9v3GZTPJRtaZo1O+RsQJqyIbor7MBsgJQ86+d3V1g3i/HZQdnUPDuZKG+svU7WINuC7z3JumkKlwa5hoJ7BE7lnzX4wPjvETPt7MwAgQp96Nl2tmblTlt2sKRsAbdO2TZ7ry1DGdsjKdtGBwVHKnUECelHFqESCrnO7NXALF+l+xKIoDvUgeIAgdDdnV90jD2toFFD3YXidXZwNMjtUVTl5qMNxdw/fxNV9R7uu3wsF47LJ2rLfNhTZPPVBCjZYL8wfh+8OttG+YANPRt+rn2fnGNdFhvmwbQ7beUd3IIP5Eep2mPdPbHJtrIHm3vG5Xxdzvx105J7Gf3tOEFgHkLLwWJo7qppi8DM3n4nW1dPwcdNg7ntEYh0KVlv5yyMverg5xwLZA6yUU8dtd6AEj6i4+w8msMZKAbba88eduAs9A5EhaC7sOgvsOY1O4MxJoH/rtjNj/+zggavn35ZifznxpMY2ctxAbx+i3XdBKKBNr1rJ6z0nmgzO65+2WZD/PoZG5se7APvO8UmVTPGCkHwvhSnUqneY2fu9j+laQGRYN//4DOb3mf0s3bsXX3gcYFJZYHomfboOQYQGyZaXuAIQQiZT2MTrd1bPgTMofUIjmaiY23+npZpH5TuSe5Im5X0cOlzgg3zDtOaxyoE3YXCJdYVs3cVc7fn8Ns313JC/wx+f9FxDO2R3LR6mbvKRotIlD1+0g2wbaFNYeDz2PQLeeNsHpz0frZ3kBlUOSbl2oljDVUH9ggCQrB9sa2Mp9xycLsDM3L3LLdzDGKC4rUTMmDW823H6Qcz5Cz44QorLIGQ0VB6BGDFLDBhJzNChABsvnvl6GDWszZA4nDpM9k27PZtPrx5HQdBhaA7UFPspBiAtUsW8Lslozl7VA8evHI8cdEtooHKnFm9591r/dCDpttQxZJ1NhlZxXabHljEToePjoMh32g6PxC5Ult64EzegBAsfggQm0ztYAQmeBWtseMDLQdqA26pgyHSNIt2+Lk2siiQhuJgZA6waSwAsjowAkhROorDXd4zQCD5384vVAiOWQrtQjsGYfPyhYzrM5UHZrUiAtCU3qHP5KZ0t7kj7OSiPSuh9wlNIZ4uV1PK3QABIajceeAEsJh427r2NtrkXKGEVabm296Jr7H5tY6EuBQbDhkqgQHjhMyOs0FRuhNZQ+x3O9AQ7GBUCLoB/p1L8BPFJ77RnJiwnVOvPaHt5SBbpk0GG0u/8t92YPiCh9oPn0zKsa+BvEEtK84bPrQhnIEkbgcjKtqGx1Xs6LrkYgHBipTxASXycLng9tVhmzmu4aNdiMfnZ1NRNdtXLmSNvx+ZI6eR01hIOjVNB/l9NiHcgxPscoP7tthJVcHhiXlj7Ov0XzWPu2+NQORCIINocPgoWGEIVQQCBNxDXdUaD4hiJI0PKJFHGNOHqBB0Ee+t2cv0exdy9l8/IqdqDbU54xgz2fGJBxYKB5tH/41bbYt76ZO2Am/pBx94Bnz3PZtG4mAEXENt9QgOh/1CkH7k1zocMgfa0NbgeQmKooSMCkEX8P7aImb/axnxMS7mTmskWdxMmTbTSXkgdg3WABvm2Rjis/9g8/3sWXFgXL7LZXPuhDKjNjrOTtjqUCFwIocSusg1FJ9mFyOf1AFrJCtKBKJC0MlUuT3872urGN4zhTd/cArT3B9AbAquYefYCi13pM3qCTZf//bFNnomkOzN7zlyF0hStk0PDR0jBBld7BoCK6IHm4WsKEqrqBB0Mg/M30RJdQN/vnQMsf56uyj7qIua8qX3nWJnEPq8NlTM67bZNNODlhA80kHR4ORnHeHOCfQIunolKkVRDgsVgo7G47YLwASvn1yxA8q3Y4whaflc/tbjDcb2SbfZNhtrmtb3Beh7oi0rWm1X5HJFQ/+T7b5hTkx+KCkb2iMQOSRRNiHdkdLzOLsGQN8Tj/xaiqJ0OmEVAhE5R0Q2iMhmEbmrlf1pIvJfEVkhImtE5Lpw2tMprPsvvHJ980yB/70dXvgWO/bVcrnnVb5R+ZLN1rnieduaDq5A+znvd3wOWxbYeQGByShTbrbrzR6xEDg9goT0w8+HH0xcClzz39BSSSiK0u0ImxCISBTwCDADGAlcKSIjWxx2C7DWGDMWmAbcKyKx4bKpU6jaZV9L1u0v8lSXQNEqtnzxJr2lFJfx2pW9tn5kewPBuUPSetusnZ/cZ9M2DAuamZuQblezOtLKO9AjaBk6qihKRBLOHsEkYLMxZqsxphF4AbiwxTEGSBGbSCcZKAO8YbQp/FTvta8lGwAwxlBUWgrA0OV/tGUJGTYnEKZpLd1g+p5o8wkNP9/2AjqawFwCnYWrKArhFYJ8YGfQdqFTFszDwAhgN7AK+KExxh9Gm8JPYM3f0o0AfLWjglhfHQC9PQWUxvRCJnzHrsLVZ3LrA78nXG9DIS/9R9OiMR1JkgqBoihNhFMIWvNfmBbbZwPLgV7AOOBhETlg9FJEZovIUhFZWlJS0tF2diyBdWRL1gPw3Bc7SKZ+/+7KXlNh9KV2Y/zVrV+j72S7+tehzvANlYBrSIVAURTCKwSFQND6hPTGtvyDuQ54xVg2A9uAA6aHGmMeM8ZMNMZMzMnJCZvBHYLTI/CXb+fC++fz1spCEqUBf/9TAegz+WKbZvmWJTD+211jY/BgsaIoEU84hWAJMEREBjgDwLOAN1ocswOYDiAiPYBhwNYw2hRejMFU76U8JhcXhv6yh4xoO+ThGnIW3LKE2BHOxLCcoR0TsXM4aI9AUZQgwiYExhgvcCvwLrAOeNEYs0ZEbhSRG53DfgecJCKrgA+AO40xpeGyKey4KxCvm/n1tlPzwBkJfPYjZ1GWuOSurfyDScy27qlQ8/0rinJME9Y01MaYecC8FmVzgt7vBr7R8ryjlmo7PrA8ejSXySdIyYampQRjw5c58JBxueCyuV1thaIo3QRdj6ADMdV7ECArfwhSP8BGDjVW253dSQgURVGC0BQTHcjewgIAhg4eYpd9rC2BBmdtgTDmElcURTkSVAg6gEavn8+37qOgwC72cvyoETYip77c5g0CzYypKEq3RYWgA7h//kZmPfY56zdtopZE8nKzbUROfTk01tqDYo9w8WpFUZQwoWMER8i+mgaeWlzAiQOzGFNZj48edkdipk0s1+CMEahrSFGUbor2CI6QOQu34Pb4+P3Fozk+s4HUnN52R0IG+BrsOAHoYLGiKN0WFYIjwO3x8ewXO7hgbC8GZcTa5R/TnMnUgclalU66JR0jUBSlm6JCEAp+P7x6ExQubVb88aZS6hp9XHp8b9j4NtSX2dXGoEkIKnZCTCK4ojrXZkVRlBDRMYJQqCuFFc9B5gDoPXF/8dur9/Cj+Dc4sd4NK5+D1HwYfKbdGVjIvXKnuoUURenWqBCEQsDP76nbX+Tx+Zm/di/L5BWiX3nBFp52Z1PLP7hHkNqrE41VFEU5NNQ1FAq1TvqjxiYh+HRzKcZdTYxphLS+du3f4LTSASHwNWjEkKIo3RrtEYRCnSMEHjsnYF9NA//72mqGJdfZ9dSm/wpGXgjRQatsBmf21DkEiqJ0Y7RHEApBPQK/33DTM19RUt3AH7/hzBlIzm0uAgCxiRDtLCyjPQJFUboxKgShEBACTx0frC/my4IyfnPBKIYkOOkjknu0fl6gV6Cho4qidGNUCELBcQ2ZxloeXrCZPpkJXHZ8b6gptvuTc1s/b78QaI9AUZTuiwpBKDhRQzXVVazYWcGNpw0iOspl1yeOim17pa9ACGmcjhEoitJ9USEIhdp99qWmirSEGNsbANsjSO7R9qpjgTWB1TWkKEo3RoUgFBzXkL+xlikDM4mLduYK1BS17RYCdQ0pinJUoEIQCo5rKNbvZsrArKbyQI+gLQJCoFFDiqJ0Y1QIDobPa9cVABJp4MRBwUJwkB5BojNGoPMIFEXpxqgQHIw6Oz5QE5VGojQwNMfx9/u8tqcQSo9AxwgURenGqBAcDGd8YKc/BwCXzx1UbkIbI1DXkKIo3ZiQhEBEXhaR80TkkIRDRM4RkQ0isllE7mpl/09EZLnzt1pEfCKSeSifEXacyWRbvI5LKJBvqKbIvrbXI+g1HvLGQs6IMBqoKIpyZIRasT8KXAVsEpE/icjwg50gIlHAI8AMYCRwpYiMDD7GGPMXY8w4Y8w44GfAQmNM2aHcQNhxBooLje0RBPINNU0ma0cI0vvC9xdBSjvHKIqidDEhCYExZr4x5lvABKAAeF9EFovIdSIS08Zpk4DNxpitxphG4AXgwnY+5krg+dBN7yScMQJval+7fSg9AkVRlKOAkF09IpIFXAtcD3wNPIAVhvfbOCUf2Bm0XeiUtXbtROAc4OU29s8WkaUisrSkpCRUkzuEuvK9+I2Q32+wLfDUgTGw9g07PyClZ6faoyiK0tGEOkbwCvAxkAjMNMZcYIz5tzHmB0BbI6GtTbc1bRw7E/i0LbeQMeYxY8xEY8zEnJycUEzuMMq2r2I3WYwZ6GhYYy1smAeb3oVpP4PouE61R1EUpaMJdT2Ch40xH7a2wxgzsbVybA+gT9B2b2B3G8fOoju6hXwesos+ZZ7rJC7Kc6KDPHXw7s8hdxRM/n7X2qcoitIBhOoaGiEi6YENEckQkZsPcs4SYIiIDBCRWGxl/0bLg0QkDTgNeD1EWzqPHZ8R769jb49TcMU5cwFqiqC8AMZeAVFtDY8oiqIcPYQqBDcYYyoCG8aYcuCG9k4wxniBW4F3gXXAi8aYNSJyo4jcGHToxcB7xpjaQ7K8E6hbM49GE0XckOkQk2gLy7bZ12QdG1AU5dggVNeQS0TEGGNgf2ho7EHOwRgzD5jXomxOi+2ngKdCtKNT8W98jy/8IzhuYH7T3ZZtta8aEqooyjFCqD2Cd4EXRWS6iJyB9ee/Ez6zugG1pSRXbeETM5YxvdNa6RGoECiKcmwQao/gTuD7wE3YaKD3gCfCZVS3wJk/EJPRi/iYKDAuEBeUqxAoinJsEZIQGGP82NnFj4bXnO6Dr76SKKBHjlPhi9heQWNN+6uSKYqiHGWEOo9giIi8JCJrRWRr4C/cxnUlW3buAmBgn15NhQH3UHurkimKohxlhDpG8CS2N+AFTgf+CfwrXEZ1B9ZstZOixw3u21QYGyQEiqIoxwihCkGCMeYDQIwx240xdwNnhM+srsUYw5addu5bUlrQQjQxzlwCTSuhKMoxRKiDxW4nBfUmEbkV2AW0k4j/6GZzcQ2NtRUQA8SlNu3Y3yM4Zm9dUZQIJNQewe3YPEO3AccDVwPXhMmmrmHli/DpgwC8tWoPqVKHkajmq4vtHyPQHoGiKMcOB+0ROJPHLjfG/ASoAa4Lu1Vdwar/QMl6luZfzd8WbOFvmQbxpDQfFA6Igk4mUxTlGOKgPQJjjA84XuQYD5PxujHVe5n9z6XkZyRwSp84iE9rfkyMDhYrinLsEeoYwdfA6yLyH2B/TiBjzCthsaor8LgRXyM+dzm/uuJ04pbVQHxq82M0akhRlGOQUIUgE9hH80ghAxw7QuCtB6CHlDMkNxnclRDXskegUUOKohx7hDqz+NgcFwjG2wBAr+hKeqUlQEMVpPdrfkxChp1VnNS5i+MoiqKEk5CEQESepJXVxYwx3+1wi7oKjxuAkUm1uFwC7qoDXUOTboBBp+s6BIqiHFOE6hp6M+h9PHYNgbZWGzs68VohGJToDIG4K5vPIQBIzITESZ1smKIoSngJ1TXUbFF5EXkemB8Wi7oI43UjQN+YSvD7rWuoZdSQoijKMUioE8paMgToe9CjjiY8gcHiCpthFHOga0hRFOUYJNQxgmqajxHsxa5RcGzg9yF+DwDpvjLrFoIDXUOKoijHIKG6hlLCbUiX4owPACQ2lFi3EGiPQFGUiCDU9QguFpG0oO10EbkobFZ1Nk7EkNvEEFVX3NQj0DECRVEigFDHCH5tjKkMbBhjKoBfh8WirsDpERRF9US8bqiwaxEcMKFMURTlGCRUIWjtuFAS1p0jIhtEZLOI3NXGMdNEZLmIrBGRhSHa07E4QlCdkG+3SzfYV3UNKYoSAYQqBEtF5D4RGSQiA0Xkr8Cy9k5wspY+AswARgJXisjIFsekA38DLjDGjAK+eag30BHU1dm5A740ZyZx6Ub7qq4hRVEigFCF4AdAI/Bv4EWgHrjlIOdMAjYbY7YaYxqBF4ALWxxzFfCKMWYHgDGmOFTDO5LCkjIAovNGAQJbF9kdGjWkKEoEEGrUUC3QqmunHfKBnUHbhcDkFscMBWJE5CMgBXjAGPPPlhcSkdnAbIC+fTt++sKukjKGApm9h4H/W/D1MzanUEx8h3+WoihKdyPUqKH3HTdOYDtDRN492GmtlLXMVxSNXfHsPOBs4JciMvSAk4x5zBgz0RgzMSen4xO+7SmtACA3Mx1O/4Vdd0DdQoqiRAih5hrKdiKFADDGlIvIwRbuLQT6BG335sD8RIVAqdPjqBWRRcBYYGOIdnUIxeU2ICoqNgFSe8HZ/w/2be5MExRFUbqMUIXALyJ9A758EelPK9lIW7AEGCIiA7CL3c/CjgkE8zrwsIhEA7FY19FfQ7SpwyircCJjox1X0MRjP+u2oihKgFCF4BfAJ0Hhnafi+OzbwhjjFZFbgXeBKGCuMWaNiNzo7J9jjFknIu8AKwE/8IQxZvXh3MjhUlnvob6+FmJoEgJFUZQIItTB4ndEZCK28l+ObcnXh3DePGBei7I5Lbb/AvwlRHs7nM3FNcRh8wwRk9BVZiiKonQZoSadux74IdbPvxyYAnxG86Urj0q2ldYSR6PdiI7rWmMURVG6gFDnEfwQOAHYbow5HRgPlITNqk6koLSWRHF6BNHaI1AUJfIIVQjcxhg3gIjEGWPWA8PCZ1bnsa20ltwEA4guQakoSkQS6mBxoTOP4DXgfREp5xhZqnJraS0zEwyQANLa1AdFUZRjm1AHiy923t4tIguANOCdsFnVSRhjKCitJaunHzwaMaQoSmQSao9gP8aYrskQGgaKqhqo9/jIiPVp6KiiKBHL4a5ZfEywrdRmHU2N9mteIUVRIhYVAiAl2qMRQ4qiRCwRLgQ1xEa7iMercwgURYlYIlwI6hiQlWSXp9RZxYqiRCgRLQS7KurpnZEA3nodLFYUJWKJaCHYW1lPz7R48DaoECiKErFErBC4PT7K6zzkpcWDp16jhhRFiVgiVgiKqtwA9EiNB69bewSKokQsESsEeyqtEOSlJagQKIoS0USsEAR6BD3T4sGjUUOKokQuESsEgR6BHSx26zwCRVEilogVgr2VblLiokmONmB8OrNYUZSIJaKFoGcgYgg0akhRlIglYoVgT5W7aQ4B6GCxoigRS8QKQVGlm56p8XZWMagQKIoSsYRVCETkHBHZICKbReSuVvZPE5FKEVnu/P0qnPYE8Pr8FFe7myKGQIVAUZSI5ZAXpgkVEYkCHgHOAgqBJSLyhjFmbYtDPzbGnB8uO1qjtKYRvwlEDFXaQh0jUBQlQglnj2ASsNkYs9UY0wi8AFwYxs8LmT2V1h2UFwgdBY0aUhQlYgmnEOQDO4O2C52ylpwoIitE5G0RGdXahURktogsFZGlJSUlR2xYSbUdIM5JjgdPnS3UCWWKokQo4RQCaaXMtNj+CuhnjBkLPAS81tqFjDGPGWMmGmMm5uTkHLFhZbWNAGQmxzaNEagQKIoSoYRTCAqBPkHbvYHdwQcYY6qMMTXO+3lAjIhkh9EmAMrqHCFIjA3qESSG+2MVRVG6JeEUgiXAEBEZICKxwCzgjeADRKSniIjzfpJjz74w2gRAeW0jCTFRJMRG6YQyRVEinrBFDRljvCJyK/AuEAXMNcasEZEbnf1zgMuAm0TEC9QDs4wxLd1HHc6+2kYyk2LthvYIFEWJcMImBLDf3TOvRdmcoPcPAw+H04bWKG8mBIEegY4RKIoSmUTkzOKyOg8ZLYVAw0cVRYlQIlMIahvITIyxG546iIqFqLB2jhRFUbotESkE5bUeMpOc9Qe8uiiNoiiRTcQJQYPXR02Dl8ykoB6BDhQrihLBRJwQlNd6AJqPEWjCOUVRIpiIE4LArOKsYCHQHoGiKBFMxAlBuTOrOCMxaB6BjhEoihLBRJwQ7M8z1KxHoEKgKErkokKgg8WKokQ4ESkEIpCWEIga0vBRRVEim4gTgvK6RtISYoiOcm5dB4sVRYlwIk4ImiWcA8c1pOGjiqJELhEnBOW1jXYdggDaI1AUJcKJOCGoqPOQHsgzZIyGjyqKEvFEnBDUNXpJjHUSzHkbAKNCoChKRBOBQuAjMTbKbngDaxGoa0hRlMglcoRg2yJ46nzSGovsEpWgi9IoiqIQSULgroKCj4n3VjT1CHRRGkVRlAgSgtgkAOL97qYxgv3rFasQKIoSuUSQECQDkCgNJMS0dA3pGIGiKJFLBAmB7REk4g5yDWmPQFEUJYKEwLb6k8Stg8WKoihBhFUIROQcEdkgIptF5K52jjtBRHwiclnYjAm4hggeI1DXkKIoStiEQESigEeAGcBI4EoRGdnGcX8G3g2XLcB+11ASDQdGDWmPQFGUCCacPYJJwGZjzFZjTCPwAnBhK8f9AHgZKA6jLRAdjxEXieIm2dTAm3dATZHdp0KgKEoEE04hyAd2Bm0XOmX7EZF84GJgTnsXEpHZIrJURJaWlJQcnjUieKMSScJN1r6lsHQurH/L7lMhUBQlggmnEEgrZabF9v3AncYYX3sXMsY8ZoyZaIyZmJOTc9gGeaMTScRNvK/WFuxZYV91QpmiKBFMdBivXQj0CdruDexuccxE4AURAcgGzhURrzHmtXAY1OhKIEncxHlrbIHfA1GxEBXOx6AoitK9CWcNuAQYIiIDgF3ALOCq4AOMMQMC70XkKeDNcIkAWCFIoIE4b3VTobqFFEWJcMImBMYYr4jcio0GigLmGmPWiMiNzv52xwXCQYMrgSQaiPFUNRVq6KiiKBFOWH0ixph5wLwWZa0KgDHm2nDaAuCWBJJdlUhDZVOh9ggURYlwImdmMVAv8SRLg81EmtzTFupAsaIoEU5EjZLWEU8ubnBXQkY/O1CsPQJFUSKcyBICE0diQAiSc2HYuSCtRbkqiqJEDhElBDUmjgTc4K6ArMEw9fauNklRFKXLiagxghp/PNH4oHYfxKd1tTmKoijdgogSgip/rH3TWK1CoCiK4hBRQlDpj2vaUCFQFEUBIkwIqryxTRsqBIqiKECECUF5MyFI7TpDFEVRuhERLATaI1AURYEIEgKvz0+lL1gI0rvMFkVRlO5ExAhBncdHHfFNBdojUBRFASJICOobfdSaICGI0zECRVEUiCAhqGv0UY+GjyqKorQkgoTAS11ACKLiICa+/RMURVEihIgRgvpGHz6i8EXFaW9AURQliIgRgrpGHwD+6CQVAkVRlCAiTghMTJJOJlMURQkiYoQgIzGG04bmIHFJGjGkKIoSRMSsRzB5YBaTB2bBqp/oZDJFUZQgIkYI9nPcZV1tgaIoSrcirK4hETlHRDaIyGYRuauV/ReKyEoRWS4iS0VkajjtURRFUQ4kbD0CEYkCHgHOAgqBJSLyhjFmbdBhHwBvGGOMiIwBXgSGh8smRVEU5UDC2SOYBGw2xmw1xjQCLwAXBh9gjKkxxhhnMwkwKIqiKJ1KOIUgH9gZtF3olDVDRC4WkfXAW8B3W7uQiMx2XEdLS0pKwmKsoihKpBJOIZBWyg5o8RtjXjXGDAcuAn7X2oWMMY8ZYyYaYybm5OR0rJWKoigRTjiFoBDoE7TdG9jd1sHGmEXAIBHJDqNNiqIoSgvCKQRLgCEiMkBEYoFZwBvBB4jIYBER5/0EIBbYF0abFEVRlBaELWrIGOMVkVuBd4EoYK4xZo2I3OjsnwNcCnxHRDxAPXBF0OCxoiiK0gnI0VbvikgJsP0wT88GSjvQnI6ku9qmdh0a3dUu6L62qV2HxuHa1c8Y0+og61EnBEeCiCw1xkzsajtao7vapnYdGt3VLui+tqldh0Y47IqYpHOKoihK66gQKIqiRDiRJgSPdbUB7dBdbVO7Do3uahd0X9vUrkOjw+2KqDECRVEU5UAirUegKIqitECFQFEUJcKJGCE42NoInWhHHxFZICLrRGSNiPzQKb9bRHY5azMsF5Fzu8C2AhFZFVgfwinLFJH3RWST85rRBXYNC3ouy0WkSkRu74pnJiJzRaRYRFYHlbX5jETkZ853boOInN3Jdv1FRNY7a368KiLpTnl/EakPem5zOtmuNv9vnfW82rHt30F2FYjIcqe8U55ZO/VDeL9jxphj/g87s3kLMBCbxmIFMLKLbMkDJjjvU4CNwEjgbuDHXfycCoDsFmX/B9zlvL8L+HM3+F/uBfp1xTMDTgUmAKsP9oyc/+sKIA4Y4HwHozrRrm8A0c77PwfZ1T/4uC54Xq3+3zrzebVlW4v99wK/6sxn1k79ENbvWKT0CA66NkJnYYzZY4z5ynlfDayjlfTc3YgLgaed909js8R2JdOBLcaYw51dfkQYmxyxrEVxW8/oQuAFY0yDMWYbsBn7XewUu4wx7xljvM7m59jEj51KG8+rLTrteR3MNicH2uXA8+H6/DZsaqt+COt3LFKEIKS1ETobEekPjAe+cIpudbrxc7vCBYNNE/6eiCwTkdlOWQ9jzB6wX1IgtwvsCmYWzX+cXf3MoO1n1J2+d98F3g7aHiAiX4vIQhE5pQvsae3/1p2e1ylAkTFmU1BZpz6zFvVDWL9jkSIEIa2N0JmISDLwMnC7MaYKeBQYBIwD9mC7pZ3NycaYCcAM4BYRObULbGgTsVlsLwD+4xR1h2fWHt3ieycivwC8wLNO0R6grzFmPPAj4DkRSe1Ek9r6v3WL5+VwJc0bHJ36zFqpH9o8tJWyQ35mkSIEh7Q2QrgRkRjsP/lZY8wrAMaYImOMzxjjBx4njF3itjDG7HZei4FXHRuKRCTPsTsPKO5su4KYAXxljCmC7vHMHNp6Rl3+vRORa4DzgW8Zx6nsuBH2Oe+XYf3KQzvLpnb+b13+vABEJBq4BPh3oKwzn1lr9QNh/o5FihAcdG2EzsLxPf4DWGeMuS+oPC/osIuB1S3PDbNdSSKSEniPHWhcjX1O1ziHXQO83pl2taBZK62rn1kQbT2jN4BZIhInIgOAIcCXnWWUiJwD3AlcYIypCyrPEZEo5/1Ax66tnWhXW/+3Ln1eQZwJrDfGFAYKOuuZtVU/EO7vWLhHwbvLH3AudgR+C/CLLrRjKrbrthJY7vydC/wLWOWUvwHkdbJdA7HRByuANYFnBGQBHwCbnNfMLnpuidhFi9KCyjr9mWGFaA/gwbbGvtfeMwJ+4XznNgAzOtmuzVj/ceB7Nsc59lLnf7wC+AqY2cl2tfl/66zn1ZZtTvlTwI0tju2UZ9ZO/RDW75immFAURYlwIsU1pCiKorSBCoGiKEqEo0KgKIoS4agQKIqiRDgqBIqiKBGOCoGidCIiMk1E3uxqOxQlGBUCRVGUCEeFQFFaQUSuFpEvndzzfxeRKBGpEZF7ReQrEflARHKcY8eJyOfSlPc/wykfLCLzRWSFc84g5/LJIvKS2LUCnnVmkypKl6FCoCgtEJERwBXYJHzjAB/wLSAJm+toArAQ+LVzyj+BO40xY7AzZgPlzwKPGGPGAidhZ7GCzSh5OzaX/EDg5DDfkqK0S3RXG6Ao3ZDpwPHAEqexnoBN8uWnKRHZM8ArIpIGpBtjFjrlTwP/cfI25RtjXgUwxrgBnOt9aZw8Ns4KWP2BT8J+V4rSBioEinIgAjxtjPlZs0KRX7Y4rr38LO25exqC3vvQ36HSxahrSFEO5APgMhHJhf3rxfbD/l4uc465CvjEGFMJlActVPJtYKGxOeQLReQi5xpxIpLYmTehKKGiLRFFaYExZq2I/C92tTYXNjvlLUAtMEpElgGV2HEEsGmB5zgV/VbgOqf828DfReS3zjW+2Ym3oSgho9lHFSVERKTGGJPc1XYoSkejriFFUZQIR3sEiqIoEY72CBRFUSIcFQJFUZQIR4VAURQlwlEhUBRFiXBUCBRFUSKc/w9j+YKlQ1dr0gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "first200trainacc = first100trainacc + train_acc \n",
    "history = history2.history['accuracy'] + history3.history['accuracy']\n",
    "plt.plot(history)\n",
    "plt.plot(first200trainacc)\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 30, 30, 64)   1792        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 30, 30, 64)   256         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 15, 15, 64)   0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 15, 15, 64)   36928       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 15, 15, 64)   256         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_32 (TensorFlow [(None, 15, 15, 64)] 0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 15, 15, 64)   36928       tf_op_layer_Relu_32[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 15, 15, 64)   256         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_16 (TensorFlowO [(None, 15, 15, 64)] 0           batch_normalization_41[0][0]     \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_33 (TensorFlow [(None, 15, 15, 64)] 0           tf_op_layer_add_16[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 15, 15, 64)   36928       tf_op_layer_Relu_33[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 15, 15, 64)   256         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_34 (TensorFlow [(None, 15, 15, 64)] 0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 15, 15, 64)   36928       tf_op_layer_Relu_34[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 15, 15, 64)   256         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_17 (TensorFlowO [(None, 15, 15, 64)] 0           batch_normalization_43[0][0]     \n",
      "                                                                 tf_op_layer_Relu_33[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_35 (TensorFlow [(None, 15, 15, 64)] 0           tf_op_layer_add_17[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 15, 15, 64)   36928       tf_op_layer_Relu_35[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 15, 15, 64)   256         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_36 (TensorFlow [(None, 15, 15, 64)] 0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 15, 15, 64)   36928       tf_op_layer_Relu_36[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 15, 15, 64)   256         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_18 (TensorFlowO [(None, 15, 15, 64)] 0           batch_normalization_45[0][0]     \n",
      "                                                                 tf_op_layer_Relu_35[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_37 (TensorFlow [(None, 15, 15, 64)] 0           tf_op_layer_add_18[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_FractionalMaxPool_4 [(None, 10, 10, 64), 0           tf_op_layer_Relu_37[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 10, 10, 128)  73856       tf_op_layer_FractionalMaxPool_4[0\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 10, 10, 128)  512         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_38 (TensorFlow [(None, 10, 10, 128) 0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 10, 10, 128)  147584      tf_op_layer_Relu_38[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 10, 10, 128)  73856       tf_op_layer_FractionalMaxPool_4[0\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 10, 10, 128)  512         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 10, 10, 128)  512         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_19 (TensorFlowO [(None, 10, 10, 128) 0           batch_normalization_47[0][0]     \n",
      "                                                                 batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_39 (TensorFlow [(None, 10, 10, 128) 0           tf_op_layer_add_19[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 10, 10, 128)  147584      tf_op_layer_Relu_39[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 10, 10, 128)  512         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_40 (TensorFlow [(None, 10, 10, 128) 0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 10, 10, 128)  147584      tf_op_layer_Relu_40[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 10, 10, 128)  512         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_20 (TensorFlowO [(None, 10, 10, 128) 0           batch_normalization_50[0][0]     \n",
      "                                                                 tf_op_layer_Relu_39[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_41 (TensorFlow [(None, 10, 10, 128) 0           tf_op_layer_add_20[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 10, 10, 128)  147584      tf_op_layer_Relu_41[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 10, 10, 128)  512         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_42 (TensorFlow [(None, 10, 10, 128) 0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 10, 10, 128)  147584      tf_op_layer_Relu_42[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 10, 10, 128)  512         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_21 (TensorFlowO [(None, 10, 10, 128) 0           batch_normalization_52[0][0]     \n",
      "                                                                 tf_op_layer_Relu_41[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_43 (TensorFlow [(None, 10, 10, 128) 0           tf_op_layer_add_21[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 10, 10, 128)  147584      tf_op_layer_Relu_43[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 10, 10, 128)  512         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_44 (TensorFlow [(None, 10, 10, 128) 0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 10, 10, 128)  147584      tf_op_layer_Relu_44[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 10, 10, 128)  512         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_22 (TensorFlowO [(None, 10, 10, 128) 0           batch_normalization_54[0][0]     \n",
      "                                                                 tf_op_layer_Relu_43[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_45 (TensorFlow [(None, 10, 10, 128) 0           tf_op_layer_add_22[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_FractionalMaxPool_5 [(None, 7, 7, 128),  0           tf_op_layer_Relu_45[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 7, 7, 256)    295168      tf_op_layer_FractionalMaxPool_5[0\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 7, 7, 256)    1024        conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_46 (TensorFlow [(None, 7, 7, 256)]  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 7, 7, 256)    590080      tf_op_layer_Relu_46[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 7, 7, 256)    295168      tf_op_layer_FractionalMaxPool_5[0\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 7, 7, 256)    1024        conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 7, 7, 256)    1024        conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_23 (TensorFlowO [(None, 7, 7, 256)]  0           batch_normalization_56[0][0]     \n",
      "                                                                 batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_47 (TensorFlow [(None, 7, 7, 256)]  0           tf_op_layer_add_23[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 7, 7, 256)    590080      tf_op_layer_Relu_47[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 7, 7, 256)    1024        conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_48 (TensorFlow [(None, 7, 7, 256)]  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 7, 7, 256)    590080      tf_op_layer_Relu_48[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 7, 7, 256)    1024        conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_24 (TensorFlowO [(None, 7, 7, 256)]  0           batch_normalization_59[0][0]     \n",
      "                                                                 tf_op_layer_Relu_47[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_49 (TensorFlow [(None, 7, 7, 256)]  0           tf_op_layer_add_24[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 7, 7, 256)    590080      tf_op_layer_Relu_49[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 7, 7, 256)    1024        conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_50 (TensorFlow [(None, 7, 7, 256)]  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 7, 7, 256)    590080      tf_op_layer_Relu_50[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 7, 7, 256)    1024        conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_25 (TensorFlowO [(None, 7, 7, 256)]  0           batch_normalization_61[0][0]     \n",
      "                                                                 tf_op_layer_Relu_49[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_51 (TensorFlow [(None, 7, 7, 256)]  0           tf_op_layer_add_25[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 7, 7, 256)    590080      tf_op_layer_Relu_51[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 7, 7, 256)    1024        conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_52 (TensorFlow [(None, 7, 7, 256)]  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 7, 7, 256)    590080      tf_op_layer_Relu_52[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 7, 7, 256)    1024        conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_26 (TensorFlowO [(None, 7, 7, 256)]  0           batch_normalization_63[0][0]     \n",
      "                                                                 tf_op_layer_Relu_51[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_53 (TensorFlow [(None, 7, 7, 256)]  0           tf_op_layer_add_26[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 7, 7, 256)    590080      tf_op_layer_Relu_53[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 7, 7, 256)    1024        conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_54 (TensorFlow [(None, 7, 7, 256)]  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 7, 7, 256)    590080      tf_op_layer_Relu_54[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 7, 7, 256)    1024        conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_27 (TensorFlowO [(None, 7, 7, 256)]  0           batch_normalization_65[0][0]     \n",
      "                                                                 tf_op_layer_Relu_53[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_55 (TensorFlow [(None, 7, 7, 256)]  0           tf_op_layer_add_27[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 7, 7, 256)    590080      tf_op_layer_Relu_55[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 7, 7, 256)    1024        conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_56 (TensorFlow [(None, 7, 7, 256)]  0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 7, 7, 256)    590080      tf_op_layer_Relu_56[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 7, 7, 256)    1024        conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_28 (TensorFlowO [(None, 7, 7, 256)]  0           batch_normalization_67[0][0]     \n",
      "                                                                 tf_op_layer_Relu_55[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_57 (TensorFlow [(None, 7, 7, 256)]  0           tf_op_layer_add_28[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_FractionalMaxPool_6 [(None, 5, 5, 256),  0           tf_op_layer_Relu_57[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 5, 5, 512)    1180160     tf_op_layer_FractionalMaxPool_6[0\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 5, 5, 512)    2048        conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_58 (TensorFlow [(None, 5, 5, 512)]  0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 5, 5, 512)    2359808     tf_op_layer_Relu_58[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 5, 5, 512)    1180160     tf_op_layer_FractionalMaxPool_6[0\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 5, 5, 512)    2048        conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 5, 5, 512)    2048        conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_29 (TensorFlowO [(None, 5, 5, 512)]  0           batch_normalization_69[0][0]     \n",
      "                                                                 batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_59 (TensorFlow [(None, 5, 5, 512)]  0           tf_op_layer_add_29[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 5, 5, 512)    2359808     tf_op_layer_Relu_59[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 5, 5, 512)    2048        conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_60 (TensorFlow [(None, 5, 5, 512)]  0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 5, 5, 512)    2359808     tf_op_layer_Relu_60[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 5, 5, 512)    2048        conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_30 (TensorFlowO [(None, 5, 5, 512)]  0           batch_normalization_72[0][0]     \n",
      "                                                                 tf_op_layer_Relu_59[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_61 (TensorFlow [(None, 5, 5, 512)]  0           tf_op_layer_add_30[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 5, 5, 512)    2359808     tf_op_layer_Relu_61[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 5, 5, 512)    2048        conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_62 (TensorFlow [(None, 5, 5, 512)]  0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 5, 5, 512)    2359808     tf_op_layer_Relu_62[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 5, 5, 512)    2048        conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_31 (TensorFlowO [(None, 5, 5, 512)]  0           batch_normalization_74[0][0]     \n",
      "                                                                 tf_op_layer_Relu_61[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_63 (TensorFlow [(None, 5, 5, 512)]  0           tf_op_layer_add_31[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_FractionalMaxPool_7 [(None, 3, 3, 512),  0           tf_op_layer_Relu_63[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 4608)         0           tf_op_layer_FractionalMaxPool_7[0\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 4608)         18432       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1024)         4719616     batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 1024)         4096        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 512)          524800      batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 512)          2048        dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 10)           5130        batch_normalization_77[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 27,952,906\n",
      "Trainable params: 27,923,594\n",
      "Non-trainable params: 29,312\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "def scheduler(epoch):\n",
    "    if epoch < 150:\n",
    "        return 0.1\n",
    "    if epoch < 225:\n",
    "        return 0.01\n",
    "    return 0.001\n",
    "\n",
    "\n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "test = TestCallback((xtest, ytest))\n",
    "callbacks = [change_lr,test]\n",
    " \n",
    "    \n",
    "img_input = Input(shape=(32,32,3))\n",
    "output = Resnet50withfractionpooling(img_input)\n",
    "resnet50 = Model(img_input,output)\n",
    "\n",
    "resnet50.summary()\n",
    "\n",
    "sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\n",
    "resnet50.compile(loss='sparse_categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 391 steps\n",
      "Epoch 1/300\n",
      "10000/10000 [==============================] - 7s 710us/sample - loss: 15.1590 - accuracy: 0.1151\n",
      "391/391 [==============================] - 82s 209ms/step - loss: 18.2256 - accuracy: 0.2627\n",
      "Epoch 2/300\n",
      "10000/10000 [==============================] - 6s 601us/sample - loss: 8.3736 - accuracy: 0.1985\n",
      "391/391 [==============================] - 75s 192ms/step - loss: 9.3364 - accuracy: 0.3820\n",
      "Epoch 3/300\n",
      "10000/10000 [==============================] - 6s 626us/sample - loss: 5.3556 - accuracy: 0.2093\n",
      "391/391 [==============================] - 75s 193ms/step - loss: 5.1086 - accuracy: 0.4414\n",
      "Epoch 4/300\n",
      "10000/10000 [==============================] - 6s 593us/sample - loss: 3.2500 - accuracy: 0.3362\n",
      "391/391 [==============================] - 74s 191ms/step - loss: 3.2096 - accuracy: 0.4875\n",
      "Epoch 5/300\n",
      "10000/10000 [==============================] - 6s 601us/sample - loss: 2.1672 - accuracy: 0.4566\n",
      "391/391 [==============================] - 74s 190ms/step - loss: 2.1787 - accuracy: 0.5383\n",
      "Epoch 6/300\n",
      "10000/10000 [==============================] - 6s 598us/sample - loss: 1.8813 - accuracy: 0.4919\n",
      "391/391 [==============================] - 74s 190ms/step - loss: 1.7007 - accuracy: 0.5746\n",
      "Epoch 7/300\n",
      "10000/10000 [==============================] - 6s 596us/sample - loss: 1.5366 - accuracy: 0.5829\n",
      "391/391 [==============================] - 74s 189ms/step - loss: 1.4749 - accuracy: 0.6010\n",
      "Epoch 8/300\n",
      "10000/10000 [==============================] - 6s 596us/sample - loss: 1.6934 - accuracy: 0.5234\n",
      "391/391 [==============================] - 74s 190ms/step - loss: 1.3600 - accuracy: 0.6263\n",
      "Epoch 9/300\n",
      "10000/10000 [==============================] - 6s 597us/sample - loss: 1.2682 - accuracy: 0.6564\n",
      "391/391 [==============================] - 74s 190ms/step - loss: 1.2971 - accuracy: 0.6485\n",
      "Epoch 10/300\n",
      "10000/10000 [==============================] - 6s 595us/sample - loss: 1.5956 - accuracy: 0.5640\n",
      "391/391 [==============================] - 74s 189ms/step - loss: 1.2648 - accuracy: 0.6608\n",
      "Epoch 11/300\n",
      "10000/10000 [==============================] - 6s 595us/sample - loss: 1.3003 - accuracy: 0.6571\n",
      "391/391 [==============================] - 74s 189ms/step - loss: 1.2253 - accuracy: 0.6798\n",
      "Epoch 12/300\n",
      "10000/10000 [==============================] - 6s 595us/sample - loss: 1.3713 - accuracy: 0.6457\n",
      "391/391 [==============================] - 74s 190ms/step - loss: 1.2686 - accuracy: 0.6760\n",
      "Epoch 13/300\n",
      "10000/10000 [==============================] - 6s 602us/sample - loss: 1.5147 - accuracy: 0.6231\n",
      "391/391 [==============================] - 74s 189ms/step - loss: 1.2383 - accuracy: 0.6912\n",
      "Epoch 14/300\n",
      "10000/10000 [==============================] - 6s 595us/sample - loss: 1.1411 - accuracy: 0.7191\n",
      "391/391 [==============================] - 73s 188ms/step - loss: 1.1759 - accuracy: 0.7085\n",
      "Epoch 15/300\n",
      "10000/10000 [==============================] - 6s 601us/sample - loss: 1.4860 - accuracy: 0.6206\n",
      "391/391 [==============================] - 74s 188ms/step - loss: 1.1494 - accuracy: 0.7140\n",
      "Epoch 16/300\n",
      "10000/10000 [==============================] - 6s 597us/sample - loss: 8.9541 - accuracy: 0.1919\n",
      "391/391 [==============================] - 74s 189ms/step - loss: 1.1829 - accuracy: 0.7102\n",
      "Epoch 17/300\n",
      "10000/10000 [==============================] - 6s 599us/sample - loss: 1.2172 - accuracy: 0.7014\n",
      "391/391 [==============================] - 74s 189ms/step - loss: 1.1676 - accuracy: 0.7255\n",
      "Epoch 18/300\n",
      "10000/10000 [==============================] - 6s 598us/sample - loss: 2.0403 - accuracy: 0.5167\n",
      "391/391 [==============================] - 74s 188ms/step - loss: 1.1338 - accuracy: 0.7306\n",
      "Epoch 19/300\n",
      "10000/10000 [==============================] - 6s 597us/sample - loss: 1.4805 - accuracy: 0.6374\n",
      "391/391 [==============================] - 74s 188ms/step - loss: 1.1679 - accuracy: 0.7260\n",
      "Epoch 20/300\n",
      "10000/10000 [==============================] - 6s 631us/sample - loss: 1.4064 - accuracy: 0.6442\n",
      "391/391 [==============================] - 74s 189ms/step - loss: 1.1453 - accuracy: 0.7355\n",
      "Epoch 21/300\n",
      "10000/10000 [==============================] - 6s 598us/sample - loss: 1.4581 - accuracy: 0.6355\n",
      "391/391 [==============================] - 73s 187ms/step - loss: 1.1235 - accuracy: 0.7412\n",
      "Epoch 22/300\n",
      "10000/10000 [==============================] - 6s 596us/sample - loss: 1.3719 - accuracy: 0.6646\n",
      "391/391 [==============================] - 73s 187ms/step - loss: 1.1170 - accuracy: 0.7459\n",
      "Epoch 23/300\n",
      "10000/10000 [==============================] - 6s 596us/sample - loss: 8.4873 - accuracy: 0.1876\n",
      "391/391 [==============================] - 73s 187ms/step - loss: 1.1160 - accuracy: 0.7452\n",
      "Epoch 24/300\n",
      "10000/10000 [==============================] - 6s 595us/sample - loss: 1.3116 - accuracy: 0.6850\n",
      "391/391 [==============================] - 74s 188ms/step - loss: 1.1498 - accuracy: 0.7420\n",
      "Epoch 25/300\n",
      "10000/10000 [==============================] - 6s 598us/sample - loss: 1.5810 - accuracy: 0.6571\n",
      "391/391 [==============================] - 74s 188ms/step - loss: 1.1005 - accuracy: 0.7548\n",
      "Epoch 26/300\n",
      "10000/10000 [==============================] - 6s 596us/sample - loss: 1.2002 - accuracy: 0.7207\n",
      "391/391 [==============================] - 73s 187ms/step - loss: 1.0889 - accuracy: 0.7563\n",
      "Epoch 27/300\n",
      "10000/10000 [==============================] - 6s 594us/sample - loss: 1.1618 - accuracy: 0.7353\n",
      "391/391 [==============================] - 73s 187ms/step - loss: 1.0951 - accuracy: 0.7571\n",
      "Epoch 28/300\n",
      "10000/10000 [==============================] - 6s 597us/sample - loss: 1.1455 - accuracy: 0.7379\n",
      "391/391 [==============================] - 73s 187ms/step - loss: 1.0830 - accuracy: 0.7624\n",
      "Epoch 29/300\n",
      "10000/10000 [==============================] - 6s 601us/sample - loss: 1.2756 - accuracy: 0.7077\n",
      "391/391 [==============================] - 73s 187ms/step - loss: 1.0874 - accuracy: 0.7600\n",
      "Epoch 30/300\n",
      "10000/10000 [==============================] - 6s 597us/sample - loss: 1.2242 - accuracy: 0.7138\n",
      "391/391 [==============================] - 73s 186ms/step - loss: 1.0923 - accuracy: 0.7639\n",
      "Epoch 31/300\n",
      "10000/10000 [==============================] - 6s 597us/sample - loss: 1.2023 - accuracy: 0.7202\n",
      "391/391 [==============================] - 73s 186ms/step - loss: 1.0798 - accuracy: 0.7689\n",
      "Epoch 32/300\n",
      "10000/10000 [==============================] - 6s 612us/sample - loss: 1.1257 - accuracy: 0.7471\n",
      "391/391 [==============================] - 73s 186ms/step - loss: 1.0810 - accuracy: 0.7680\n",
      "Epoch 33/300\n",
      "10000/10000 [==============================] - 6s 597us/sample - loss: 1.3019 - accuracy: 0.7052\n",
      "391/391 [==============================] - 73s 186ms/step - loss: 1.0891 - accuracy: 0.7658\n",
      "Epoch 34/300\n",
      "10000/10000 [==============================] - 6s 597us/sample - loss: 1.2990 - accuracy: 0.7048\n",
      "391/391 [==============================] - 73s 186ms/step - loss: 1.0793 - accuracy: 0.7697\n",
      "Epoch 35/300\n",
      "10000/10000 [==============================] - 6s 597us/sample - loss: 1.3640 - accuracy: 0.6832\n",
      "391/391 [==============================] - 72s 185ms/step - loss: 1.0649 - accuracy: 0.7763\n",
      "Epoch 36/300\n",
      "10000/10000 [==============================] - 6s 597us/sample - loss: 1.4252 - accuracy: 0.6733\n",
      "391/391 [==============================] - 72s 185ms/step - loss: 1.0595 - accuracy: 0.7761\n",
      "Epoch 37/300\n",
      "10000/10000 [==============================] - 6s 642us/sample - loss: 1.1915 - accuracy: 0.7413\n",
      "391/391 [==============================] - 73s 186ms/step - loss: 1.0770 - accuracy: 0.7726\n",
      "Epoch 38/300\n",
      "10000/10000 [==============================] - 6s 595us/sample - loss: 1.2280 - accuracy: 0.7303\n",
      "391/391 [==============================] - 72s 184ms/step - loss: 1.0657 - accuracy: 0.7767\n",
      "Epoch 39/300\n",
      "10000/10000 [==============================] - 6s 594us/sample - loss: 1.3348 - accuracy: 0.6942\n",
      "391/391 [==============================] - 72s 183ms/step - loss: 1.0727 - accuracy: 0.7745\n",
      "Epoch 40/300\n",
      "10000/10000 [==============================] - 6s 597us/sample - loss: 1.2006 - accuracy: 0.7376\n",
      "391/391 [==============================] - 72s 183ms/step - loss: 1.0663 - accuracy: 0.7772\n",
      "Epoch 41/300\n",
      "10000/10000 [==============================] - 6s 597us/sample - loss: 1.5156 - accuracy: 0.6271\n",
      "391/391 [==============================] - 72s 184ms/step - loss: 1.0668 - accuracy: 0.7804\n",
      "Epoch 42/300\n",
      "10000/10000 [==============================] - 6s 596us/sample - loss: 1.7526 - accuracy: 0.6290\n",
      "391/391 [==============================] - 72s 183ms/step - loss: 1.1598 - accuracy: 0.7573\n",
      "Epoch 43/300\n",
      "10000/10000 [==============================] - 6s 597us/sample - loss: 2.1273 - accuracy: 0.6097\n",
      "391/391 [==============================] - 72s 183ms/step - loss: 1.1622 - accuracy: 0.7612\n",
      "Epoch 44/300\n",
      "10000/10000 [==============================] - 6s 595us/sample - loss: 1.3401 - accuracy: 0.6892\n",
      "391/391 [==============================] - 72s 183ms/step - loss: 1.1124 - accuracy: 0.7753\n",
      "Epoch 45/300\n",
      "10000/10000 [==============================] - 6s 603us/sample - loss: 1.3818 - accuracy: 0.6809\n",
      "391/391 [==============================] - 72s 183ms/step - loss: 1.0814 - accuracy: 0.7774\n",
      "Epoch 46/300\n",
      "10000/10000 [==============================] - 6s 596us/sample - loss: 1.1594 - accuracy: 0.7519\n",
      "391/391 [==============================] - 72s 183ms/step - loss: 1.0580 - accuracy: 0.7830\n",
      "Epoch 47/300\n",
      "10000/10000 [==============================] - 6s 596us/sample - loss: 1.5294 - accuracy: 0.6575\n",
      "391/391 [==============================] - 71s 182ms/step - loss: 1.0579 - accuracy: 0.7844\n",
      "Epoch 48/300\n",
      "10000/10000 [==============================] - 6s 598us/sample - loss: 1.1675 - accuracy: 0.7489\n",
      "391/391 [==============================] - 71s 183ms/step - loss: 1.0689 - accuracy: 0.7819\n",
      "Epoch 49/300\n",
      "10000/10000 [==============================] - 6s 605us/sample - loss: 1.2176 - accuracy: 0.7393\n",
      "391/391 [==============================] - 71s 183ms/step - loss: 1.0711 - accuracy: 0.7824\n",
      "Epoch 50/300\n",
      "10000/10000 [==============================] - 6s 630us/sample - loss: 11.7411 - accuracy: 0.4367\n",
      "391/391 [==============================] - 72s 183ms/step - loss: 1.0905 - accuracy: 0.7750\n",
      "Epoch 51/300\n",
      "10000/10000 [==============================] - 6s 595us/sample - loss: 1.1698 - accuracy: 0.7557\n",
      "391/391 [==============================] - 71s 182ms/step - loss: 1.1242 - accuracy: 0.7738\n",
      "Epoch 52/300\n",
      "10000/10000 [==============================] - 6s 595us/sample - loss: 1.2331 - accuracy: 0.7226\n",
      "391/391 [==============================] - 71s 182ms/step - loss: 1.0636 - accuracy: 0.7866\n",
      "Epoch 53/300\n",
      "10000/10000 [==============================] - 6s 601us/sample - loss: 1.2100 - accuracy: 0.7437\n",
      "391/391 [==============================] - 71s 182ms/step - loss: 1.0527 - accuracy: 0.7867\n",
      "Epoch 54/300\n",
      "10000/10000 [==============================] - 6s 594us/sample - loss: 1.0729 - accuracy: 0.7801\n",
      "391/391 [==============================] - 71s 182ms/step - loss: 1.0612 - accuracy: 0.7865\n",
      "Epoch 55/300\n",
      "10000/10000 [==============================] - 6s 595us/sample - loss: 1.2070 - accuracy: 0.7371\n",
      "391/391 [==============================] - 71s 182ms/step - loss: 1.0422 - accuracy: 0.7881\n",
      "Epoch 56/300\n",
      "10000/10000 [==============================] - 6s 594us/sample - loss: 1.2027 - accuracy: 0.7319\n",
      "391/391 [==============================] - 71s 181ms/step - loss: 1.0398 - accuracy: 0.7882\n",
      "Epoch 57/300\n",
      "10000/10000 [==============================] - 6s 595us/sample - loss: 1.1187 - accuracy: 0.7659\n",
      "391/391 [==============================] - 71s 180ms/step - loss: 1.0416 - accuracy: 0.7915\n",
      "Epoch 58/300\n",
      "10000/10000 [==============================] - 6s 599us/sample - loss: 1.3210 - accuracy: 0.7250\n",
      "391/391 [==============================] - 71s 181ms/step - loss: 1.0322 - accuracy: 0.7936\n",
      "Epoch 59/300\n",
      "10000/10000 [==============================] - 6s 598us/sample - loss: 1.2828 - accuracy: 0.7296\n",
      "391/391 [==============================] - 71s 181ms/step - loss: 1.0523 - accuracy: 0.7909\n",
      "Epoch 60/300\n",
      "10000/10000 [==============================] - 6s 593us/sample - loss: 1.2338 - accuracy: 0.7433\n",
      "391/391 [==============================] - 70s 180ms/step - loss: 1.0582 - accuracy: 0.7920\n",
      "Epoch 61/300\n",
      "10000/10000 [==============================] - 6s 599us/sample - loss: 1.0458 - accuracy: 0.7913\n",
      "391/391 [==============================] - 70s 180ms/step - loss: 1.0391 - accuracy: 0.7933\n",
      "Epoch 62/300\n",
      "10000/10000 [==============================] - 6s 623us/sample - loss: 1.1425 - accuracy: 0.7580\n",
      "391/391 [==============================] - 71s 180ms/step - loss: 1.0376 - accuracy: 0.7947\n",
      "Epoch 63/300\n",
      "10000/10000 [==============================] - 6s 599us/sample - loss: 1.2685 - accuracy: 0.7213\n",
      "391/391 [==============================] - 70s 179ms/step - loss: 1.0423 - accuracy: 0.7949\n",
      "Epoch 64/300\n",
      "10000/10000 [==============================] - 6s 593us/sample - loss: 1.3088 - accuracy: 0.7226\n",
      "391/391 [==============================] - 70s 179ms/step - loss: 1.0306 - accuracy: 0.7962\n",
      "Epoch 65/300\n",
      "10000/10000 [==============================] - 6s 593us/sample - loss: 1.1401 - accuracy: 0.7612\n",
      "391/391 [==============================] - 69s 177ms/step - loss: 1.0346 - accuracy: 0.7946\n",
      "Epoch 66/300\n",
      "10000/10000 [==============================] - 6s 590us/sample - loss: 1.5463 - accuracy: 0.6271\n",
      "391/391 [==============================] - 69s 176ms/step - loss: 1.0284 - accuracy: 0.7970\n",
      "Epoch 67/300\n",
      "10000/10000 [==============================] - 6s 595us/sample - loss: 1.0938 - accuracy: 0.7849\n",
      "391/391 [==============================] - 69s 176ms/step - loss: 1.0278 - accuracy: 0.7956\n",
      "Epoch 68/300\n",
      "10000/10000 [==============================] - 6s 590us/sample - loss: 1.2661 - accuracy: 0.7269\n",
      "391/391 [==============================] - 69s 175ms/step - loss: 1.0266 - accuracy: 0.7966\n",
      "Epoch 69/300\n",
      "10000/10000 [==============================] - 6s 594us/sample - loss: 1.1489 - accuracy: 0.7529\n",
      "391/391 [==============================] - 68s 175ms/step - loss: 1.0304 - accuracy: 0.7967\n",
      "Epoch 70/300\n",
      "10000/10000 [==============================] - 6s 592us/sample - loss: 1.2690 - accuracy: 0.7147\n",
      "391/391 [==============================] - 68s 175ms/step - loss: 1.0336 - accuracy: 0.7961\n",
      "Epoch 71/300\n",
      "10000/10000 [==============================] - 6s 593us/sample - loss: 1.3134 - accuracy: 0.7084\n",
      "391/391 [==============================] - 68s 175ms/step - loss: 1.0290 - accuracy: 0.7973\n",
      "Epoch 72/300\n",
      "10000/10000 [==============================] - 6s 593us/sample - loss: 1.4571 - accuracy: 0.6661\n",
      "391/391 [==============================] - 68s 174ms/step - loss: 1.0288 - accuracy: 0.7978\n",
      "Epoch 73/300\n",
      "10000/10000 [==============================] - 6s 593us/sample - loss: 1.2948 - accuracy: 0.7082\n",
      "391/391 [==============================] - 68s 175ms/step - loss: 1.0341 - accuracy: 0.7984\n",
      "Epoch 74/300\n",
      "10000/10000 [==============================] - 6s 590us/sample - loss: 1.2262 - accuracy: 0.7425\n",
      "391/391 [==============================] - 68s 173ms/step - loss: 1.0371 - accuracy: 0.7978\n",
      "Epoch 75/300\n",
      "10000/10000 [==============================] - 6s 591us/sample - loss: 1.4240 - accuracy: 0.6576\n",
      "391/391 [==============================] - 68s 174ms/step - loss: 1.0325 - accuracy: 0.7978\n",
      "Epoch 76/300\n",
      "10000/10000 [==============================] - 6s 591us/sample - loss: 1.0923 - accuracy: 0.7823\n",
      "391/391 [==============================] - 68s 174ms/step - loss: 1.0321 - accuracy: 0.7976\n",
      "Epoch 77/300\n",
      "10000/10000 [==============================] - 6s 595us/sample - loss: 1.0829 - accuracy: 0.7815\n",
      "391/391 [==============================] - 68s 175ms/step - loss: 1.0292 - accuracy: 0.8022\n",
      "Epoch 78/300\n",
      "10000/10000 [==============================] - 6s 592us/sample - loss: 1.4158 - accuracy: 0.6877\n",
      "391/391 [==============================] - 68s 174ms/step - loss: 1.0278 - accuracy: 0.8001\n",
      "Epoch 79/300\n",
      "10000/10000 [==============================] - 6s 594us/sample - loss: 1.1981 - accuracy: 0.7544\n",
      "391/391 [==============================] - 68s 174ms/step - loss: 1.0430 - accuracy: 0.7982\n",
      "Epoch 80/300\n",
      "10000/10000 [==============================] - 6s 590us/sample - loss: 1.4425 - accuracy: 0.6577\n",
      "391/391 [==============================] - 68s 173ms/step - loss: 1.0306 - accuracy: 0.8011\n",
      "Epoch 81/300\n",
      "10000/10000 [==============================] - 6s 589us/sample - loss: 1.2190 - accuracy: 0.7340\n",
      "391/391 [==============================] - 67s 173ms/step - loss: 1.0280 - accuracy: 0.8001\n",
      "Epoch 82/300\n",
      "10000/10000 [==============================] - 6s 590us/sample - loss: 1.1145 - accuracy: 0.7750\n",
      "391/391 [==============================] - 68s 173ms/step - loss: 1.0209 - accuracy: 0.8018\n",
      "Epoch 83/300\n",
      "10000/10000 [==============================] - 6s 590us/sample - loss: 1.4004 - accuracy: 0.6888\n",
      "391/391 [==============================] - 68s 173ms/step - loss: 1.0296 - accuracy: 0.7981\n",
      "Epoch 84/300\n",
      "10000/10000 [==============================] - 6s 591us/sample - loss: 1.2029 - accuracy: 0.7499\n",
      "391/391 [==============================] - 68s 173ms/step - loss: 1.0265 - accuracy: 0.8014\n",
      "Epoch 85/300\n",
      "10000/10000 [==============================] - 6s 596us/sample - loss: 1.0724 - accuracy: 0.7878\n",
      "391/391 [==============================] - 68s 173ms/step - loss: 1.0246 - accuracy: 0.8006\n",
      "Epoch 86/300\n",
      "10000/10000 [==============================] - 6s 632us/sample - loss: 1.2209 - accuracy: 0.7302\n",
      "391/391 [==============================] - 68s 175ms/step - loss: 1.0310 - accuracy: 0.8012\n",
      "Epoch 87/300\n",
      "10000/10000 [==============================] - 6s 591us/sample - loss: 1.3713 - accuracy: 0.7077\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 1.0231 - accuracy: 0.8042\n",
      "Epoch 88/300\n",
      "10000/10000 [==============================] - 6s 593us/sample - loss: 1.1300 - accuracy: 0.7765\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 1.0251 - accuracy: 0.8049\n",
      "Epoch 89/300\n",
      "10000/10000 [==============================] - 6s 587us/sample - loss: 1.2079 - accuracy: 0.7557\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 1.0215 - accuracy: 0.8043\n",
      "Epoch 90/300\n",
      "10000/10000 [==============================] - 6s 591us/sample - loss: 1.2069 - accuracy: 0.7468\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 1.0275 - accuracy: 0.8036\n",
      "Epoch 91/300\n",
      "10000/10000 [==============================] - 6s 593us/sample - loss: 1.2656 - accuracy: 0.7274\n",
      "391/391 [==============================] - 68s 174ms/step - loss: 1.0187 - accuracy: 0.8059\n",
      "Epoch 92/300\n",
      "10000/10000 [==============================] - 6s 594us/sample - loss: 1.0405 - accuracy: 0.7963\n",
      "391/391 [==============================] - 68s 174ms/step - loss: 1.0187 - accuracy: 0.8044\n",
      "Epoch 93/300\n",
      "10000/10000 [==============================] - 6s 603us/sample - loss: 1.1443 - accuracy: 0.7709\n",
      "391/391 [==============================] - 68s 174ms/step - loss: 1.0168 - accuracy: 0.8057\n",
      "Epoch 94/300\n",
      "10000/10000 [==============================] - 6s 601us/sample - loss: 1.3959 - accuracy: 0.6930\n",
      "391/391 [==============================] - 68s 175ms/step - loss: 1.0202 - accuracy: 0.8055\n",
      "Epoch 95/300\n",
      "10000/10000 [==============================] - 6s 602us/sample - loss: 1.1868 - accuracy: 0.7545\n",
      "391/391 [==============================] - 69s 175ms/step - loss: 1.0225 - accuracy: 0.8071\n",
      "Epoch 96/300\n",
      "10000/10000 [==============================] - 6s 593us/sample - loss: 1.1871 - accuracy: 0.7500\n",
      "391/391 [==============================] - 69s 176ms/step - loss: 1.0154 - accuracy: 0.8054\n",
      "Epoch 97/300\n",
      "10000/10000 [==============================] - 6s 600us/sample - loss: 1.2173 - accuracy: 0.7477\n",
      "391/391 [==============================] - 68s 175ms/step - loss: 1.0174 - accuracy: 0.8054\n",
      "Epoch 98/300\n",
      "10000/10000 [==============================] - 6s 596us/sample - loss: 1.1759 - accuracy: 0.7616\n",
      "391/391 [==============================] - 69s 175ms/step - loss: 1.0185 - accuracy: 0.8071\n",
      "Epoch 99/300\n",
      "10000/10000 [==============================] - 6s 597us/sample - loss: 1.1751 - accuracy: 0.7558\n",
      "391/391 [==============================] - 68s 175ms/step - loss: 1.0176 - accuracy: 0.8050\n",
      "Epoch 100/300\n",
      "10000/10000 [==============================] - 6s 596us/sample - loss: 1.1963 - accuracy: 0.7580\n",
      "391/391 [==============================] - 68s 175ms/step - loss: 1.0143 - accuracy: 0.8070\n",
      "Epoch 101/300\n",
      "10000/10000 [==============================] - 6s 600us/sample - loss: 1.7539 - accuracy: 0.6245\n",
      "391/391 [==============================] - 68s 174ms/step - loss: 1.0126 - accuracy: 0.8077\n",
      "Epoch 102/300\n",
      "10000/10000 [==============================] - 6s 599us/sample - loss: 1.1591 - accuracy: 0.7585\n",
      "391/391 [==============================] - 68s 174ms/step - loss: 1.0144 - accuracy: 0.8075\n",
      "Epoch 103/300\n",
      "10000/10000 [==============================] - 6s 594us/sample - loss: 1.1304 - accuracy: 0.7704\n",
      "391/391 [==============================] - 68s 175ms/step - loss: 1.0102 - accuracy: 0.8076\n",
      "Epoch 104/300\n",
      "10000/10000 [==============================] - 6s 593us/sample - loss: 1.5561 - accuracy: 0.6637\n",
      "391/391 [==============================] - 68s 175ms/step - loss: 1.0164 - accuracy: 0.8086\n",
      "Epoch 105/300\n",
      "10000/10000 [==============================] - 6s 596us/sample - loss: 1.0800 - accuracy: 0.7859\n",
      "391/391 [==============================] - 69s 175ms/step - loss: 1.0186 - accuracy: 0.8065\n",
      "Epoch 106/300\n",
      "10000/10000 [==============================] - 6s 596us/sample - loss: 1.1715 - accuracy: 0.7542\n",
      "391/391 [==============================] - 68s 175ms/step - loss: 1.0146 - accuracy: 0.8076\n",
      "Epoch 107/300\n",
      "10000/10000 [==============================] - 6s 596us/sample - loss: 1.1937 - accuracy: 0.7490\n",
      "391/391 [==============================] - 69s 175ms/step - loss: 1.0160 - accuracy: 0.8082\n",
      "Epoch 108/300\n",
      "10000/10000 [==============================] - 6s 591us/sample - loss: 1.1366 - accuracy: 0.7691\n",
      "391/391 [==============================] - 68s 175ms/step - loss: 1.0139 - accuracy: 0.8086\n",
      "Epoch 109/300\n",
      "10000/10000 [==============================] - 6s 602us/sample - loss: 1.7132 - accuracy: 0.6275\n",
      "391/391 [==============================] - 68s 175ms/step - loss: 1.0136 - accuracy: 0.8087\n",
      "Epoch 110/300\n",
      "10000/10000 [==============================] - 6s 597us/sample - loss: 1.1587 - accuracy: 0.7597\n",
      "391/391 [==============================] - 68s 175ms/step - loss: 1.0150 - accuracy: 0.8084\n",
      "Epoch 111/300\n",
      "10000/10000 [==============================] - 6s 598us/sample - loss: 1.2754 - accuracy: 0.7254\n",
      "391/391 [==============================] - 68s 175ms/step - loss: 1.0113 - accuracy: 0.8103\n",
      "Epoch 112/300\n",
      "10000/10000 [==============================] - 6s 597us/sample - loss: 1.2579 - accuracy: 0.7294\n",
      "391/391 [==============================] - 68s 175ms/step - loss: 1.0120 - accuracy: 0.8098\n",
      "Epoch 113/300\n",
      "10000/10000 [==============================] - 6s 599us/sample - loss: 1.4049 - accuracy: 0.6875\n",
      "391/391 [==============================] - 68s 175ms/step - loss: 1.0154 - accuracy: 0.8090\n",
      "Epoch 114/300\n",
      "10000/10000 [==============================] - 6s 596us/sample - loss: 1.0992 - accuracy: 0.7806\n",
      "391/391 [==============================] - 69s 175ms/step - loss: 1.0175 - accuracy: 0.8107\n",
      "Epoch 115/300\n",
      "10000/10000 [==============================] - 6s 595us/sample - loss: 1.1003 - accuracy: 0.7868\n",
      "391/391 [==============================] - 68s 174ms/step - loss: 1.0174 - accuracy: 0.8074\n",
      "Epoch 116/300\n",
      "10000/10000 [==============================] - 6s 606us/sample - loss: 1.1171 - accuracy: 0.7840\n",
      "391/391 [==============================] - 68s 175ms/step - loss: 1.0189 - accuracy: 0.8088\n",
      "Epoch 117/300\n",
      "10000/10000 [==============================] - 6s 601us/sample - loss: 1.2066 - accuracy: 0.7500\n",
      "391/391 [==============================] - 68s 175ms/step - loss: 1.0158 - accuracy: 0.8082\n",
      "Epoch 118/300\n",
      "10000/10000 [==============================] - 6s 616us/sample - loss: 1.1527 - accuracy: 0.7603\n",
      "391/391 [==============================] - 68s 175ms/step - loss: 1.0168 - accuracy: 0.8086\n",
      "Epoch 119/300\n",
      "10000/10000 [==============================] - 6s 596us/sample - loss: 1.1722 - accuracy: 0.7599\n",
      "391/391 [==============================] - 68s 174ms/step - loss: 1.0153 - accuracy: 0.8091\n",
      "Epoch 120/300\n",
      "10000/10000 [==============================] - 6s 597us/sample - loss: 1.1564 - accuracy: 0.7627\n",
      "391/391 [==============================] - 68s 174ms/step - loss: 1.0151 - accuracy: 0.8108\n",
      "Epoch 121/300\n",
      "10000/10000 [==============================] - 6s 596us/sample - loss: 1.3045 - accuracy: 0.7250\n",
      "391/391 [==============================] - 68s 175ms/step - loss: 1.0145 - accuracy: 0.8089\n",
      "Epoch 122/300\n",
      "10000/10000 [==============================] - 6s 595us/sample - loss: 1.2201 - accuracy: 0.7414\n",
      "391/391 [==============================] - 68s 174ms/step - loss: 1.0179 - accuracy: 0.8101\n",
      "Epoch 123/300\n",
      "10000/10000 [==============================] - 6s 595us/sample - loss: 1.1671 - accuracy: 0.7657\n",
      "391/391 [==============================] - 68s 175ms/step - loss: 1.0143 - accuracy: 0.8101\n",
      "Epoch 124/300\n",
      "10000/10000 [==============================] - 6s 594us/sample - loss: 1.1827 - accuracy: 0.7590\n",
      "391/391 [==============================] - 68s 174ms/step - loss: 1.0138 - accuracy: 0.8105\n",
      "Epoch 125/300\n",
      "10000/10000 [==============================] - 6s 597us/sample - loss: 1.1922 - accuracy: 0.7540\n",
      "391/391 [==============================] - 68s 173ms/step - loss: 1.0049 - accuracy: 0.8123\n",
      "Epoch 126/300\n",
      "10000/10000 [==============================] - 6s 592us/sample - loss: 1.2041 - accuracy: 0.7538\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 1.0267 - accuracy: 0.8082\n",
      "Epoch 127/300\n",
      "10000/10000 [==============================] - 6s 609us/sample - loss: 1.1749 - accuracy: 0.7632\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 1.0271 - accuracy: 0.8086\n",
      "Epoch 128/300\n",
      "10000/10000 [==============================] - 6s 592us/sample - loss: 1.2510 - accuracy: 0.7379\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 1.0105 - accuracy: 0.8110\n",
      "Epoch 129/300\n",
      "10000/10000 [==============================] - 6s 591us/sample - loss: 1.3106 - accuracy: 0.7244\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 1.0194 - accuracy: 0.8109\n",
      "Epoch 130/300\n",
      "10000/10000 [==============================] - 6s 593us/sample - loss: 1.1148 - accuracy: 0.7806\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 1.0136 - accuracy: 0.8111\n",
      "Epoch 131/300\n",
      "10000/10000 [==============================] - 6s 591us/sample - loss: 1.0915 - accuracy: 0.7900\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 1.0087 - accuracy: 0.8133\n",
      "Epoch 132/300\n",
      "10000/10000 [==============================] - 6s 595us/sample - loss: 1.1818 - accuracy: 0.7534\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 1.0087 - accuracy: 0.8142\n",
      "Epoch 133/300\n",
      "10000/10000 [==============================] - 6s 595us/sample - loss: 1.1976 - accuracy: 0.7533\n",
      "391/391 [==============================] - 67s 171ms/step - loss: 1.0107 - accuracy: 0.8110\n",
      "Epoch 134/300\n",
      "10000/10000 [==============================] - 6s 593us/sample - loss: 1.0986 - accuracy: 0.7804\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 1.0052 - accuracy: 0.8132\n",
      "Epoch 135/300\n",
      "10000/10000 [==============================] - 6s 590us/sample - loss: 1.1881 - accuracy: 0.7607\n",
      "391/391 [==============================] - 67s 171ms/step - loss: 1.0020 - accuracy: 0.8119\n",
      "Epoch 136/300\n",
      "10000/10000 [==============================] - 6s 589us/sample - loss: 1.1387 - accuracy: 0.7628\n",
      "391/391 [==============================] - 67s 171ms/step - loss: 1.0052 - accuracy: 0.8097\n",
      "Epoch 137/300\n",
      "10000/10000 [==============================] - 6s 592us/sample - loss: 1.3541 - accuracy: 0.7446\n",
      "391/391 [==============================] - 67s 171ms/step - loss: 1.0108 - accuracy: 0.8116\n",
      "Epoch 138/300\n",
      "10000/10000 [==============================] - 6s 588us/sample - loss: 1.4103 - accuracy: 0.7066\n",
      "391/391 [==============================] - 67s 170ms/step - loss: 1.0131 - accuracy: 0.8098\n",
      "Epoch 139/300\n",
      "10000/10000 [==============================] - 6s 593us/sample - loss: 1.1188 - accuracy: 0.7890\n",
      "391/391 [==============================] - 67s 171ms/step - loss: 1.0108 - accuracy: 0.8112\n",
      "Epoch 140/300\n",
      "10000/10000 [==============================] - 6s 587us/sample - loss: 1.1353 - accuracy: 0.7736\n",
      "391/391 [==============================] - 67s 171ms/step - loss: 1.0050 - accuracy: 0.8148\n",
      "Epoch 141/300\n",
      "10000/10000 [==============================] - 6s 633us/sample - loss: 1.1251 - accuracy: 0.7754\n",
      "391/391 [==============================] - 67s 171ms/step - loss: 0.9988 - accuracy: 0.8151\n",
      "Epoch 142/300\n",
      "10000/10000 [==============================] - 6s 589us/sample - loss: 1.1098 - accuracy: 0.7817\n",
      "391/391 [==============================] - 67s 170ms/step - loss: 1.0074 - accuracy: 0.8124\n",
      "Epoch 143/300\n",
      "10000/10000 [==============================] - 6s 601us/sample - loss: 1.2888 - accuracy: 0.7217\n",
      "391/391 [==============================] - 67s 170ms/step - loss: 1.0052 - accuracy: 0.8138\n",
      "Epoch 144/300\n",
      "10000/10000 [==============================] - 6s 589us/sample - loss: 1.1039 - accuracy: 0.7899\n",
      "391/391 [==============================] - 67s 170ms/step - loss: 1.0135 - accuracy: 0.8125\n",
      "Epoch 145/300\n",
      "10000/10000 [==============================] - 6s 590us/sample - loss: 1.4178 - accuracy: 0.6897\n",
      "391/391 [==============================] - 67s 170ms/step - loss: 1.0147 - accuracy: 0.8111\n",
      "Epoch 146/300\n",
      "10000/10000 [==============================] - 6s 590us/sample - loss: 1.0742 - accuracy: 0.7938\n",
      "391/391 [==============================] - 67s 170ms/step - loss: 1.0146 - accuracy: 0.8122\n",
      "Epoch 147/300\n",
      "10000/10000 [==============================] - 6s 587us/sample - loss: 1.2928 - accuracy: 0.7223\n",
      "391/391 [==============================] - 66s 169ms/step - loss: 1.0052 - accuracy: 0.8164\n",
      "Epoch 148/300\n",
      "10000/10000 [==============================] - 6s 589us/sample - loss: 1.4023 - accuracy: 0.7080\n",
      "391/391 [==============================] - 66s 169ms/step - loss: 1.0048 - accuracy: 0.8139\n",
      "Epoch 149/300\n",
      "10000/10000 [==============================] - 6s 599us/sample - loss: 1.1378 - accuracy: 0.7637\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 1.0030 - accuracy: 0.8128\n",
      "Epoch 150/300\n",
      "10000/10000 [==============================] - 6s 593us/sample - loss: 1.3374 - accuracy: 0.7134\n",
      "391/391 [==============================] - 68s 174ms/step - loss: 1.0013 - accuracy: 0.8134\n",
      "Epoch 151/300\n",
      "10000/10000 [==============================] - 6s 588us/sample - loss: 0.8269 - accuracy: 0.8693\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 0.8819 - accuracy: 0.8544\n",
      "Epoch 152/300\n",
      "10000/10000 [==============================] - 6s 589us/sample - loss: 0.7538 - accuracy: 0.8821\n",
      "391/391 [==============================] - 67s 171ms/step - loss: 0.7590 - accuracy: 0.8854\n",
      "Epoch 153/300\n",
      "10000/10000 [==============================] - 6s 590us/sample - loss: 0.7181 - accuracy: 0.8852\n",
      "391/391 [==============================] - 67s 171ms/step - loss: 0.6996 - accuracy: 0.8950\n",
      "Epoch 154/300\n",
      "10000/10000 [==============================] - 6s 590us/sample - loss: 0.7057 - accuracy: 0.8853\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 0.6595 - accuracy: 0.9000\n",
      "Epoch 155/300\n",
      "10000/10000 [==============================] - 6s 620us/sample - loss: 0.6867 - accuracy: 0.8799\n",
      "391/391 [==============================] - 68s 173ms/step - loss: 0.6263 - accuracy: 0.9037\n",
      "Epoch 156/300\n",
      "10000/10000 [==============================] - 6s 590us/sample - loss: 0.6738 - accuracy: 0.8833\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 0.5897 - accuracy: 0.9071\n",
      "Epoch 157/300\n",
      "10000/10000 [==============================] - 6s 595us/sample - loss: 0.6115 - accuracy: 0.8969\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 0.5623 - accuracy: 0.9115\n",
      "Epoch 158/300\n",
      "10000/10000 [==============================] - 6s 588us/sample - loss: 0.6195 - accuracy: 0.8916\n",
      "391/391 [==============================] - 67s 171ms/step - loss: 0.5404 - accuracy: 0.9132\n",
      "Epoch 159/300\n",
      "10000/10000 [==============================] - 6s 592us/sample - loss: 0.6227 - accuracy: 0.8835\n",
      "391/391 [==============================] - 67s 171ms/step - loss: 0.5202 - accuracy: 0.9147\n",
      "Epoch 160/300\n",
      "10000/10000 [==============================] - 6s 589us/sample - loss: 0.6176 - accuracy: 0.8843\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 0.5041 - accuracy: 0.9154\n",
      "Epoch 161/300\n",
      "10000/10000 [==============================] - 6s 591us/sample - loss: 0.5956 - accuracy: 0.8850\n",
      "391/391 [==============================] - 67s 170ms/step - loss: 0.4885 - accuracy: 0.9187\n",
      "Epoch 162/300\n",
      "10000/10000 [==============================] - 6s 591us/sample - loss: 0.6040 - accuracy: 0.8860\n",
      "391/391 [==============================] - 67s 171ms/step - loss: 0.4737 - accuracy: 0.9186\n",
      "Epoch 163/300\n",
      "10000/10000 [==============================] - 6s 588us/sample - loss: 0.5857 - accuracy: 0.8870\n",
      "391/391 [==============================] - 67s 171ms/step - loss: 0.4646 - accuracy: 0.9194\n",
      "Epoch 164/300\n",
      "10000/10000 [==============================] - 6s 590us/sample - loss: 0.5860 - accuracy: 0.8830\n",
      "391/391 [==============================] - 67s 171ms/step - loss: 0.4572 - accuracy: 0.9191\n",
      "Epoch 165/300\n",
      "10000/10000 [==============================] - 6s 596us/sample - loss: 0.6070 - accuracy: 0.8798\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 0.4450 - accuracy: 0.9199\n",
      "Epoch 166/300\n",
      "10000/10000 [==============================] - 6s 590us/sample - loss: 0.5751 - accuracy: 0.8847\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 0.4422 - accuracy: 0.9220\n",
      "Epoch 167/300\n",
      "10000/10000 [==============================] - 6s 592us/sample - loss: 0.5690 - accuracy: 0.8841\n",
      "391/391 [==============================] - 67s 171ms/step - loss: 0.4330 - accuracy: 0.9215\n",
      "Epoch 168/300\n",
      "10000/10000 [==============================] - 6s 590us/sample - loss: 0.6175 - accuracy: 0.8704\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 0.4380 - accuracy: 0.9207\n",
      "Epoch 169/300\n",
      "10000/10000 [==============================] - 6s 594us/sample - loss: 0.5711 - accuracy: 0.8822\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 0.4344 - accuracy: 0.9209\n",
      "Epoch 170/300\n",
      "10000/10000 [==============================] - 6s 590us/sample - loss: 0.6098 - accuracy: 0.8772\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 0.4313 - accuracy: 0.9212\n",
      "Epoch 171/300\n",
      "10000/10000 [==============================] - 6s 590us/sample - loss: 0.5789 - accuracy: 0.8811\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 0.4267 - accuracy: 0.9221\n",
      "Epoch 172/300\n",
      "10000/10000 [==============================] - 6s 589us/sample - loss: 0.5798 - accuracy: 0.8786\n",
      "391/391 [==============================] - 67s 171ms/step - loss: 0.4252 - accuracy: 0.9224\n",
      "Epoch 173/300\n",
      "10000/10000 [==============================] - 6s 603us/sample - loss: 0.5832 - accuracy: 0.8814\n",
      "391/391 [==============================] - 68s 174ms/step - loss: 0.4198 - accuracy: 0.9229\n",
      "Epoch 174/300\n",
      "10000/10000 [==============================] - 6s 591us/sample - loss: 0.6080 - accuracy: 0.8753\n",
      "391/391 [==============================] - 68s 174ms/step - loss: 0.4253 - accuracy: 0.9224\n",
      "Epoch 175/300\n",
      "10000/10000 [==============================] - 6s 593us/sample - loss: 0.5739 - accuracy: 0.8819\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 0.4204 - accuracy: 0.9236\n",
      "Epoch 176/300\n",
      "10000/10000 [==============================] - 6s 589us/sample - loss: 0.6315 - accuracy: 0.8671\n",
      "391/391 [==============================] - 67s 171ms/step - loss: 0.4236 - accuracy: 0.9230\n",
      "Epoch 177/300\n",
      "10000/10000 [==============================] - 6s 593us/sample - loss: 0.6598 - accuracy: 0.8584\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 0.4153 - accuracy: 0.9252\n",
      "Epoch 178/300\n",
      "10000/10000 [==============================] - 6s 601us/sample - loss: 0.5682 - accuracy: 0.8840\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 0.4134 - accuracy: 0.9247\n",
      "Epoch 179/300\n",
      "10000/10000 [==============================] - 6s 595us/sample - loss: 0.6051 - accuracy: 0.8739\n",
      "391/391 [==============================] - 68s 173ms/step - loss: 0.4174 - accuracy: 0.9236\n",
      "Epoch 180/300\n",
      "10000/10000 [==============================] - 6s 587us/sample - loss: 0.6584 - accuracy: 0.8616\n",
      "391/391 [==============================] - 67s 173ms/step - loss: 0.4187 - accuracy: 0.9246\n",
      "Epoch 181/300\n",
      "10000/10000 [==============================] - 6s 594us/sample - loss: 0.6081 - accuracy: 0.8716\n",
      "391/391 [==============================] - 68s 173ms/step - loss: 0.4184 - accuracy: 0.9247\n",
      "Epoch 182/300\n",
      "10000/10000 [==============================] - 6s 596us/sample - loss: 0.5697 - accuracy: 0.8835\n",
      "391/391 [==============================] - 68s 174ms/step - loss: 0.4146 - accuracy: 0.9266\n",
      "Epoch 183/300\n",
      "10000/10000 [==============================] - 6s 594us/sample - loss: 0.6236 - accuracy: 0.8697\n",
      "391/391 [==============================] - 68s 174ms/step - loss: 0.4165 - accuracy: 0.9260\n",
      "Epoch 184/300\n",
      "10000/10000 [==============================] - 6s 596us/sample - loss: 0.5960 - accuracy: 0.8805\n",
      "391/391 [==============================] - 68s 173ms/step - loss: 0.4174 - accuracy: 0.9255\n",
      "Epoch 185/300\n",
      "10000/10000 [==============================] - 6s 594us/sample - loss: 0.5524 - accuracy: 0.8894\n",
      "391/391 [==============================] - 68s 173ms/step - loss: 0.4184 - accuracy: 0.9255\n",
      "Epoch 186/300\n",
      "10000/10000 [==============================] - 6s 594us/sample - loss: 0.6091 - accuracy: 0.8753\n",
      "391/391 [==============================] - 68s 173ms/step - loss: 0.4149 - accuracy: 0.9278\n",
      "Epoch 187/300\n",
      "10000/10000 [==============================] - 6s 593us/sample - loss: 0.5807 - accuracy: 0.8848\n",
      "391/391 [==============================] - 68s 173ms/step - loss: 0.4124 - accuracy: 0.9278\n",
      "Epoch 188/300\n",
      "10000/10000 [==============================] - 6s 591us/sample - loss: 0.6174 - accuracy: 0.8757\n",
      "391/391 [==============================] - 68s 174ms/step - loss: 0.4144 - accuracy: 0.9276\n",
      "Epoch 189/300\n",
      "10000/10000 [==============================] - 6s 598us/sample - loss: 0.6449 - accuracy: 0.8676\n",
      "391/391 [==============================] - 68s 173ms/step - loss: 0.4143 - accuracy: 0.9276\n",
      "Epoch 190/300\n",
      "10000/10000 [==============================] - 6s 593us/sample - loss: 0.6215 - accuracy: 0.8753\n",
      "391/391 [==============================] - 68s 173ms/step - loss: 0.4150 - accuracy: 0.9295\n",
      "Epoch 191/300\n",
      "10000/10000 [==============================] - 6s 597us/sample - loss: 0.6134 - accuracy: 0.8741\n",
      "391/391 [==============================] - 68s 173ms/step - loss: 0.4160 - accuracy: 0.9288\n",
      "Epoch 192/300\n",
      "10000/10000 [==============================] - 6s 593us/sample - loss: 0.6612 - accuracy: 0.8634\n",
      "391/391 [==============================] - 68s 174ms/step - loss: 0.4161 - accuracy: 0.9286\n",
      "Epoch 193/300\n",
      "10000/10000 [==============================] - 6s 597us/sample - loss: 0.6162 - accuracy: 0.8744\n",
      "391/391 [==============================] - 68s 173ms/step - loss: 0.4138 - accuracy: 0.9292\n",
      "Epoch 194/300\n",
      "10000/10000 [==============================] - 6s 592us/sample - loss: 0.6225 - accuracy: 0.8737\n",
      "391/391 [==============================] - 68s 173ms/step - loss: 0.4116 - accuracy: 0.9310\n",
      "Epoch 195/300\n",
      "10000/10000 [==============================] - 6s 595us/sample - loss: 0.6520 - accuracy: 0.8686\n",
      "391/391 [==============================] - 68s 173ms/step - loss: 0.4116 - accuracy: 0.9308\n",
      "Epoch 196/300\n",
      "10000/10000 [==============================] - 6s 592us/sample - loss: 0.6492 - accuracy: 0.8675\n",
      "391/391 [==============================] - 68s 173ms/step - loss: 0.4168 - accuracy: 0.9297\n",
      "Epoch 197/300\n",
      "10000/10000 [==============================] - 6s 598us/sample - loss: 0.6161 - accuracy: 0.8786\n",
      "391/391 [==============================] - 68s 174ms/step - loss: 0.4194 - accuracy: 0.9294\n",
      "Epoch 198/300\n",
      "10000/10000 [==============================] - 6s 595us/sample - loss: 0.6111 - accuracy: 0.8807\n",
      "391/391 [==============================] - 68s 174ms/step - loss: 0.4113 - accuracy: 0.9319\n",
      "Epoch 199/300\n",
      "10000/10000 [==============================] - 6s 594us/sample - loss: 0.6400 - accuracy: 0.8702\n",
      "391/391 [==============================] - 68s 173ms/step - loss: 0.4107 - accuracy: 0.9326\n",
      "Epoch 200/300\n",
      "10000/10000 [==============================] - 6s 599us/sample - loss: 0.6229 - accuracy: 0.8774\n",
      "391/391 [==============================] - 68s 174ms/step - loss: 0.4161 - accuracy: 0.9311\n",
      "Epoch 201/300\n",
      "10000/10000 [==============================] - 6s 633us/sample - loss: 0.6194 - accuracy: 0.8785\n",
      "391/391 [==============================] - 68s 175ms/step - loss: 0.4170 - accuracy: 0.9305\n",
      "Epoch 202/300\n",
      "10000/10000 [==============================] - 6s 593us/sample - loss: 0.5869 - accuracy: 0.8845\n",
      "391/391 [==============================] - 68s 174ms/step - loss: 0.4161 - accuracy: 0.9320\n",
      "Epoch 203/300\n",
      "10000/10000 [==============================] - 6s 594us/sample - loss: 0.5908 - accuracy: 0.8855\n",
      "391/391 [==============================] - 68s 174ms/step - loss: 0.4128 - accuracy: 0.9318\n",
      "Epoch 204/300\n",
      "10000/10000 [==============================] - 6s 594us/sample - loss: 0.6065 - accuracy: 0.8786\n",
      "391/391 [==============================] - 68s 174ms/step - loss: 0.4157 - accuracy: 0.9319\n",
      "Epoch 205/300\n",
      "10000/10000 [==============================] - 6s 598us/sample - loss: 0.6305 - accuracy: 0.8758\n",
      "391/391 [==============================] - 68s 173ms/step - loss: 0.4127 - accuracy: 0.9339\n",
      "Epoch 206/300\n",
      "10000/10000 [==============================] - 6s 596us/sample - loss: 0.6072 - accuracy: 0.8857\n",
      "391/391 [==============================] - 68s 174ms/step - loss: 0.4149 - accuracy: 0.9322\n",
      "Epoch 207/300\n",
      "10000/10000 [==============================] - 6s 596us/sample - loss: 0.6699 - accuracy: 0.8615\n",
      "391/391 [==============================] - 68s 174ms/step - loss: 0.4215 - accuracy: 0.9308\n",
      "Epoch 208/300\n",
      "10000/10000 [==============================] - 6s 592us/sample - loss: 0.5958 - accuracy: 0.8829\n",
      "391/391 [==============================] - 68s 173ms/step - loss: 0.4090 - accuracy: 0.9346\n",
      "Epoch 209/300\n",
      "10000/10000 [==============================] - 6s 595us/sample - loss: 0.6494 - accuracy: 0.8772\n",
      "391/391 [==============================] - 68s 173ms/step - loss: 0.4091 - accuracy: 0.9353\n",
      "Epoch 210/300\n",
      "10000/10000 [==============================] - 6s 591us/sample - loss: 0.6753 - accuracy: 0.8691\n",
      "391/391 [==============================] - 67s 173ms/step - loss: 0.4139 - accuracy: 0.9344\n",
      "Epoch 211/300\n",
      "10000/10000 [==============================] - 6s 593us/sample - loss: 0.5938 - accuracy: 0.8858\n",
      "391/391 [==============================] - 67s 173ms/step - loss: 0.4132 - accuracy: 0.9341\n",
      "Epoch 212/300\n",
      "10000/10000 [==============================] - 6s 593us/sample - loss: 0.6305 - accuracy: 0.8778\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 0.4115 - accuracy: 0.9343\n",
      "Epoch 213/300\n",
      "10000/10000 [==============================] - 6s 598us/sample - loss: 0.6164 - accuracy: 0.8824\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 0.4101 - accuracy: 0.9359\n",
      "Epoch 214/300\n",
      "10000/10000 [==============================] - 6s 592us/sample - loss: 0.6886 - accuracy: 0.8617\n",
      "391/391 [==============================] - 67s 171ms/step - loss: 0.4160 - accuracy: 0.9330\n",
      "Epoch 215/300\n",
      "10000/10000 [==============================] - 6s 592us/sample - loss: 0.6071 - accuracy: 0.8871\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 0.4159 - accuracy: 0.9337\n",
      "Epoch 216/300\n",
      "10000/10000 [==============================] - 6s 592us/sample - loss: 0.6122 - accuracy: 0.8811\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 0.4166 - accuracy: 0.9332\n",
      "Epoch 217/300\n",
      "10000/10000 [==============================] - 6s 592us/sample - loss: 0.7111 - accuracy: 0.8595\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 0.4086 - accuracy: 0.9369\n",
      "Epoch 218/300\n",
      "10000/10000 [==============================] - 6s 590us/sample - loss: 0.6295 - accuracy: 0.8765\n",
      "391/391 [==============================] - 67s 171ms/step - loss: 0.4135 - accuracy: 0.9354\n",
      "Epoch 219/300\n",
      "10000/10000 [==============================] - 6s 590us/sample - loss: 0.6154 - accuracy: 0.8854\n",
      "391/391 [==============================] - 67s 171ms/step - loss: 0.4124 - accuracy: 0.9365\n",
      "Epoch 220/300\n",
      "10000/10000 [==============================] - 6s 609us/sample - loss: 0.6170 - accuracy: 0.8811\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 0.4131 - accuracy: 0.9369\n",
      "Epoch 221/300\n",
      "10000/10000 [==============================] - 6s 598us/sample - loss: 0.6232 - accuracy: 0.8846\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 0.4135 - accuracy: 0.9359\n",
      "Epoch 222/300\n",
      "10000/10000 [==============================] - 6s 589us/sample - loss: 0.6813 - accuracy: 0.8694\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 0.4128 - accuracy: 0.9364\n",
      "Epoch 223/300\n",
      "10000/10000 [==============================] - 6s 595us/sample - loss: 0.6114 - accuracy: 0.8820\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 0.4097 - accuracy: 0.9383\n",
      "Epoch 224/300\n",
      "10000/10000 [==============================] - 6s 608us/sample - loss: 0.6174 - accuracy: 0.8810\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 0.4084 - accuracy: 0.9391\n",
      "Epoch 225/300\n",
      "10000/10000 [==============================] - 6s 594us/sample - loss: 0.6841 - accuracy: 0.8689\n",
      "391/391 [==============================] - 67s 171ms/step - loss: 0.4121 - accuracy: 0.9371\n",
      "Epoch 226/300\n",
      "10000/10000 [==============================] - 6s 592us/sample - loss: 0.5152 - accuracy: 0.9123\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 0.3741 - accuracy: 0.9519\n",
      "Epoch 227/300\n",
      "10000/10000 [==============================] - 6s 590us/sample - loss: 0.5024 - accuracy: 0.9139\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 0.3468 - accuracy: 0.9604\n",
      "Epoch 228/300\n",
      "10000/10000 [==============================] - 6s 590us/sample - loss: 0.5106 - accuracy: 0.9116\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 0.3347 - accuracy: 0.9643\n",
      "Epoch 229/300\n",
      "10000/10000 [==============================] - 6s 597us/sample - loss: 0.5007 - accuracy: 0.9143\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 0.3247 - accuracy: 0.9680\n",
      "Epoch 230/300\n",
      "10000/10000 [==============================] - 6s 591us/sample - loss: 0.5042 - accuracy: 0.9149\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 0.3171 - accuracy: 0.9703\n",
      "Epoch 231/300\n",
      "10000/10000 [==============================] - 6s 591us/sample - loss: 0.5007 - accuracy: 0.9162\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 0.3104 - accuracy: 0.9718\n",
      "Epoch 232/300\n",
      "10000/10000 [==============================] - 6s 591us/sample - loss: 0.5083 - accuracy: 0.9152\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 0.3077 - accuracy: 0.9722\n",
      "Epoch 233/300\n",
      "10000/10000 [==============================] - 6s 592us/sample - loss: 0.5157 - accuracy: 0.9128\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 0.3040 - accuracy: 0.9722\n",
      "Epoch 234/300\n",
      "10000/10000 [==============================] - 6s 589us/sample - loss: 0.4936 - accuracy: 0.9178\n",
      "391/391 [==============================] - 68s 173ms/step - loss: 0.2962 - accuracy: 0.9751\n",
      "Epoch 235/300\n",
      "10000/10000 [==============================] - 6s 593us/sample - loss: 0.5026 - accuracy: 0.9154\n",
      "391/391 [==============================] - 68s 173ms/step - loss: 0.2928 - accuracy: 0.9754\n",
      "Epoch 236/300\n",
      "10000/10000 [==============================] - 6s 587us/sample - loss: 0.5001 - accuracy: 0.9162\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 0.2887 - accuracy: 0.9764\n",
      "Epoch 237/300\n",
      "10000/10000 [==============================] - 6s 598us/sample - loss: 0.5036 - accuracy: 0.9165\n",
      "391/391 [==============================] - 67s 173ms/step - loss: 0.2856 - accuracy: 0.9769\n",
      "Epoch 238/300\n",
      "10000/10000 [==============================] - 6s 606us/sample - loss: 0.4919 - accuracy: 0.9185\n",
      "391/391 [==============================] - 68s 173ms/step - loss: 0.2816 - accuracy: 0.9775\n",
      "Epoch 239/300\n",
      "10000/10000 [==============================] - 6s 591us/sample - loss: 0.4974 - accuracy: 0.9149\n",
      "391/391 [==============================] - 68s 173ms/step - loss: 0.2766 - accuracy: 0.9779\n",
      "Epoch 240/300\n",
      "10000/10000 [==============================] - 6s 593us/sample - loss: 0.5020 - accuracy: 0.9163\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 0.2760 - accuracy: 0.9784\n",
      "Epoch 241/300\n",
      "10000/10000 [==============================] - 6s 592us/sample - loss: 0.4979 - accuracy: 0.9154\n",
      "391/391 [==============================] - 67s 173ms/step - loss: 0.2706 - accuracy: 0.9796\n",
      "Epoch 242/300\n",
      "10000/10000 [==============================] - 6s 592us/sample - loss: 0.4897 - accuracy: 0.9173\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 0.2693 - accuracy: 0.9793\n",
      "Epoch 243/300\n",
      "10000/10000 [==============================] - 6s 593us/sample - loss: 0.4884 - accuracy: 0.9189\n",
      "391/391 [==============================] - 68s 173ms/step - loss: 0.2645 - accuracy: 0.9820\n",
      "Epoch 244/300\n",
      "10000/10000 [==============================] - 6s 592us/sample - loss: 0.4837 - accuracy: 0.9206\n",
      "391/391 [==============================] - 68s 173ms/step - loss: 0.2634 - accuracy: 0.9808\n",
      "Epoch 245/300\n",
      "10000/10000 [==============================] - 6s 595us/sample - loss: 0.4779 - accuracy: 0.9197\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 0.2598 - accuracy: 0.9814\n",
      "Epoch 246/300\n",
      "10000/10000 [==============================] - 6s 590us/sample - loss: 0.4885 - accuracy: 0.9159\n",
      "391/391 [==============================] - 67s 173ms/step - loss: 0.2566 - accuracy: 0.9824\n",
      "Epoch 247/300\n",
      "10000/10000 [==============================] - 6s 591us/sample - loss: 0.5028 - accuracy: 0.9149\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 0.2552 - accuracy: 0.9818\n",
      "Epoch 248/300\n",
      "10000/10000 [==============================] - 6s 591us/sample - loss: 0.4960 - accuracy: 0.9181\n",
      "391/391 [==============================] - 68s 173ms/step - loss: 0.2513 - accuracy: 0.9824\n",
      "Epoch 249/300\n",
      "10000/10000 [==============================] - 6s 590us/sample - loss: 0.4885 - accuracy: 0.9145\n",
      "391/391 [==============================] - 67s 173ms/step - loss: 0.2495 - accuracy: 0.9836\n",
      "Epoch 250/300\n",
      "10000/10000 [==============================] - 6s 586us/sample - loss: 0.4962 - accuracy: 0.9166\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 0.2468 - accuracy: 0.9833\n",
      "Epoch 251/300\n",
      "10000/10000 [==============================] - 6s 589us/sample - loss: 0.4930 - accuracy: 0.9170\n",
      "391/391 [==============================] - 67s 172ms/step - loss: 0.2457 - accuracy: 0.9838\n",
      "Epoch 252/300\n",
      "10000/10000 [==============================] - 6s 590us/sample - loss: 0.5030 - accuracy: 0.9121\n",
      "391/391 [==============================] - 68s 174ms/step - loss: 0.2432 - accuracy: 0.9839\n",
      "Epoch 253/300\n",
      "10000/10000 [==============================] - 6s 596us/sample - loss: 0.4915 - accuracy: 0.9194\n",
      "391/391 [==============================] - 68s 173ms/step - loss: 0.2395 - accuracy: 0.9843\n",
      "Epoch 254/300\n",
      "10000/10000 [==============================] - 6s 593us/sample - loss: 0.4885 - accuracy: 0.9187\n",
      "391/391 [==============================] - 68s 174ms/step - loss: 0.2366 - accuracy: 0.9850\n",
      "Epoch 255/300\n",
      "10000/10000 [==============================] - 6s 596us/sample - loss: 0.4847 - accuracy: 0.9198\n",
      "391/391 [==============================] - 69s 176ms/step - loss: 0.2368 - accuracy: 0.9842\n",
      "Epoch 256/300\n",
      "10000/10000 [==============================] - 6s 589us/sample - loss: 0.4740 - accuracy: 0.9205\n",
      "391/391 [==============================] - 68s 173ms/step - loss: 0.2338 - accuracy: 0.9854\n",
      "Epoch 257/300\n",
      "10000/10000 [==============================] - 6s 593us/sample - loss: 0.4925 - accuracy: 0.9186\n",
      "391/391 [==============================] - 68s 175ms/step - loss: 0.2321 - accuracy: 0.9852\n",
      "Epoch 258/300\n",
      "10000/10000 [==============================] - 6s 611us/sample - loss: 0.4992 - accuracy: 0.9141\n",
      "391/391 [==============================] - 69s 176ms/step - loss: 0.2313 - accuracy: 0.9848\n",
      "Epoch 259/300\n",
      "10000/10000 [==============================] - 7s 650us/sample - loss: 0.4955 - accuracy: 0.9153\n",
      "391/391 [==============================] - 74s 189ms/step - loss: 0.2278 - accuracy: 0.9855\n",
      "Epoch 260/300\n",
      "10000/10000 [==============================] - 7s 651us/sample - loss: 0.4752 - accuracy: 0.9202\n",
      "391/391 [==============================] - 73s 188ms/step - loss: 0.2273 - accuracy: 0.9853\n",
      "Epoch 261/300\n",
      "10000/10000 [==============================] - 7s 661us/sample - loss: 0.4781 - accuracy: 0.9177\n",
      "391/391 [==============================] - 74s 189ms/step - loss: 0.2251 - accuracy: 0.9860\n",
      "Epoch 262/300\n",
      "10000/10000 [==============================] - 7s 654us/sample - loss: 0.4845 - accuracy: 0.9171\n",
      "391/391 [==============================] - 74s 189ms/step - loss: 0.2207 - accuracy: 0.9867\n",
      "Epoch 263/300\n",
      "10000/10000 [==============================] - 7s 651us/sample - loss: 0.4854 - accuracy: 0.9156\n",
      "391/391 [==============================] - 74s 189ms/step - loss: 0.2210 - accuracy: 0.9856\n",
      "Epoch 264/300\n",
      "10000/10000 [==============================] - 7s 656us/sample - loss: 0.4758 - accuracy: 0.9199\n",
      "391/391 [==============================] - 74s 189ms/step - loss: 0.2185 - accuracy: 0.9867\n",
      "Epoch 265/300\n",
      "10000/10000 [==============================] - 7s 657us/sample - loss: 0.4829 - accuracy: 0.9178\n",
      "391/391 [==============================] - 74s 190ms/step - loss: 0.2184 - accuracy: 0.9860\n",
      "Epoch 266/300\n",
      "10000/10000 [==============================] - 7s 653us/sample - loss: 0.4783 - accuracy: 0.9181\n",
      "391/391 [==============================] - 74s 189ms/step - loss: 0.2172 - accuracy: 0.9867\n",
      "Epoch 267/300\n",
      "10000/10000 [==============================] - 7s 654us/sample - loss: 0.5019 - accuracy: 0.9107\n",
      "391/391 [==============================] - 74s 190ms/step - loss: 0.2137 - accuracy: 0.9872\n",
      "Epoch 268/300\n",
      "10000/10000 [==============================] - 7s 651us/sample - loss: 0.4784 - accuracy: 0.9167\n",
      "391/391 [==============================] - 74s 190ms/step - loss: 0.2119 - accuracy: 0.9872\n",
      "Epoch 269/300\n",
      "10000/10000 [==============================] - 7s 690us/sample - loss: 0.4864 - accuracy: 0.9178\n",
      "391/391 [==============================] - 74s 190ms/step - loss: 0.2103 - accuracy: 0.9874\n",
      "Epoch 270/300\n",
      "10000/10000 [==============================] - 7s 654us/sample - loss: 0.4938 - accuracy: 0.9166\n",
      "391/391 [==============================] - 74s 189ms/step - loss: 0.2077 - accuracy: 0.9879\n",
      "Epoch 271/300\n",
      "10000/10000 [==============================] - 7s 657us/sample - loss: 0.4804 - accuracy: 0.9182\n",
      "391/391 [==============================] - 74s 189ms/step - loss: 0.2086 - accuracy: 0.9877\n",
      "Epoch 272/300\n",
      "10000/10000 [==============================] - 7s 653us/sample - loss: 0.4817 - accuracy: 0.9175\n",
      "391/391 [==============================] - 74s 189ms/step - loss: 0.2061 - accuracy: 0.9880\n",
      "Epoch 273/300\n",
      "10000/10000 [==============================] - 7s 656us/sample - loss: 0.4917 - accuracy: 0.9172\n",
      "391/391 [==============================] - 74s 190ms/step - loss: 0.2039 - accuracy: 0.9881\n",
      "Epoch 274/300\n",
      "10000/10000 [==============================] - 7s 655us/sample - loss: 0.4673 - accuracy: 0.9215\n",
      "391/391 [==============================] - 74s 190ms/step - loss: 0.2021 - accuracy: 0.9887\n",
      "Epoch 275/300\n",
      "10000/10000 [==============================] - 7s 654us/sample - loss: 0.4800 - accuracy: 0.9167\n",
      "391/391 [==============================] - 74s 190ms/step - loss: 0.1992 - accuracy: 0.9889\n",
      "Epoch 276/300\n",
      "10000/10000 [==============================] - 7s 654us/sample - loss: 0.4818 - accuracy: 0.9175\n",
      "391/391 [==============================] - 74s 190ms/step - loss: 0.1992 - accuracy: 0.9887\n",
      "Epoch 277/300\n",
      "10000/10000 [==============================] - 7s 680us/sample - loss: 0.4964 - accuracy: 0.9135\n",
      "391/391 [==============================] - 74s 190ms/step - loss: 0.1984 - accuracy: 0.9887\n",
      "Epoch 278/300\n",
      "10000/10000 [==============================] - 7s 657us/sample - loss: 0.4851 - accuracy: 0.9181\n",
      "391/391 [==============================] - 74s 190ms/step - loss: 0.1964 - accuracy: 0.9889\n",
      "Epoch 279/300\n",
      "10000/10000 [==============================] - 7s 652us/sample - loss: 0.4646 - accuracy: 0.9201\n",
      "391/391 [==============================] - 74s 189ms/step - loss: 0.1944 - accuracy: 0.9890\n",
      "Epoch 280/300\n",
      "10000/10000 [==============================] - 7s 654us/sample - loss: 0.4868 - accuracy: 0.9161\n",
      "391/391 [==============================] - 74s 189ms/step - loss: 0.1946 - accuracy: 0.9892\n",
      "Epoch 281/300\n",
      "10000/10000 [==============================] - 7s 652us/sample - loss: 0.4696 - accuracy: 0.9179\n",
      "391/391 [==============================] - 74s 189ms/step - loss: 0.1933 - accuracy: 0.9888\n",
      "Epoch 282/300\n",
      "10000/10000 [==============================] - 7s 653us/sample - loss: 0.4605 - accuracy: 0.9227\n",
      "391/391 [==============================] - 74s 190ms/step - loss: 0.1906 - accuracy: 0.9892\n",
      "Epoch 283/300\n",
      "10000/10000 [==============================] - 6s 649us/sample - loss: 0.4753 - accuracy: 0.9168\n",
      "391/391 [==============================] - 74s 189ms/step - loss: 0.1899 - accuracy: 0.9893\n",
      "Epoch 284/300\n",
      "10000/10000 [==============================] - 7s 651us/sample - loss: 0.4726 - accuracy: 0.9193\n",
      "391/391 [==============================] - 74s 189ms/step - loss: 0.1892 - accuracy: 0.9892\n",
      "Epoch 285/300\n",
      "10000/10000 [==============================] - 7s 662us/sample - loss: 0.4703 - accuracy: 0.9196\n",
      "391/391 [==============================] - 74s 189ms/step - loss: 0.1897 - accuracy: 0.9886\n",
      "Epoch 286/300\n",
      "10000/10000 [==============================] - 7s 653us/sample - loss: 0.4651 - accuracy: 0.9215\n",
      "391/391 [==============================] - 74s 189ms/step - loss: 0.1860 - accuracy: 0.9894\n",
      "Epoch 287/300\n",
      "10000/10000 [==============================] - 7s 655us/sample - loss: 0.4581 - accuracy: 0.9205\n",
      "391/391 [==============================] - 74s 189ms/step - loss: 0.1858 - accuracy: 0.9890\n",
      "Epoch 288/300\n",
      "10000/10000 [==============================] - 7s 652us/sample - loss: 0.4850 - accuracy: 0.9157\n",
      "391/391 [==============================] - 74s 189ms/step - loss: 0.1838 - accuracy: 0.9898\n",
      "Epoch 289/300\n",
      "10000/10000 [==============================] - 7s 657us/sample - loss: 0.4738 - accuracy: 0.9189\n",
      "391/391 [==============================] - 74s 189ms/step - loss: 0.1836 - accuracy: 0.9897\n",
      "Epoch 290/300\n",
      "10000/10000 [==============================] - 7s 697us/sample - loss: 0.4718 - accuracy: 0.9184\n",
      "391/391 [==============================] - 74s 191ms/step - loss: 0.1827 - accuracy: 0.9895\n",
      "Epoch 291/300\n",
      "10000/10000 [==============================] - 7s 653us/sample - loss: 0.4602 - accuracy: 0.9206\n",
      "391/391 [==============================] - 74s 189ms/step - loss: 0.1799 - accuracy: 0.9902\n",
      "Epoch 292/300\n",
      "10000/10000 [==============================] - 7s 666us/sample - loss: 0.4591 - accuracy: 0.9210\n",
      "391/391 [==============================] - 74s 189ms/step - loss: 0.1789 - accuracy: 0.9897\n",
      "Epoch 293/300\n",
      "10000/10000 [==============================] - 7s 659us/sample - loss: 0.4643 - accuracy: 0.9181\n",
      "391/391 [==============================] - 74s 189ms/step - loss: 0.1779 - accuracy: 0.9899\n",
      "Epoch 294/300\n",
      "10000/10000 [==============================] - 6s 649us/sample - loss: 0.4824 - accuracy: 0.9168\n",
      "391/391 [==============================] - 74s 189ms/step - loss: 0.1776 - accuracy: 0.9897\n",
      "Epoch 295/300\n",
      "10000/10000 [==============================] - 7s 653us/sample - loss: 0.4668 - accuracy: 0.9202\n",
      "391/391 [==============================] - 74s 189ms/step - loss: 0.1750 - accuracy: 0.9906\n",
      "Epoch 296/300\n",
      "10000/10000 [==============================] - 7s 659us/sample - loss: 0.4661 - accuracy: 0.9188\n",
      "391/391 [==============================] - 74s 188ms/step - loss: 0.1736 - accuracy: 0.9907\n",
      "Epoch 297/300\n",
      "10000/10000 [==============================] - 7s 653us/sample - loss: 0.4653 - accuracy: 0.9192\n",
      "391/391 [==============================] - 74s 189ms/step - loss: 0.1745 - accuracy: 0.9901\n",
      "Epoch 298/300\n",
      "10000/10000 [==============================] - 7s 656us/sample - loss: 0.4895 - accuracy: 0.9157\n",
      "391/391 [==============================] - 74s 190ms/step - loss: 0.1726 - accuracy: 0.9906\n",
      "Epoch 299/300\n",
      "10000/10000 [==============================] - 7s 654us/sample - loss: 0.4812 - accuracy: 0.9169\n",
      "391/391 [==============================] - 74s 190ms/step - loss: 0.1714 - accuracy: 0.9904\n",
      "Epoch 300/300\n",
      "10000/10000 [==============================] - 6s 650us/sample - loss: 0.4782 - accuracy: 0.9172\n",
      "391/391 [==============================] - 74s 189ms/step - loss: 0.1700 - accuracy: 0.9902\n"
     ]
    }
   ],
   "source": [
    "train_acc = []\n",
    "history3 = resnet50.fit(datagen.flow(xtrain, ytrain, batch_size=128),epochs=300,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(history3.history['accuracy'])\n",
    "plt.plot(train_acc)\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50.save_weights('./resnet50-300')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50.save('my_model.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model('my_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
